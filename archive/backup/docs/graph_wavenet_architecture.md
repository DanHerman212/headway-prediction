Technical Implementation Strategy: Real-Time Headway Prediction for NYCT Lines A, C, and E using Graph WaveNet1. Operational Context and Problem Definition1.1 The Eighth Avenue Corridor ChallengeThe New York City Transit (NYCT) subway system represents one of the most complex, high-density signaling environments in the world. Among its various trunk lines, the Eighth Avenue Line—carrying the A, C, and E services—presents a unique set of operational challenges that defy simple linear prediction models. The A train, extending over 32 miles from Inwood-207th Street in Manhattan to Far Rockaway or Lefferts Boulevard in Queens, is the longest single route in the system. Its operation is inextricably linked with the C train, which provides local service along the same corridor, and the E train, which interlines with the A and C from 50th Street down to Canal Street before diverging towards the Queens Boulevard Line.2This shared infrastructure creates a phenomenon known in transit operations as "delay propagation" or "knock-on delays".4 In a fixed-block signaling system, which still governs large portions of the A/C/E lines, the occupancy of a track block by one train physically prevents a trailing train from entering that segment. Consequently, a minor mechanical failure on an E train at 42nd Street–Port Authority Bus Terminal does not merely delay the E line; it creates a backward-propagating wave of congestion that impacts C trains on the local track and, depending on switch configurations at interlockings like Canal Street or 59th Street–Columbus Circle, can ripple into the A train's express service.1Traditional approaches to arrival prediction have relied heavily on historical averages or simple autoregressive models (e.g., ARIMA), which treat each line or station as an isolated time series.6 These models operate on the assumption of stationarity and fail to capture the spatial dependencies inherent in a networked system. They cannot effectively "see" that a delay at a merge point miles away is the precursor to a headway gap at the current station. Furthermore, the reliance on Euclidean distance in some spatial models is flawed for subway networks; two stations might be geographically close but topologically distant (e.g., on different lines), or geographically distant but operationally coupled (e.g., express stops).81.2 The Objective: Dynamic Headway PredictionThe primary objective of this technical implementation is to move beyond static schedule adherence and predict real-time headway. Headway is defined as the elapsed time between the arrival of two consecutive vehicles serving the same route and direction at a specific node.10For the A, C, and E lines, headway regularity is often a more critical metric of passenger satisfaction than absolute schedule adherence. "Bus bunching"—or in this case, "train bunching"—occurs when a delayed train picks up an excess load of passengers, increasing its dwell times at subsequent stations, which causes it to fall further behind while the trailing train catches up.12 Accurate prediction of these dynamic headway fluctuations requires a model that can ingest the state of the entire Eighth Avenue corridor simultaneously, understanding that the "state" of the system at Inwood (Node A) has a causal relationship with the state at 125th Street (Node B), modulated by the underlying graph topology of the tracks.1.3 The Selection of Graph WaveNetTo address these challenges, this report details the implementation of Graph WaveNet, a deep learning architecture that combines Graph Convolutional Networks (GCNs) with Dilated Temporal Convolutional Networks (TCNs).6Graph WaveNet is selected for three specific technical advantages relevant to the A/C/E lines:Handling Non-Euclidean Data: It models the subway system as a graph where stations are nodes and tracks are edges, capturing the physical connectivity of the network better than grid-based CNNs or vector-based RNNs.7Long-Term Temporal Dependencies: The "WaveNet" component utilizes dilated causal convolutions, which allow the model to have an exponentially growing receptive field. This means the model can efficiently look back at an hour or more of historical data to identify long-term trends (e.g., a slow degradation in service speed) without the computational expense and vanishing gradient problems associated with Long Short-Term Memory (LSTM) units.16Adaptive Adjacency Matrix: Perhaps most critically, Graph WaveNet includes a self-adaptive adjacency mechanism. It learns a dependency matrix from the data, identifying "latent" connections between nodes that are not physically connected by tracks but are operationally correlated—for instance, the relationship between a dispatch terminal and a merge point, or the synchronization between the E and C lines during late-night service.182. Data Engineering and Ingestion PipelineThe efficacy of any deep learning model is bounded by the quality of its input data. For the NYC subway, this requires a robust pipeline to ingest, sanitize, and transform the MTA's GTFS-Realtime feeds into a structured tensor format suitable for TensorFlow Keras.2.1 The GTFS-Realtime ProtocolThe Metropolitan Transportation Authority (MTA) provides real-time train data via the GTFS-Realtime (GTFS-RT) specification, encoded as Protocol Buffers (protobufs). While standard GTFS-RT is a generic format, the MTA utilizes specific extensions to handle the complexity of the subway system.20For the A, C, and E lines, the relevant data is contained within Feed 26 (often referred to as the ACE feed, though it also contains H and S shuttles).22 This feed is updated approximately every 30 seconds.21 The ingestion pipeline must parse three primary entities from this feed:2.1.1 Trip Updates (TripUpdate)This entity provides the predicted arrival and departure times for future stops for a specific trip. It is the primary source for calculating current deviations from the schedule.21Critical Fields: trip_id, stop_id, arrival.time, departure.time.NYCT Extension: The MTA includes a NyctTripDescriptor which indicates the train_id, direction, and whether the train is assigned to a specific schedule line. This is crucial for distinguishing between an 'A' train running on the 'C' line tracks (reroute) versus a standard service.212.1.2 Vehicle Positions (VehiclePosition)This entity provides the current location of the train, often expressed as an index relative to the series of stops or a timestamp of the last verified location.21Relevance: TripUpdates are predictions; VehiclePositions are ground truth observations. The model must trust positions more than predictions. However, in the tunnel network, "position" is often inferred from track circuit activations rather than GPS, leading to quantization noise where a train appears to "jump" between stations.242.1.3 Alerts (Alert)Service alerts provide unstructured or semi-structured text describing delays, reroutes, or suspensions.21Feature Extraction: These must be parsed to create categorical features (e.g., is_rerouted=1, track_maintenance=1) that effectively "switch" the graph topology in the model's perception. If an Alert states "A trains running local from 59th to Canal," the edges in the graph representing the express run must be deactivated, and the local edges activated.2.2 Data Sanitization: The "Ghost Train" PhenomenonOne of the most persistent challenges in modeling NYCT data is the presence of "ghost trains"—entities that appear in the feed but do not correspond to physical trains, or trains that vanish and reappear.25Detection and Filtering Strategy:Kinematic Validation: The pipeline must implement a physics-based filter. If a train's reported position updates imply a travel speed between stations exceeding physical limits (e.g., > 60 mph on the A line), the data point is flagged as a sensor error and discarded or smoothed using a Kalman Filter.27Dwell Time Thresholds: Trains that report remaining at a single station for excessive periods (e.g., > 15-20 minutes) without a corresponding "Held" status in the Alerts feed are often phantom data artifacts caused by a track circuit failure "locking" a block. These must be masked from the headway calculation to prevent the model from learning artificial infinite headways.29Route Validation: The stop_id sequence in a TripUpdate must be validated against the shapes.txt and stop_times.txt from the static GTFS feed.30 An 'E' train suddenly reporting a stop at a station only served by the 'F' line (without a reroute alert) indicates a feed error or a mislabeled trip_id.2.3 Constructing the Input TensorGraph WaveNet requires a specific 4-dimensional input tensor format: (Batch_Size, Input_Sequence_Length, Number_of_Nodes, Number_of_Features).312.3.1 Definition of Nodes ($N$)Unlike simple road networks where a sensor is a node, a subway station is a complex entity. A naive approach of treating "42nd Street" as a single node is insufficient because it conflates Northbound and Southbound traffic, as well as Express and Local tracks.Proposed Node Architecture: Nodes are defined as Directed Platform Tuples.Node $i$: (Station_ID, Line_ID, Direction)Example: (A24, A, North) represents the A train platform at 42nd St/Port Authority, Northbound.Example: (A24, C, North) represents the C train platform at the same station.Handling Shared Platforms: Even if A and C trains share the same physical platform edge (as they do at 42nd St), they are modeled as distinct logical nodes. The "shared track" relationship is encoded via a highly weighted edge in the adjacency matrix. This separation allows the model to predict headways for "A service" distinct from "C service" while still acknowledging their physical coupling.33Total Nodes: The A/C/E system involves approximately 60-80 stations per line. With directionality and line separation, the graph will contain approximately $N \approx 300-400$ nodes.2.3.2 Feature Dimension ($F$)For each node at each time step $t$, we extract a feature vector $X_t$:Observed Headway ($h_t$): The time elapsed since the previous train departure.Scheduled Headway ($h_{sched}$): Derived from GTFS Static stop_times.txt. The deviation $(h_t - h_{sched})$ is a powerful predictor signal.30Active Delay: The current lateness of the train approaching the node (in seconds).Time Encoding: Cyclical encoding of the timestamp to capture seasonality.$Time_{sin} = \sin(2\pi \cdot t / 1440)$$Time_{cos} = \cos(2\pi \cdot t / 1440)$Service State: One-hot encoded vector representing the current service alert status (Normal, Delays, Suspended).2.3.3 Temporal DimensionsInput Sequence ($T_{in}$): We select a lookback window of 12 steps. If data is aggregated into 5-minute buckets, this represents 1 hour of history.17Prediction Horizon ($T_{out}$): We aim to predict the next 12 steps (1 hour), allowing for strategic dispatching decisions.2.4 Preprocessing Pipeline ImplementationThe preprocessing logic is implemented in Python, leveraging pandas for time-series manipulation and nyct-gtfs for parsing.StepOperationDescription1IngestPoll https://api-endpoint.mta.info/Dataservice/mtagtfsfeed (Feed 26) every 30s.2ParseExtract TripUpdates and VehiclePositions into a normalized DataFrame.3FilterRemove ghost trains (speed > 60mph, dwell > 20min). Match against Static GTFS.4PivotReshape data into (Time, Node) matrix. Handle missing timestamps via forward-fill (with decay) or masking.5EngineerCalculate actual headways, attach static schedule features, generate time embeddings.6NormalizeZ-score normalization ($X_{norm} = (X - \mu) / \sigma$) is crucial for neural network convergence.357TensorizeUse tf.keras.utils.timeseries_dataset_from_array to create sliding windows for training.3. Mathematical Foundations of Spatiotemporal Graph ModelingTo effectively implement Graph WaveNet, one must understand the mathematical principles that allow it to process the non-Euclidean structure of the subway system.3.1 Graph RepresentationThe subway network is represented as a graph $G = (V, E)$, where $V$ is the set of $N$ nodes (stations) and $E$ is the set of edges (tracks).The connectivity is described by the Adjacency Matrix $A \in \mathbb{R}^{N \times N}$.$A_{ij} = 1$ if there is a direct track connection from station $i$ to station $j$.$A_{ij} = 0$ otherwise.For the A/C/E lines, the adjacency matrix is sparse. However, as noted in Section 2.3.1, we introduce Lateral Edges to represent shared infrastructure. If Node $i$ (A train at Canal) and Node $j$ (C train at Canal) share a track, we set $A_{ij} = w$, where $w$ is a weight representing the strength of the coupling (e.g., 1.0 for shared track, 0.5 for adjacent track in same tunnel).363.2 Graph Convolution: Spectral vs. SpatialTraditional Convolutional Neural Networks (CNNs) rely on the regular grid structure of images to define the convolution operation (sliding a filter). In graphs, there is no "up" or "down."Spectral Approaches (e.g., ChebNet): Operate in the Fourier domain of the graph using the Graph Laplacian. While mathematically elegant, they are computationally expensive ($O(N^3)$ for eigendecomposition) and assume a fixed graph structure.38Spatial Approaches (Diffusion): Graph WaveNet utilizes a spatial approach based on Diffusion Convolution. It models the flow of information (traffic/trains) as a diffusion process across the graph. This is defined as:$$Z = \sum_{k=0}^{K} P^k X W_k$$Here, $P$ is the transition matrix ($P = D^{-1}A$), $k$ is the diffusion step (number of hops), $X$ is the input signal, and $W_k$ are the learnable weights for each diffusion step. This approximates the spectral convolution without expensive eigendecomposition and allows for directed graphs (asymmetric $A$), which is crucial because train delays propagate differently upstream vs. downstream.153.3 The Adaptive Adjacency MatrixA core innovation of Graph WaveNet is the recognition that the physical track map ($A_{phys}$) does not capture all dependencies. For example, crew availability at the 207th St Yard (Node $i$) might affect the departure of trains at Far Rockaway (Node $j$) due to scheduling logistics, despite them being at opposite ends of the line.To capture these hidden dependencies, the model learns a Self-Adaptive Adjacency Matrix $\tilde{A}_{adp}$.This is constructed using two learnable node embedding dictionaries, $E_1, E_2 \in \mathbb{R}^{N \times d}$, where $d$ is the embedding dimension (typically 10).$$\tilde{A}_{adp} = \text{Softmax}(\text{ReLU}(E_1 E_2^T))$$The Softmax function ensures the weights are normalized, and ReLU enforces sparsity (eliminating weak connections). This matrix is learned end-to-end via backpropagation, allowing the model to "discover" the latent topology of the subway operation.183.4 Dilated Causal ConvolutionsWhile Graph Convolution handles spatial dependencies ($N$ dimension), the temporal dependencies ($T$ dimension) are handled by Dilated Causal Convolutions (TCN).Causal: The convolution at time $t$ only convolves elements from time $t, t-1, t-2...$. It strictly prevents information leakage from the future ($t+1$), which is a common pitfall in bidirectional LSTMs used for forecasting.40Dilated: Standard convolution has a receptive field linear to the number of layers. Dilated convolution skips inputs with a step size $d$. By stacking layers with exponentially increasing dilation factors ($1, 2, 4, 8...$), the receptive field grows exponentially.Layer 1 (Dilation 1): Sees inputs $[t, t-1]$.Layer 2 (Dilation 2): Sees outputs of Layer 1 at $[t, t-2]$. Effectively sees inputs $[t, t-1, t-2, t-3]$.Layer 3 (Dilation 4): Sees $[t...t-7]$.This architecture allows Graph WaveNet to handle very long sequences (capturing daily seasonality) with a shallow network, ensuring fast training and inference compared to RNNs.64. The Graph WaveNet ArchitectureThe Graph WaveNet architecture is a stack of Spatio-Temporal Blocks. We define the specific architectural hyperparameters tailored for the A/C/E line complexity.4.1 The Gated TCN UnitThe temporal convolution layer employs a Gated Linear Unit (GLU) mechanism, similar to the original WaveNet for audio generation or the PixelCNN.$$h = \tanh(W_{filter} * X) \odot \sigma(W_{gate} * X)$$The $\tanh$ activation acts as the filter (feature extractor).The $\sigma$ (sigmoid) activation acts as the gate, controlling what information is passed through (0 to 1).$\odot$ denotes element-wise multiplication (Hadamard product).This gating mechanism is superior to simple ReLU for time series as it allows the model to control the flow of information through the deep stack, effectively managing the "vanishing gradient" problem.194.2 The Spatio-Temporal Block StructureEach block consists of the following sequence:Input: Tensor $X^{(l)}$ from previous layer.TCN: Gated Dilated Convolution processes the time dimension.GCN: The output of the TCN is fed into the Graph Convolution layer.The GCN aggregates features from neighbor nodes (using both $A_{phys}$ and $\tilde{A}_{adp}$).Residual Connection: The input $X^{(l)}$ is added to the output of the GCN (via a 1x1 convolution to match dimensions if necessary) to form the input for the next block $X^{(l+1)}$.Skip Connection: The output is also sent to a final aggregation layer (Skip Output) to form the final prediction.4.3 Proposed Hyperparameters for A/C/E ImplementationBased on benchmarks on similar traffic datasets (METR-LA, PEMS-BAY), the following configuration is recommended for the NYCT scale 35:Number of Blocks: 4Layers per Block: 2 (Total 8 layers)Dilations: (Repeating cycle to maintain local resolution) or.Hidden Channels: 64.Skip Channels: 256.Dropout: 0.3 (to prevent overfitting on noisy GTFS data).Graph Diffusion Steps ($K$): 2 (Meaning each node aggregates information from neighbors 2 hops away).5. TensorFlow Keras Implementation StrategyThis section translates the theoretical architecture into a concrete implementation plan using the TensorFlow Keras Functional API and Model Subclassing.5.1 Environment and DependenciesThe implementation requires tensorflow >= 2.10 to leverage the latest Keras optimizations.Pythonimport tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import pandas as pd
from nyct_gtfs import NYCTFeed
5.2 Custom Layer: Adaptive Adjacency MatrixWe must implement the adaptive matrix as a custom Keras layer to ensure the embedding weights are trainable.Pythonclass AdaptiveAdjacency(layers.Layer):
    def __init__(self, num_nodes, embed_dim=10, **kwargs):
        super().__init__(**kwargs)
        self.num_nodes = num_nodes
        self.embed_dim = embed_dim

    def build(self, input_shape):
        # Initialize two learnable embedding matrices E1 and E2
        self.node_E1 = self.add_weight(
            name="node_embedding_1",
            shape=(self.num_nodes, self.embed_dim),
            initializer="uniform",
            trainable=True
        )
        self.node_E2 = self.add_weight(
            name="node_embedding_2",
            shape=(self.num_nodes, self.embed_dim),
            initializer="uniform",
            trainable=True
        )

    def call(self, inputs):
        # Calculate E1 * E2^T
        logits = tf.matmul(self.node_E1, self.node_E2, transpose_b=True)
        # Apply ReLU then Softmax as per Graph WaveNet paper
        adj = tf.nn.softmax(tf.nn.relu(logits), axis=-1)
        return adj
5.3 Custom Layer: Graph ConvolutionThe diffusion convolution requires custom logic to handle the support matrices (forward, backward, and adaptive).Pythonclass GraphConv(layers.Layer):
    def __init__(self, out_channels, diffusion_steps=2, **kwargs):
        super().__init__(**kwargs)
        self.out_channels = out_channels
        self.k = diffusion_steps

    def build(self, input_shape):
        # input_shape is (Batch, Nodes, Features/Time)
        self.num_nodes = input_shape
        self.in_channels = input_shape[-1]
        
        # Weights for the diffusion steps: (K+1) * In_Channels * Out_Channels
        self.weights_pool = self.add_weight(
            name="gcn_weights",
            shape=(self.k + 1, self.in_channels, self.out_channels),
            initializer="glorot_uniform",
            trainable=True
        )

    def call(self, inputs, adj_matrices):
        # inputs: Node features
        # adj_matrices: List of [A_forward, A_backward, A_adaptive]
        
        output = tf.zeros_like(inputs) # Placeholder logic
        # Implementation of Eq: Z = Sum(P^k * X * W_k)
        # This involves sparse matrix multiplication logic or Einstein summation
        # tf.einsum is highly recommended for efficient tensor contraction here.
        return output
5.4 Building the Main ModelThe model is assembled using the Functional API to allow for complex branching (skip connections).Inputs: input_layer = layers.Input(shape=(num_nodes, input_window, num_features))Input Projection: A 1x1 Conv layer maps the input features (headway, delay, time) to the hidden channel dimension (64).x = layers.Conv2D(filters=64, kernel_size=(1,1))(input_layer)ST-Blocks: Loop through the defined number of blocks.Inside the loop, apply the TCN (Dilated Conv1D).Apply the AdaptiveAdjacency layer to get $\tilde{A}$.Apply the GraphConv layer using the output of TCN and $\tilde{A}$.Add Residual connection.Collect Skip connection output.Output Module:Sum all skip connection outputs.Apply ReLU.Apply two 1x1 Conv layers (End-to-End convolution) to project from hidden channels to the output prediction horizon ($T_{out}$).output = layers.Conv2D(filters=output_horizon, kernel_size=(1,1))(skip_sum)Model Compilation:model = keras.Model(inputs=input_layer, outputs=output)5.5 Loss Function EngineeringFor subway headways, standard Mean Squared Error (MSE) is often insufficient because it is sensitive to outliers (e.g., a 40-minute gap due to a medical emergency). Mean Absolute Error (MAE) is more robust.However, we must implement a Masked MAE. The subway system has periods (late nights) or specific stations (part-time booths) where data might be missing or zero-filled. We do not want the model to learn to predict "0" for missing data.Pythondef masked_mae_loss(y_true, y_pred):
    mask = tf.not_equal(y_true, 0)
    mask = tf.cast(mask, tf.float32)
    loss = tf.abs(y_true - y_pred)
    loss *= mask # Zero out loss where data is missing
    return tf.reduce_mean(loss)
This loss function is critical for the A/C/E line because the Rockaway Park branch (A) and the World Trade Center branch (E) have different service hours than the trunk lines, creating extensive "null" periods in the tensor.215.6 Optimizer and CallbacksOptimizer: Adam (tf.keras.optimizers.Adam) with a learning rate of $0.001$.Scheduler: Implement ReduceLROnPlateau to lower the learning rate if validation loss stagnates, allowing the model to fine-tune the adaptive adjacency matrix weights.Early Stopping: Monitor validation MAE with a patience of 10-20 epochs to prevent overfitting.Curriculum Learning: It is advisable to start training with a simplified dataset (e.g., only peak hour data) where headways are regular, and then gradually introduce off-peak and weekend data. This helps the model learn the "normal" physics of the system first before trying to model the stochasticity of late-night dispatching.436. Deployment, Latency, and ScalabilityDeploying a deep learning model for real-time transit data requires a robust architecture that can handle the 30-second update cycle of the MTA feeds without inducing latency that renders the prediction stale.6.1 Inference Pipeline ArchitectureThe proposed deployment architecture consists of four microservices:Ingest Service (Python/Kafka): connect to the MTA GTFS-RT API. Parses the protobufs and pushes normalized events ({node_id, timestamp, headway}) to an Apache Kafka topic.State Manager (Redis): A worker consumes the Kafka stream and updates a "Current State Tensor" in a Redis cache. This tensor represents the sliding window ($T_{in}$) required for the model. Redis is chosen for its sub-millisecond read/write latency.Inference Engine (TensorFlow Serving): A Dockerized TensorFlow Serving container hosts the trained Graph WaveNet model (saved in SavedModel format). It exposes a gRPC endpoint.API Gateway (FastAPI): When a user (or the frontend app) requests a prediction for the A line:The API fetches the current tensor from Redis.Sends it to TF Serving via gRPC.Receives the prediction tensor.Formats it into JSON (e.g., "Next A train at 42nd St in 4 min, 9 min, 15 min") and returns it.6.2 Handling Latency ConstraintsThe Graph WaveNet model is highly efficient due to its fully convolutional nature (unlike RNNs which process sequentially).Inference Time: For a graph of ~400 nodes and history of 12 steps, inference on a standard NVIDIA T4 GPU (or even modern CPU) takes < 100ms.44End-to-End Latency: The total time from MTA feed update to updated prediction availability should be < 2 seconds. This is well within the 30-second refresh rate of the feed, ensuring the system is always "real-time".456.3 Online Learning and DriftSubway dynamics change. A construction project might close a track for 6 months, radically altering the adjacency matrix. The deployment plan includes an Offline Retraining Loop:Daily: Archive the day's GTFS-RT data.Weekly: Fine-tune the model on the last 4 weeks of data. This allows the Adaptive Adjacency Matrix to "learn" the new topology (e.g., learning that A trains are now bypassing 145th St) without manual reconfiguration of the graph.467. Evaluation and BenchmarkingTo ensure the system provides value over existing static schedules, we define rigorous evaluation metrics.7.1 Quantitative MetricsMAE (Mean Absolute Error): The primary metric. Measured in minutes. A success target would be an MAE < 2 minutes for a 30-minute prediction horizon.MAPE (Mean Absolute Percentage Error): Critical because a 2-minute error on a 5-minute headway (40%) is far worse than on a 20-minute headway (10%).RMSE (Root Mean Squared Error): Penalizes large errors more heavily. Useful for identifying if the model fails catastrophically during disruptions.7.2 Operational Metrics (Transit Specific)Regularity Index Prediction: Can the model accurately predict the variance of the next 3 trains? (i.e., predicting bunching). This is measured by the correlation between the predicted coefficient of variation and the actual coefficient of variation of the headways.11Prediction Padding: The buffer time required to ensure a passenger makes the train 95% of the time. If the model predicts 5 mins, but the train arrives in 3, the passenger misses it. We measure the "Early Arrival Rate" (bad for passengers) vs. "Late Arrival Rate".297.3 Benchmarking CandidatesThe Graph WaveNet performance should be compared against:Historical Average (HA): The baseline.VAR (Vector Auto-Regression): A standard statistical baseline for multivariate time series.LSTM / GRU: Deep learning baselines without graph structure.DCRNN: A graph-based RNN. Comparing against DCRNN validates the efficiency of the TCN architecture in Graph WaveNet.178. Conclusion and Future OutlookThe implementation of Graph WaveNet for the NYC Subway A, C, and E lines represents a significant leap forward in transit analytics. By shifting from linear, schedule-based assumptions to a graph-based, spatiotemporal deep learning approach, we can capture the complex, ripple-effect dynamics that define the Eighth Avenue corridor.The key to success lies in the meticulous engineering of the data pipeline—specifically the handling of "ghost trains" and the accurate construction of the "station-direction" graph—and the proper implementation of the Adaptive Adjacency Matrix. This matrix acts as the brain of the system, autonomously learning the hidden operational constraints of the MTA's dispatching, from interlining delays at Canal Street to crew logistics at terminal points.With a robust Keras implementation and a scalable deployment architecture, this system can provide commuters and controllers with the most accurate, real-time view of subway performance possible, transforming raw data into actionable intelligence for millions of daily riders.Note: This report synthesizes research from sources 20 through 47 to provide a comprehensive technical roadmap. All architectural decisions are grounded in the specific constraints of the NYCT environment and the state-of-the-art in geometric deep learning.