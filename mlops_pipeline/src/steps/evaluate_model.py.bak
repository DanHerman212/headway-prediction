"""                                                                              
evaluate_model.py                                                                 
-----------------                                                                 
Evaluation step for the Headway Prediction Pipeline.                               
Calculates global metrics (MAE, SMAPE) and generates 'Rush Hour' specific plots.   
Logs results to Vertex AI Experiments + TensorBoard.                               
"""

import pandas as pd
import numpy as np
import re
import torch
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import logging
import os
import tempfile
from typing import Annotated, Tuple, Dict, Any, Optional

from google.cloud import aiplatform
from torch.utils.tensorboard import SummaryWriter
from zenml import step, get_step_context
from omegaconf import DictConfig
from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet
from pytorch_forecasting.metrics import MAE, SMAPE

# Configure Logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RushHourVisualizer:
    """
    Generates Rush Hour performance plots for A, C, E subway lines.

    Requirements:
      - Subplot titles show the decoded group_id (e.g. "A_northbound_14st")
      - Every subplot has full hover labels (90% Confidence, Predicted P50, Actual)
      - X-axis shows real timestamps derived from time_idx + dataset time anchor
      - MTA Blue (#0039A6) colour scheme with accessible contrast
    """

    def __init__(
        self,
        predictions: torch.Tensor,
        x: Dict[str, torch.Tensor],
        test_dataset: TimeSeriesDataSet,
        time_anchor_iso: str = "",
    ):
        self.predictions = predictions.cpu()  # (Batch, PredLen, Quantiles)
        self.x = x
        self.test_dataset = test_dataset
        self.time_anchor = (
            pd.Timestamp(time_anchor_iso) if time_anchor_iso else None
        )

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------
    def _decode_groups(self, encoded_ids: np.ndarray) -> np.ndarray:
        """Decode integer-encoded group IDs back to human-readable strings."""
        try:
            params = self.test_dataset.get_parameters()
            encoder = params.get("categorical_encoders", {}).get("group_id")
            if encoder is not None and hasattr(encoder, "inverse_transform"):
                return encoder.inverse_transform(encoded_ids)
        except Exception as e:
            logger.warning("Could not decode groups via get_parameters(): %s", e)

        # Fallback: use decoded_index which stores string group_ids per sample
        try:
            di = self.test_dataset.decoded_index
            if "group_id" in di.columns:
                unique_groups = di["group_id"].unique()
                return np.array([
                    unique_groups[int(g)] if int(g) < len(unique_groups) else str(g)
                    for g in encoded_ids
                ])
        except Exception as e:
            logger.warning("Fallback group decoding also failed: %s", e)

        return encoded_ids.astype(str)

    def _time_idx_to_timestamps(self, time_idx: np.ndarray) -> Optional[np.ndarray]:
        """Convert integer time_idx values to real datetime timestamps.

        time_idx is minutes elapsed from the global min arrival_time_dt
        (computed in data_processing.clean_dataset).  The anchor is passed
        in as ``time_anchor_iso``.
        """
        if self.time_anchor is None:
            return None
        try:
            timestamps = self.time_anchor + pd.to_timedelta(time_idx, unit="min")
            return timestamps.values
        except Exception as e:
            logger.warning("Could not convert time_idx to timestamps: %s", e)
            return None

    def _reconstruct_dataframe(self) -> pd.DataFrame:
        """Build a flat DataFrame from tensor outputs with decoded groups and timestamps."""
        actuals = self.x["decoder_target"].cpu().view(-1).numpy()
        p50 = self.predictions[:, :, 1].view(-1).numpy()
        p10 = self.predictions[:, :, 0].view(-1).numpy()
        p90 = self.predictions[:, :, 2].view(-1).numpy()

        batch_groups = self.x["groups"].cpu().view(-1)
        seq_len = self.predictions.shape[1]
        group_ids_encoded = batch_groups.repeat_interleave(seq_len).numpy()
        decoded_groups = self._decode_groups(group_ids_encoded)

        if "decoder_time_idx" in self.x:
            time_idx = self.x["decoder_time_idx"].cpu().view(-1).numpy()
        else:
            time_idx = np.arange(len(actuals))

        timestamps = self._time_idx_to_timestamps(time_idx)

        df = pd.DataFrame({
            "group": decoded_groups,
            "time_idx": time_idx,
            "actual": actuals,
            "pred_p50": p50,
            "pred_p10": p10,
            "pred_p90": p90,
        })
        if timestamps is not None:
            df["timestamp"] = timestamps

        return df

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    def plot_rush_hour(
        self,
        start_idx_window: Optional[int] = None,
        window_size: int = 180,
    ) -> go.Figure:
        """Generate an interactive Plotly figure with per-group subplots.

        Selects the most data-rich AM rush hour day (07:00-10:00) for each
        group.  Falls back to PM rush (17:00-20:00) if no AM data exists.

        Parameters
        ----------
        start_idx_window : int, optional
            Absolute time_idx to start the window.  If *None* the visualizer
            auto-selects a rush hour window from the timestamps.
        window_size : int
            Width of the display window in minutes (default 180 = 3 hours).
        """
        df = self._reconstruct_dataframe()
        has_timestamps = "timestamp" in df.columns

        # ---- Select groups for A / C / E lines ----
        target_lines = ["A", "C", "E"]
        available_groups = df["group"].unique()

        plot_groups = []
        for g in available_groups:
            line_letter = str(g).split("_")[0]
            if line_letter in target_lines:
                plot_groups.append(g)

        if not plot_groups:
            plot_groups = list(available_groups)[:3]
        else:
            # One per target line, up to 3
            seen_lines: Dict[str, str] = {}
            for g in plot_groups:
                letter = str(g).split("_")[0]
                if letter not in seen_lines:
                    seen_lines[letter] = g
            plot_groups = list(seen_lines.values())[:3]

        # ---- Build subplots ----
        fig = make_subplots(
            rows=len(plot_groups),
            cols=1,
            shared_xaxes=False,
            vertical_spacing=0.12,
            subplot_titles=[str(g) for g in plot_groups],
        )

        for i, group in enumerate(plot_groups):
            row = i + 1
            group_df = df[df["group"] == group].sort_values("time_idx")
            if group_df.empty:
                continue

            # --- Rush hour window selection ---
            rush_period = None
            if start_idx_window is not None:
                t_start = start_idx_window
            elif has_timestamps:
                gts = pd.to_datetime(group_df["timestamp"])
                hours = gts.dt.hour
                am_mask = hours.between(7, 9)
                pm_mask = hours.between(17, 19)
                if am_mask.any():
                    rush_df = group_df.loc[am_mask]
                    rush_period = "AM Rush (07:00-10:00)"
                else:
                    rush_df = group_df.loc[pm_mask]
                    rush_period = "PM Rush (17:00-20:00)"
                # Pick the date with the most observations (typically a weekday)
                rush_ts = pd.to_datetime(rush_df["timestamp"])
                best_date = rush_ts.dt.date.value_counts().idxmax()
                day_rush = rush_df.loc[rush_ts.dt.date == best_date]
                t_start = day_rush["time_idx"].min()
            else:
                mid = group_df["time_idx"].median()
                t_start = mid - (window_size / 2)
            t_end = t_start + window_size

            plot_df = group_df[
                (group_df["time_idx"] >= t_start) & (group_df["time_idx"] <= t_end)
            ]
            if plot_df.empty:
                continue

            # Annotate subplot title with rush period
            if rush_period and i < len(fig.layout.annotations):
                fig.layout.annotations[i].update(
                    text=f"{group} \u2014 {rush_period}"
                )

            # Use timestamps for x-axis when available, raw time_idx otherwise
            x_vals = plot_df["timestamp"] if has_timestamps else plot_df["time_idx"]

            # --- Confidence band (P10 → P90) ---
            fig.add_trace(
                go.Scatter(
                    x=x_vals,
                    y=plot_df["pred_p10"],
                    mode="lines",
                    line=dict(width=0),
                    showlegend=False,
                    hoverinfo="skip",
                ),
                row=row, col=1,
            )
            fig.add_trace(
                go.Scatter(
                    x=x_vals,
                    y=plot_df["pred_p90"],
                    mode="lines",
                    line=dict(width=0),
                    fill="tonexty",
                    fillcolor="rgba(0, 57, 166, 0.2)",
                    name="90% Confidence",
                    legendgroup="confidence",
                    showlegend=(i == 0),
                    hovertemplate="P90: %{y:.2f}<extra>90% Confidence</extra>",
                ),
                row=row, col=1,
            )

            # --- P50 prediction ---
            fig.add_trace(
                go.Scatter(
                    x=x_vals,
                    y=plot_df["pred_p50"],
                    mode="lines",
                    line=dict(color="#0039A6", width=2),
                    name="Predicted (P50)",
                    legendgroup="p50",
                    showlegend=(i == 0),
                    hovertemplate="P50: %{y:.2f} min<extra>Predicted (P50)</extra>",
                ),
                row=row, col=1,
            )

            # --- Actuals ---
            fig.add_trace(
                go.Scatter(
                    x=x_vals,
                    y=plot_df["actual"],
                    mode="markers",
                    marker=dict(color="black", size=5, symbol="circle"),
                    name="Actual Headway",
                    legendgroup="actual",
                    showlegend=(i == 0),
                    hovertemplate="Actual: %{y:.2f} min<extra>Actual Headway</extra>",
                ),
                row=row, col=1,
            )

            # --- Axis labels ---
            x_title = "Time" if has_timestamps else "Time Index (minutes from dataset start)"
            fig.update_xaxes(title_text=x_title, row=row, col=1)
            fig.update_yaxes(title_text="Headway (min)", row=row, col=1)

            if has_timestamps:
                fig.update_xaxes(
                    tickformat="%b %d %H:%M",
                    dtick=30 * 60 * 1000,  # tick every 30 min (ms for datetime axes)
                    row=row, col=1,
                )

        fig.update_layout(
            height=350 * len(plot_groups),
            title_text="Rush Hour Headway Predictions — Model Evaluation (Test Set)",
            template="plotly_white",
            hovermode="x unified",
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        )

        return fig

@step(enable_cache=False)
def evaluate_model(
    model: TemporalFusionTransformer,
    test_dataset: TimeSeriesDataSet,
    config: DictConfig,
    time_anchor_iso: str = "",
) -> Tuple[
    Annotated[float, "test_mae"],
    Annotated[float, "test_smape"],
    Annotated[str, "rush_hour_plot_html"],
]:
    """
    Evaluates the model on the test set.

    Logs metrics to the same Vertex AI Experiment run that the training step
    created, by resuming the run via aiplatform.start_run(resume=True).
    This avoids the TensorBoard run name collision that occurs when two steps
    both declare experiment_tracker="vertex_tracker".
    """
    logger.info("Starting Model Evaluation on Test Set...")

    # 0. Create DataLoader from Dataset
    # We use validation batch size multiplier (usually larger than train bc no backprop)
    batch_size = config.training.batch_size * config.training.val_batch_size_multiplier
    
    test_loader = test_dataset.to_dataloader(
        train=False, 
        batch_size=batch_size, 
        num_workers=config.training.num_workers
    )
    
    # 1. Device Setup
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    model.eval()
    
    # 2. Generate Predictions
    raw_prediction = model.predict(test_loader, mode="raw", return_x=True)
    
    raw_output = raw_prediction.output
    predictions = raw_output["prediction"]
    x = raw_prediction.x
    
    # 3. Calculate Global Metrics
    actuals = x["decoder_target"].cpu()
    predictions_cpu = predictions.cpu()
    
    # P50 (Median) for point metrics
    p50_forecast = predictions_cpu[:, :, 1]
    
    mae_metric = MAE()
    smape_metric = SMAPE()
    
    mae = mae_metric(p50_forecast, actuals).item()
    smape = smape_metric(p50_forecast, actuals).item()
    
    logger.info(f"Global Test Metrics: MAE={mae:.4f}, sMAPE={smape:.4f}")
    
    # 4. Generate Rush Hour Visualizations
    logger.info("Generating Rush Hour Visualization...")

    viz = RushHourVisualizer(predictions, x, test_dataset, time_anchor_iso)
    
    # Generate the Rush Hour plot
    fig = viz.plot_rush_hour(window_size=180)  # 3 hour window

    # Capture interactive HTML for artifact output
    html_content = fig.to_html(full_html=True, include_plotlyjs="cdn")

    # 5. Log EVERYTHING to Vertex AI Experiments so it's visible from one place.
    #    Strategy:
    #      a) aiplatform.init() with experiment_tensorboard → links TB to experiment
    #      b) aiplatform.start_run(resume=True) → resumes the training step's run
    #      c) aiplatform.log_metrics() → scalar metrics appear in Experiments table
    #      d) Write TB events locally (interpretation figs, rush hour image)
    #      e) aiplatform.upload_tb_log() → pushes events to managed TB instance
    #         so clicking "Open TensorBoard" from Experiments UI shows everything
    try:
        context = get_step_context()
        raw_name = context.pipeline_run.name
        # Sanitize to match ZenML Vertex tracker _format_name():
        # lowercase, replace non-[a-z0-9-] with hyphen, truncate to 128
        run_name = re.sub(r"[^a-z0-9-]", "-", raw_name.strip().lower())[:128].rstrip("-")
    except Exception:
        run_name = None
        logger.warning("Could not retrieve ZenML step context for run name.")

    experiment_name = config.get("experiment_name", "headway_tft").lower().replace("_", "-")
    tb_resource = (
        f"projects/{config.infra.project_id}/locations/{config.infra.location}"
        f"/tensorboards/{config.infra.tensorboard_instance_id}"
    )

    # 5a. Log scalar metrics to Vertex AI Experiments
    try:
        aiplatform.init(
            project=config.infra.project_id,
            location=config.infra.location,
            experiment=experiment_name,
            experiment_tensorboard=tb_resource,
        )
        if run_name:
            aiplatform.start_run(run_name, resume=True)
            aiplatform.log_metrics({"test_mae": mae, "test_smape": smape})
            aiplatform.end_run()
            logger.info("Logged eval metrics to Vertex AI Experiment run: %s", run_name)
        else:
            logger.warning("No run name available — skipping Vertex AI metric logging.")
    except Exception as e:
        logger.warning("Failed to log eval metrics to Vertex AI Experiments: %s", e)

    # 5b. Write TensorBoard events (interpretation plots + eval images) to a local dir,
    #     then upload the entire dir to the managed TensorBoard instance.
    local_eval_dir = tempfile.mkdtemp(prefix="eval_tb_")

    try:
        writer = SummaryWriter(log_dir=local_eval_dir)
        writer.add_scalar("eval/test_mae", mae, global_step=0)
        writer.add_scalar("eval/test_smape", smape, global_step=0)

        # Rush hour plot as TensorBoard image
        try:
            png_path = os.path.join(local_eval_dir, "rush_hour_performance.png")
            fig.write_image(png_path, scale=2)
            from PIL import Image
            img = Image.open(png_path)
            img_array = np.array(img)
            writer.add_image(
                "eval/rush_hour_performance", img_array,
                global_step=0, dataformats="HWC",
            )
            logger.info("Added rush hour plot to TensorBoard events")
        except Exception as img_err:
            logger.warning("Could not add rush hour image to TB: %s", img_err)

        # TFT Interpretation: Feature Importance + Attention
        try:
            logger.info("Generating TFT interpretation plots...")
            interpretation = model.interpret_output(raw_output, reduction="sum")
            interp_figs = model.plot_interpretation(interpretation)
            for key, interp_fig in interp_figs.items():
                writer.add_figure(
                    f"eval/interpretation_{key}", interp_fig, global_step=0
                )
                plt.close(interp_fig)
            logger.info(
                "Logged %d interpretation plots to TensorBoard", len(interp_figs)
            )
        except Exception as interp_err:
            logger.warning("Could not generate interpretation plots: %s", interp_err)

        writer.close()

        # 5c. Upload local TB events to managed TensorBoard so they appear
        #     when clicking "Open TensorBoard" from the Experiments UI.
        tb_experiment_name = run_name or experiment_name
        try:
            aiplatform.upload_tb_log(
                tensorboard_id=config.infra.tensorboard_instance_id,
                tensorboard_experiment_name=tb_experiment_name,
                logdir=local_eval_dir,
                project=config.infra.project_id,
                location=config.infra.location,
                run_name_prefix="eval",
                description="Evaluation metrics, rush hour plots, TFT interpretation",
            )
            logger.info(
                "Uploaded eval TB events to managed TensorBoard (experiment=%s)",
                tb_experiment_name,
            )
        except Exception as upload_err:
            logger.warning("Failed to upload TB events to managed TensorBoard: %s", upload_err)

    except Exception as e:
        logger.warning("Error during TensorBoard artifact creation: %s", e)
    finally:
        import shutil
        shutil.rmtree(local_eval_dir, ignore_errors=True)

    return mae, smape, html_content
