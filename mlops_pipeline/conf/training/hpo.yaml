# HPO Training Profile
# Used alongside the 'model' config during Vizier trials.
# Designed for speed: fewer epochs, tighter early stopping, partial data usage.

# Hardware & Loader Parameters
batch_size: 128
val_batch_size_multiplier: 10
num_workers: 8
pin_memory: true

# Trainer Parameters
max_epochs: 15                    # Reduced from 100
gradient_clip_val: 0.1
limit_train_batches: 0.5          # Use only 50% of data per epoch for speed
precision: "bf16-mixed"
accelerator: "auto"
devices: 1
enable_model_summary: false       # Reduce log noise in trials

# Early Stopping (Aggressive)
early_stopping_patience: 5
early_stopping_min_delta: 1e-4