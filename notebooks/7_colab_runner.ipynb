{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Environment Detection & Drive Mounting\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab. Mounting Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running Locally.\")\n",
    "\n",
    "# 2. Path Setup\n",
    "# We need to find the project root to import 'src'\n",
    "if IN_COLAB:\n",
    "    # Common path pattern; adjust 'headway-prediction' if your folder name is different on Drive\n",
    "    SEARCH_PATH = '/content/drive/MyDrive'\n",
    "    PROJECT_DIR = None\n",
    "    \n",
    "    # Simple search for the project folder\n",
    "    for root, dirs, files in os.walk(SEARCH_PATH):\n",
    "        if 'src' in dirs and 'requirements.txt' in files:\n",
    "            PROJECT_DIR = root\n",
    "            break\n",
    "            \n",
    "    if PROJECT_DIR:\n",
    "        print(f\"Project root found at: {PROJECT_DIR}\")\n",
    "        os.chdir(PROJECT_DIR)\n",
    "        sys.path.append(PROJECT_DIR)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not find project root in Drive. Make sure 'headway-prediction' is synced.\")\n",
    "else:\n",
    "    # Assume we are in 'notebooks/' and project root is one level up\n",
    "    PROJECT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    sys.path.append(PROJECT_DIR)\n",
    "    print(f\"Running locally from: {PROJECT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ff273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable Mixed Precision to speed up training on T4/A100/V100 GPUs\n",
    "# This uses float16 for calculations but float32 for variable stability.\n",
    "# Expected speedup: 2x-3x\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "print(f\"Global Mixed Precision Policy set to: {policy.name}\")\n",
    "print(\"Compute dtype:\", policy.compute_dtype)\n",
    "print(\"Variable dtype:\", policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import Config\n",
    "from src.data.dataset import SubwayDataGenerator\n",
    "from src.models.st_covnet import HeadwayConvLSTM\n",
    "\n",
    "# 3. DEBUG FLAG\n",
    "# Set to True for a quick 30-second \"Smoke Test\"\n",
    "# Set to False for full production training\n",
    "DEBUG_MODE = False\n",
    "\n",
    "print(f\"--- DEBUG MODE: {DEBUG_MODE} ---\")\n",
    "\n",
    "# Initialize Config\n",
    "config = Config()\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    print(\"WARNING: Overriding config for fast smoke test.\")\n",
    "    config.BATCH_SIZE = 32     # Smaller batch for stability\n",
    "    config.EPOCHS = 1          # Single pass to prove end-to-end flow\n",
    "    config.FILTERS = 16        # Tiny model for speed\n",
    "    \n",
    "    # If using Drive, paths are already set by os.chdir(PROJECT_DIR) + Config logic\n",
    "    # but we verify them anyway\n",
    "    print(f\"Data Dir: {config.DATA_DIR}\")\n",
    "\n",
    "# 4. Load Data\n",
    "gen = SubwayDataGenerator(config)\n",
    "gen.load_data()\n",
    "\n",
    "# 5. Create Datasets\n",
    "if DEBUG_MODE:\n",
    "    # Use a tiny slice of data (e.g., first 1000 samples)\n",
    "    print(\"Creating tiny DEBUG dataset...\")\n",
    "    train_ds = gen.make_dataset(start_index=0, end_index=1000)\n",
    "    val_ds = gen.make_dataset(start_index=1000, end_index=1200)\n",
    "else:\n",
    "    # Full data split\n",
    "    # Requested split: 60% Train, 20% Val, 20% Test\n",
    "    total_len = len(gen.headway_data)\n",
    "    train_end = int(total_len * 0.6)\n",
    "    val_end = int(total_len * 0.8)\n",
    "    \n",
    "    print(f\"Creating FULL datasets (N={total_len})...\")\n",
    "    print(f\"Train: 0-{train_end} | Val: {train_end}-{val_end}\")\n",
    "    \n",
    "    train_ds = gen.make_dataset(start_index=0, end_index=train_end)\n",
    "    val_ds = gen.make_dataset(start_index=train_end, end_index=val_end)\n",
    "\n",
    "print(\"Datasets ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build and Compile Model\n",
    "from src.metrics import rmse_seconds, mae_seconds\n",
    "\n",
    "model_builder = HeadwayConvLSTM(config)\n",
    "model = model_builder.build_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "    loss='mse',\n",
    "    metrics=['mae', rmse_seconds, mae_seconds]\n",
    ")\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b25da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Evaluation\n",
    "from src.evaluator import Evaluator\n",
    "\n",
    "print(\"running evaluation checks...\")\n",
    "evaluator = Evaluator(config)\n",
    "\n",
    "# 1. Plot Loss Curves\n",
    "evaluator.plot_loss(history)\n",
    "\n",
    "# 2. Visualize Predictions\n",
    "# Using the updated method in src/evaluator.py which now supports the \"Micrograph\" style\n",
    "print(\"Generating spatiotemporal visualization...\")\n",
    "evaluator.plot_spatiotemporal_prediction(model, val_ds, sample_idx=0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
