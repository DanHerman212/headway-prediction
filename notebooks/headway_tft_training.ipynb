{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf408fa",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ac4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f1/dmgdbt5j5nb5ht5_vfxmr8gm0000gn/T/ipykernel_68953/194670054.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version: 2.10.0\n",
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer,  Baseline, QuantileLoss\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# supress warnings to keep the output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "print(f\"Pytorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee18535",
   "metadata": {},
   "source": [
    "## Observation of Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e2e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (75197, 20)\n",
      "Date range: 2025-07-18T08:53:57+00:00 to 2026-01-19T10:43:23+00:00\n",
      "\n",
      "Missing Values:\n",
      "trip_uid                   0\n",
      "trip_date                  0\n",
      "arrival_time               0\n",
      "timestamp                  0\n",
      "group_id                   0\n",
      "route_id                   0\n",
      "direction                  0\n",
      "stop_id                    0\n",
      "time_idx                   0\n",
      "day_of_week                0\n",
      "hour_sin                   0\n",
      "hour_cos                   0\n",
      "regime_id                  0\n",
      "track_id                   0\n",
      "service_headway          260\n",
      "preceding_train_gap        2\n",
      "empirical_median           0\n",
      "travel_time_14th          50\n",
      "travel_time_23rd       23448\n",
      "travel_time_34th          29\n",
      "dtype: int64\n",
      "\n",
      "Column Data Types:\n",
      "trip_uid                object\n",
      "trip_date               object\n",
      "arrival_time            object\n",
      "timestamp              float64\n",
      "group_id                object\n",
      "route_id                object\n",
      "direction               object\n",
      "stop_id                 object\n",
      "time_idx                 int64\n",
      "day_of_week              int64\n",
      "hour_sin               float64\n",
      "hour_cos               float64\n",
      "regime_id               object\n",
      "track_id                object\n",
      "service_headway        float64\n",
      "preceding_train_gap    float64\n",
      "empirical_median       float64\n",
      "travel_time_14th       float64\n",
      "travel_time_23rd       float64\n",
      "travel_time_34th       float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_uid</th>\n",
       "      <th>trip_date</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>direction</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>regime_id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>service_headway</th>\n",
       "      <th>preceding_train_gap</th>\n",
       "      <th>empirical_median</th>\n",
       "      <th>travel_time_14th</th>\n",
       "      <th>travel_time_23rd</th>\n",
       "      <th>travel_time_34th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1752815340_A..S53R</td>\n",
       "      <td>2025-07-18 05:09:00+00:00</td>\n",
       "      <td>2025-07-18T09:42:36+00:00</td>\n",
       "      <td>1.752832e+09</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>A32S</td>\n",
       "      <td>29213862</td>\n",
       "      <td>4</td>\n",
       "      <td>0.566406</td>\n",
       "      <td>-0.824126</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.366667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1752816300_A..S58R</td>\n",
       "      <td>2025-07-18 05:25:00+00:00</td>\n",
       "      <td>2025-07-18T10:00:11+00:00</td>\n",
       "      <td>1.752833e+09</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>A32S</td>\n",
       "      <td>29213880</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>17.583333</td>\n",
       "      <td>17.583333</td>\n",
       "      <td>10.633333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1752817230_A..S57R</td>\n",
       "      <td>2025-07-18 05:40:30+00:00</td>\n",
       "      <td>2025-07-18T10:12:37+00:00</td>\n",
       "      <td>1.752834e+09</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>A32S</td>\n",
       "      <td>29213892</td>\n",
       "      <td>4</td>\n",
       "      <td>0.453990</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>12.433333</td>\n",
       "      <td>12.433333</td>\n",
       "      <td>10.633333</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1752818010_A..S58R</td>\n",
       "      <td>2025-07-18 05:53:30+00:00</td>\n",
       "      <td>2025-07-18T10:23:36+00:00</td>\n",
       "      <td>1.752834e+09</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>A32S</td>\n",
       "      <td>29213903</td>\n",
       "      <td>4</td>\n",
       "      <td>0.410719</td>\n",
       "      <td>-0.911762</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>10.983333</td>\n",
       "      <td>10.983333</td>\n",
       "      <td>10.633333</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1752818580_A..S57R</td>\n",
       "      <td>2025-07-18 06:03:00+00:00</td>\n",
       "      <td>2025-07-18T10:32:36+00:00</td>\n",
       "      <td>1.752835e+09</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>A32S</td>\n",
       "      <td>29213912</td>\n",
       "      <td>4</td>\n",
       "      <td>0.374607</td>\n",
       "      <td>-0.927184</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.633333</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             trip_uid                  trip_date               arrival_time  \\\n",
       "0  1752815340_A..S53R  2025-07-18 05:09:00+00:00  2025-07-18T09:42:36+00:00   \n",
       "1  1752816300_A..S58R  2025-07-18 05:25:00+00:00  2025-07-18T10:00:11+00:00   \n",
       "2  1752817230_A..S57R  2025-07-18 05:40:30+00:00  2025-07-18T10:12:37+00:00   \n",
       "3  1752818010_A..S58R  2025-07-18 05:53:30+00:00  2025-07-18T10:23:36+00:00   \n",
       "4  1752818580_A..S57R  2025-07-18 06:03:00+00:00  2025-07-18T10:32:36+00:00   \n",
       "\n",
       "      timestamp group_id route_id direction stop_id  time_idx  day_of_week  \\\n",
       "0  1.752832e+09  A_South        A         S    A32S  29213862            4   \n",
       "1  1.752833e+09  A_South        A         S    A32S  29213880            4   \n",
       "2  1.752834e+09  A_South        A         S    A32S  29213892            4   \n",
       "3  1.752834e+09  A_South        A         S    A32S  29213903            4   \n",
       "4  1.752835e+09  A_South        A         S    A32S  29213912            4   \n",
       "\n",
       "   hour_sin  hour_cos regime_id track_id  service_headway  \\\n",
       "0  0.566406 -0.824126       Day       A3        12.666667   \n",
       "1  0.500000 -0.866025       Day       A3        17.583333   \n",
       "2  0.453990 -0.891007       Day       A3        12.433333   \n",
       "3  0.410719 -0.911762       Day       A3        10.983333   \n",
       "4  0.374607 -0.927184       Day       A3         9.000000   \n",
       "\n",
       "   preceding_train_gap  empirical_median  travel_time_14th  travel_time_23rd  \\\n",
       "0                  NaN         17.366667          2.000000               NaN   \n",
       "1            17.583333         10.633333          2.000000               NaN   \n",
       "2            12.433333         10.633333          1.833333               NaN   \n",
       "3            10.983333         10.633333          1.833333               NaN   \n",
       "4             9.000000         10.633333          1.833333               NaN   \n",
       "\n",
       "   travel_time_34th  \n",
       "0          4.416667  \n",
       "1          4.100000  \n",
       "2          3.933333  \n",
       "3          4.016667  \n",
       "4          3.916667  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define path to the processed parquet file\n",
    "# note: adjust relative path if your notebook location differs\n",
    "DATA_PATH = \"../local_artifacts/processed_data/training_data.parquet\"\n",
    "\n",
    "# load the df\n",
    "data = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "# display basic statistics\n",
    "print(f\"Dataset Shape: {data.shape}\")\n",
    "print(f\"Date range: {data['arrival_time'].min()} to {data['arrival_time'].max()}\")\n",
    "\n",
    "# check for nulls in potential target/group columns\n",
    "# we assume 'headway' is the target column\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Inspect column types\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# preview\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b24005",
   "metadata": {},
   "source": [
    "## Apply Imputation and 2nd Level Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94c7c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Shape:(74937, 20)\n",
      "Max time_idx (Sequence length): 29661\n",
      "Travel time 23rd Unique values: [ 3.2         3.65       -1.          3.48333333  3.58333333]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_uid</th>\n",
       "      <th>trip_date</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>direction</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>regime_id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>service_headway</th>\n",
       "      <th>preceding_train_gap</th>\n",
       "      <th>empirical_median</th>\n",
       "      <th>travel_time_14th</th>\n",
       "      <th>travel_time_23rd</th>\n",
       "      <th>travel_time_34th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23270</th>\n",
       "      <td>1752813480_A..S74X043</td>\n",
       "      <td>2025-07-18 04:38:00+00:00</td>\n",
       "      <td>2025-07-18T09:09:03+00:00</td>\n",
       "      <td>1.752830e+09</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>A32S</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.678801</td>\n",
       "      <td>-0.734323</td>\n",
       "      <td>Day</td>\n",
       "      <td>A1</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>17.366667</td>\n",
       "      <td>1.933333</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23273</th>\n",
       "      <td>1752814680_A..S74X043</td>\n",
       "      <td>2025-07-18 04:58:00+00:00</td>\n",
       "      <td>2025-07-18T09:29:56+00:00</td>\n",
       "      <td>1.752831e+09</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>A32S</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.612217</td>\n",
       "      <td>-0.790690</td>\n",
       "      <td>Day</td>\n",
       "      <td>A1</td>\n",
       "      <td>20.883333</td>\n",
       "      <td>6.983333</td>\n",
       "      <td>17.366667</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1752815340_A..S53R</td>\n",
       "      <td>2025-07-18 05:09:00+00:00</td>\n",
       "      <td>2025-07-18T09:42:36+00:00</td>\n",
       "      <td>1.752832e+09</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>A32S</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.566406</td>\n",
       "      <td>-0.824126</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.366667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>4.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1752816300_A..S58R</td>\n",
       "      <td>2025-07-18 05:25:00+00:00</td>\n",
       "      <td>2025-07-18T10:00:11+00:00</td>\n",
       "      <td>1.752833e+09</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>A32S</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>17.583333</td>\n",
       "      <td>17.583333</td>\n",
       "      <td>10.633333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>4.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1752817230_A..S57R</td>\n",
       "      <td>2025-07-18 05:40:30+00:00</td>\n",
       "      <td>2025-07-18T10:12:37+00:00</td>\n",
       "      <td>1.752834e+09</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>A32S</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.453990</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>12.433333</td>\n",
       "      <td>12.433333</td>\n",
       "      <td>10.633333</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>3.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    trip_uid                  trip_date  \\\n",
       "23270  1752813480_A..S74X043  2025-07-18 04:38:00+00:00   \n",
       "23273  1752814680_A..S74X043  2025-07-18 04:58:00+00:00   \n",
       "0         1752815340_A..S53R  2025-07-18 05:09:00+00:00   \n",
       "1         1752816300_A..S58R  2025-07-18 05:25:00+00:00   \n",
       "2         1752817230_A..S57R  2025-07-18 05:40:30+00:00   \n",
       "\n",
       "                    arrival_time     timestamp group_id route_id direction  \\\n",
       "23270  2025-07-18T09:09:03+00:00  1.752830e+09  A_South        A         S   \n",
       "23273  2025-07-18T09:29:56+00:00  1.752831e+09  A_South        A         S   \n",
       "0      2025-07-18T09:42:36+00:00  1.752832e+09  A_South        A         S   \n",
       "1      2025-07-18T10:00:11+00:00  1.752833e+09  A_South        A         S   \n",
       "2      2025-07-18T10:12:37+00:00  1.752834e+09  A_South        A         S   \n",
       "\n",
       "      stop_id  time_idx  day_of_week  hour_sin  hour_cos regime_id track_id  \\\n",
       "23270    A32S         0            4  0.678801 -0.734323       Day       A1   \n",
       "23273    A32S         1            4  0.612217 -0.790690       Day       A1   \n",
       "0        A32S         2            4  0.566406 -0.824126       Day       A3   \n",
       "1        A32S         3            4  0.500000 -0.866025       Day       A3   \n",
       "2        A32S         4            4  0.453990 -0.891007       Day       A3   \n",
       "\n",
       "       service_headway  preceding_train_gap  empirical_median  \\\n",
       "23270        15.100000            15.100000         17.366667   \n",
       "23273        20.883333             6.983333         17.366667   \n",
       "0            12.666667             0.000000         17.366667   \n",
       "1            17.583333            17.583333         10.633333   \n",
       "2            12.433333            12.433333         10.633333   \n",
       "\n",
       "       travel_time_14th  travel_time_23rd  travel_time_34th  \n",
       "23270          1.933333              3.20          4.766667  \n",
       "23273          2.200000              3.65          4.916667  \n",
       "0              2.000000             -1.00          4.416667  \n",
       "1              2.000000             -1.00          4.100000  \n",
       "2              1.833333             -1.00          3.933333  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with missing headways\n",
    "data = data.dropna(subset=['service_headway'])\n",
    "\n",
    "# 2. structural missingness\n",
    "# -1.0 indicates station skipped (express) distinct from 0.0\n",
    "data['travel_time_23rd'] = data['travel_time_23rd'].fillna(-1.0)\n",
    "\n",
    "# fill other numerical gaps with 0.0 (safe for now)\n",
    "data['preceding_train_gap'] = data['preceding_train_gap'].fillna(0.0)\n",
    "data['travel_time_14th'] = data['travel_time_14th'].fillna(0.0)\n",
    "data['travel_time_34th'] = data['travel_time_34th'].fillna(0.0)\n",
    "\n",
    "# 3. ensure categoricals are strings\n",
    "cat_cols = ['group_id','route_id','direction','regime_id','track_id']\n",
    "for col in cat_cols:\n",
    "    data[col] = data[col].astype(str)\n",
    "\n",
    "# 4. stricktly re-index time_idx\n",
    "# sort ensures we process trains in correct arrival order per group\n",
    "data = data.sort_values(['group_id', 'arrival_time'])\n",
    "\n",
    "# create a continuous counter for each group\n",
    "# this tells TFT specific train order\n",
    "data[\"time_idx\"] = data.groupby('group_id').cumcount()\n",
    "\n",
    "print(f\"Cleaned Shape:{data.shape}\")\n",
    "print(f\"Max time_idx (Sequence length): {data['time_idx'].max()}\")\n",
    "# verify we still have distinct -1.0 signal\n",
    "print(f\"Travel time 23rd Unique values: {data['travel_time_23rd'].unique()[:5]}\")\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6160e505",
   "metadata": {},
   "source": [
    "## Time Based Splits and ML Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f23f743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 49743\n",
      "Val Rows (with context): 12304\n",
      "Test Rows (with context): 12588\n",
      "Train Batches: 776\n",
      "Val Batches: 20\n",
      "Test Batches 20\n",
      "Total Batches (Train/Val/Test): 776 / 20 / 20\n",
      "\n",
      "Feature names: ['route_id', 'direction', 'regime_id', 'track_id']\n",
      "Encoder Shape (Batch, Time, Features): torch.Size([64, 12, 13])\n"
     ]
    }
   ],
   "source": [
    "# 1 define date splits\n",
    "# converting string to datetime for comparison\n",
    "if data['arrival_time'].dtype == 'object':\n",
    "    data['arrival_time_dt'] = pd.to_datetime(data['arrival_time'])\n",
    "else:\n",
    "    data['arrival_time_dt'] = data['arrival_time']\n",
    "\n",
    "# define boundaries \n",
    "train_end_date = pd.Timestamp(\"2025-11-18\", tz=\"UTC\")\n",
    "val_end_date = pd.Timestamp(\"2025-12-18\", tz=\"UTC\")\n",
    "test_end_date = pd.Timestamp(\"2026-01-18\", tz=\"UTC\")\n",
    "\n",
    "\n",
    "# 2 helper function to create dataset inputs with correct lookback context\n",
    "def get_slice_with_lookback(full_df, start_date, end_date, lookback=12):\n",
    "    \"\"\"\n",
    "    Return rows between start_date and end_date,\n",
    "    Plus the last 'lookback' rows before start_date for EACH group\n",
    "    \"\"\"\n",
    "    # get the core data for the period\n",
    "    mask = (full_df['arrival_time_dt'] >= start_date) & (full_df['arrival_time_dt'] < end_date)\n",
    "    core_df = full_df[mask]\n",
    "\n",
    "    # we need to preprend the last 'lookback' rows from BEFORE start_date for EACH group\n",
    "    # to serve as the history for the first few predictions\n",
    "    prior_df = full_df[full_df['arrival_time_dt'] < start_date]\n",
    "    pre_data = []\n",
    "\n",
    "    for g_id, group in prior_df.groupby('group_id'):\n",
    "        pre_data.append(group.tail(lookback))\n",
    "    \n",
    "    if pre_data:\n",
    "        lookback_df = pd.concat(pre_data)\n",
    "        # concat and sort to ensure time continuity per group\n",
    "        return pd.concat([lookback_df, core_df]).sort_values(['group_id','time_idx'])\n",
    "    return core_df\n",
    "\n",
    "# 3 create physical dataframes\n",
    "train_df = data[data['arrival_time_dt']< train_end_date]\n",
    "val_df_input = get_slice_with_lookback(data, train_end_date, val_end_date, lookback=12)\n",
    "test_df_input = get_slice_with_lookback(data, val_end_date, test_end_date, lookback=12)\n",
    "\n",
    "print(f\"Train rows: {len(train_df)}\")\n",
    "print(f\"Val Rows (with context): {len(val_df_input)}\")\n",
    "print(f\"Test Rows (with context): {len(test_df_input)}\")\n",
    "\n",
    "# create datasets\n",
    "max_prediction_length = 1\n",
    "max_encoder_length = 12\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"service_headway\",\n",
    "    group_ids=[\"group_id\"],\n",
    "    min_encoder_length=10,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"route_id\",\"direction\"],\n",
    "    time_varying_known_categoricals=[\"regime_id\", \"track_id\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"hour_sin\", \"hour_cos\", \"empirical_median\"],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"service_headway\",\n",
    "        \"preceding_train_gap\",\n",
    "        \"travel_time_14th\",\n",
    "        \"travel_time_23rd\",\n",
    "        \"travel_time_34th\"\n",
    "    ],\n",
    "target_normalizer=GroupNormalizer(\n",
    "    groups=[\"group_id\"], transformation=\"softplus\"\n",
    "),\n",
    "add_relative_time_idx=True,\n",
    "add_target_scales=True,\n",
    "add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# use from_dataset with sliced dataframes\n",
    "# we do not use min_prediction_idx because the df slicing handled the time separation\n",
    "validation = TimeSeriesDataSet.from_dataset(training, val_df_input, predict=False, stop_randomization=True)\n",
    "test = TimeSeriesDataSet.from_dataset(training, test_df_input, predict=False, stop_randomization=True)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "test_dataloader = test.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "print(f\"Train Batches: {len(train_dataloader)}\")\n",
    "print(f\"Val Batches: {len(val_dataloader)}\")\n",
    "print(f\"Test Batches {len(test_dataloader)}\")\n",
    "print(f\"Total Batches (Train/Val/Test): {len(train_dataloader)} / {len(val_dataloader)} / {len(test_dataloader)}\")\n",
    "\n",
    "# visualize what the model sees\n",
    "# this helps debug if features are being scaled correctly\n",
    "x, y = next(iter(train_dataloader))\n",
    "print(\"\\nFeature names:\", training.static_categoricals + training.time_varying_known_categoricals)\n",
    "# x keys: \"encoder_cat\", \"encoder_cont\", \"decoder_cat\", \"decoder_cont\"\n",
    "print(\"Encoder Shape (Batch, Time, Features):\", x['encoder_cont'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae8d7a",
   "metadata": {},
   "source": [
    "## Model Architecture and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fa537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch TensorBoard pointing to the logs directory we configured above\n",
    "# This will open a panel below. It might be empty at first until training starts writing logs.\n",
    "%tensorboard --logdir tensorboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 configure the Temporal Fusion Transformer\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.001,    # start with resonable default\n",
    "    hidden_size=32,        # critical for small dataset, keep params low\n",
    "    attention_head_size=4,  # sufficient for simple temporal pattersn\n",
    "    dropout=0.3,            # high dropout to force generalization\n",
    "    hidden_continuous_size=16, #reduce preojection size\n",
    "    output_size=3,          # 3 quantiles [0.1, 0.5, 0.9]\n",
    "    loss=QuantileLoss([0.1, 0.5, 0.9]),    # standard loss for probabilistic forecasting\n",
    "    log_interval=10,        # logging frequency\n",
    "    reduce_on_plateau_patience=4, # reduce LR if loss doesn't improve for 4 epochs\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# 2 configure training callbacks\n",
    "# early stopping prevents overfitting by monitoring validation loss\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=15, # aggressive for small dataset\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# configure logger - using the explicit tensorboard_logs directory\n",
    "logger = TensorBoardLogger(\"tensorboard_logs\", name=\"headway_tft\")\n",
    "\n",
    "# initialize trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\", # uses gpu if found\n",
    "    devices=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1, # critical: prevents exploding gradients from outliers\n",
    "    callbacks=[lr_logger,early_stop_callback],\n",
    "    logger=logger,\n",
    "    limit_train_batches=1.0\n",
    ")\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42283488",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "Part A: Loss Visualization (The Developer's View)<br>\n",
    "Standard TensorBoard-style plot of Train vs Val Loss.<br>\n",
    "<br>\n",
    "Part B: Performance Metrics (The Data Scientist's View)<br>\n",
    "We will run the model on the Test Set (which it has never seen) and compute:<br>\n",
    "<br>\n",
    "MAE (in minutes)<br>\n",
    "sMAPE (Percentage error)<br>\n",
    "Part C: Representative Predictions (The Operator's View)<br>\n",
    "We will select a few specific examples from the test set to plot:<br>\n",
    "<br>\n",
    "Routine: A standard rush-hour sequence.<br>\n",
    "Disruption: A case where preceding_train_gap was high (interaction delay).<br>\n",
    "Overnight: A case from the \"Regime Shift\" (22:00-23:00) to see if it widens confidence intervals as requested.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06663005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load best model\n",
    "best_model_path = None\n",
    "if getattr(trainer, \"checkpoint_callbacks\", None):\n",
    "    for cb in trainer.checkpoint_callbacks:\n",
    "        path = getattr(cb, \"best_model_path\", None)\n",
    "        if path:\n",
    "            best_model_path = path\n",
    "            print(f\"Best model found at: {best_model_path}\")\n",
    "            break\n",
    "\n",
    "if best_model_path is None and getattr(trainer, \"checkpoint_callback\", None):\n",
    "    best_model_path = getattr(trainer.checkpoint_callback, \"best_model_path\", None)\n",
    "    print(f\"Best model found via fallback: {best_model_path}\")\n",
    "\n",
    "if best_model_path is None:\n",
    "    raise ValueError(\"No best model checkpoint found. Did training fail?\")\n",
    "\n",
    "# Load model\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# Performance: Move to GPU if available for inference\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Inference Device: {device}\")\n",
    "best_tft.to(device)\n",
    "\n",
    "# 2. Global Predictions\n",
    "print(\"Generating predictions on full test set...\")\n",
    "raw_prediction = best_tft.predict(test_dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "# Unpack tuple if necessary\n",
    "if isinstance(raw_prediction, tuple) or isinstance(raw_prediction, list):\n",
    "    x = raw_prediction[1]\n",
    "    raw_prediction = raw_prediction[0]\n",
    "else:\n",
    "    x = raw_prediction\n",
    "\n",
    "# 3. Metrics (Move to CPU for calculation)\n",
    "print(\"Calculating Metrics...\")\n",
    "predictions = raw_prediction.prediction.cpu()\n",
    "actuals = x[\"decoder_target\"].cpu()\n",
    "\n",
    "mae_metric = MAE()\n",
    "smape_metric = SMAPE()\n",
    "quantile_loss_metric = QuantileLoss(quantiles=[0.1, 0.5, 0.9])\n",
    "\n",
    "loss_val = quantile_loss_metric(predictions, actuals)\n",
    "p50_forecast = predictions[:, :, 1] # Median\n",
    "mae_val = mae_metric(p50_forecast, actuals)\n",
    "smape_val = smape_metric(p50_forecast, actuals)\n",
    "\n",
    "print(f\"\\n--- Global Test Metrics ---\")\n",
    "print(f\"Quantile Loss: {loss_val.mean().item():.4f}\")\n",
    "print(f\"MAE (P50):     {mae_val.mean().item():.4f} minutes\")\n",
    "print(f\"sMAPE (P50):   {smape_val.mean().item():.4f}\")\n",
    "\n",
    "# Calibration\n",
    "p10 = predictions[:, :, 0]\n",
    "p90 = predictions[:, :, 2]\n",
    "p10_coverage = (actuals <= p10).float().mean()\n",
    "p90_coverage = (actuals <= p90).float().mean()\n",
    "print(f\"P10 Coverage:  {p10_coverage.item():.3f} (Target 0.10)\")\n",
    "print(f\"P90 Coverage:  {p90_coverage.item():.3f} (Target 0.90)\")\n",
    "\n",
    "# --- NEW SECTION: Group Breakdown ---\n",
    "print(\"\\n--- Metrics by Group ID ---\")\n",
    "if \"groups\" in x:\n",
    "    # Get group IDs from tensor\n",
    "    group_ids = x[\"groups\"].cpu().view(-1).numpy()\n",
    "    \n",
    "    # Use the encoder from training object to map int -> string name\n",
    "    group_encoder = training.categorical_encoders[\"group_id\"]\n",
    "    \n",
    "    # Create DataFrame for aggregation\n",
    "    # We calculate the absolute error per sample first\n",
    "    abs_errors = torch.abs(p50_forecast - actuals).mean(dim=1).numpy()\n",
    "    \n",
    "    res_df = pd.DataFrame({\n",
    "        \"group_idx\": group_ids,\n",
    "        \"mae\": abs_errors\n",
    "    })\n",
    "    \n",
    "    # Map index to name\n",
    "    unique_idxs = np.unique(group_ids)\n",
    "    # inverse_transform takes a long tensor\n",
    "    decoded_names = group_encoder.inverse_transform(torch.tensor(unique_idxs, dtype=torch.long))\n",
    "\n",
    "    idx_map = dict(zip(unique_idxs, decoded_names))\n",
    "    res_df[\"group_name\"] = res_df[\"group_idx\"].map(idx_map)\n",
    "    \n",
    "    # Group By and Aggregate\n",
    "    grouped_stats = res_df.groupby(\"group_name\")[\"mae\"].agg(['mean', 'count']).rename(columns={'mean': 'MAE', 'count': 'Samples'})\n",
    "    print(grouped_stats.sort_values(\"MAE\", ascending=False))\n",
    "else:\n",
    "    print(\"Could not find group information in input tensors.\")\n",
    "\n",
    "# 4. Visualization Sample\n",
    "print(\"\\nVisualizing Single Prediction (Manual Plot):\")\n",
    "idx = 0 \n",
    "\n",
    "encoder_target = x['encoder_target'][idx].cpu()\n",
    "decoder_target = x['decoder_target'][idx].cpu()\n",
    "prediction_p10 = predictions[idx, :, 0].cpu()\n",
    "prediction_p50 = predictions[idx, :, 1].cpu()\n",
    "prediction_p90 = predictions[idx, :, 2].cpu()\n",
    "\n",
    "enc_len = len(encoder_target)\n",
    "dec_len = len(decoder_target)\n",
    "history_time = range(-enc_len, 0)\n",
    "future_time = range(0, dec_len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_time, encoder_target, label=\"History\", color=\"gray\", marker=\".\")\n",
    "plt.plot(future_time, decoder_target, label=\"Actual Future\", color=\"black\", marker=\"o\")\n",
    "plt.plot(future_time, prediction_p50, label=\"Forecast P50\", color=\"blue\", marker=\"x\")\n",
    "plt.fill_between(future_time, prediction_p10, prediction_p90, color=\"blue\", alpha=0.2, label=\"Confidence (P10-P90)\")\n",
    "plt.title(f\"Test Sample #{idx}: Headway Prediction\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
