{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf408fa",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ac4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f1/dmgdbt5j5nb5ht5_vfxmr8gm0000gn/T/ipykernel_18439/2661648363.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version: 2.10.0\n",
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, TQDMProgressBar\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer,  Baseline, QuantileLoss\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# supress warnings to keep the output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "print(f\"Pytorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee18535",
   "metadata": {},
   "source": [
    "## Observation of Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e2e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (74937, 21)\n",
      "Date range: 2025-07-18T09:09:03+00:00 to 2026-01-19T10:43:23+00:00\n",
      "\n",
      "Missing Values:\n",
      "trip_uid                          0\n",
      "arrival_time                      0\n",
      "group_id                          0\n",
      "route_id                          0\n",
      "time_idx                          0\n",
      "day_of_week                       0\n",
      "hour_sin                          0\n",
      "hour_cos                          0\n",
      "regime_id                         0\n",
      "track_id                          0\n",
      "service_headway                   0\n",
      "preceding_train_gap               1\n",
      "preceding_route_id                1\n",
      "empirical_median                  0\n",
      "upstream_headway_14th            49\n",
      "travel_time_14th                 48\n",
      "travel_time_14th_deviation       48\n",
      "travel_time_23rd              23418\n",
      "travel_time_23rd_deviation    23418\n",
      "travel_time_34th                 27\n",
      "travel_time_34th_deviation       27\n",
      "dtype: int64\n",
      "\n",
      "Column Data Types:\n",
      "trip_uid                       object\n",
      "arrival_time                   object\n",
      "group_id                       object\n",
      "route_id                       object\n",
      "time_idx                        int64\n",
      "day_of_week                     int64\n",
      "hour_sin                      float64\n",
      "hour_cos                      float64\n",
      "regime_id                      object\n",
      "track_id                       object\n",
      "service_headway               float64\n",
      "preceding_train_gap           float64\n",
      "preceding_route_id             object\n",
      "empirical_median              float64\n",
      "upstream_headway_14th         float64\n",
      "travel_time_14th              float64\n",
      "travel_time_14th_deviation    float64\n",
      "travel_time_23rd              float64\n",
      "travel_time_23rd_deviation    float64\n",
      "travel_time_34th              float64\n",
      "travel_time_34th_deviation    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# define path to the processed parquet file\n",
    "# note: adjust relative path if your notebook location differs\n",
    "DATA_PATH = \"../local_artifacts/processed_data/training_data.parquet\"\n",
    "\n",
    "# load the df\n",
    "try:\n",
    "    data = pd.read_parquet(DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {DATA_PATH}. Please run the pipeline first.\")\n",
    "    # create dummy data so the cell doesn't crash completely during static analysis checks\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "if not data.empty:\n",
    "    # display basic statistics\n",
    "    print(f\"Dataset Shape: {data.shape}\")\n",
    "    \n",
    "    if 'arrival_time' in data.columns:\n",
    "        print(f\"Date range: {data['arrival_time'].min()} to {data['arrival_time'].max()}\")\n",
    "    \n",
    "    # check for nulls in potential target/group columns\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(data.isnull().sum())\n",
    "\n",
    "    # Inspect column types\n",
    "    print(\"\\nColumn Data Types:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # preview\n",
    "    data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b24005",
   "metadata": {},
   "source": [
    "## Apply Imputation and 2nd Level Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94c7c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Shape:(74937, 23)\n",
      "Max time_idx (Sequence length): 266494\n",
      "Valid Mean 23rd: 3.77\n",
      "\n",
      "Remaining Nulls:\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_uid</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>group_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>regime_id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>...</th>\n",
       "      <th>empirical_median</th>\n",
       "      <th>upstream_headway_14th</th>\n",
       "      <th>travel_time_14th</th>\n",
       "      <th>travel_time_14th_deviation</th>\n",
       "      <th>travel_time_23rd</th>\n",
       "      <th>travel_time_23rd_deviation</th>\n",
       "      <th>travel_time_34th</th>\n",
       "      <th>travel_time_34th_deviation</th>\n",
       "      <th>arrival_time_dt</th>\n",
       "      <th>stops_at_23rd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47375</th>\n",
       "      <td>1752813480_A..S74X043</td>\n",
       "      <td>2025-07-18T09:09:03+00:00</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.678801</td>\n",
       "      <td>-0.734323</td>\n",
       "      <td>Day</td>\n",
       "      <td>A1</td>\n",
       "      <td>...</td>\n",
       "      <td>17.366667</td>\n",
       "      <td>15.083333</td>\n",
       "      <td>1.933333</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>-0.241667</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>2025-07-18 09:09:03+00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47376</th>\n",
       "      <td>1752814680_A..S74X043</td>\n",
       "      <td>2025-07-18T09:29:56+00:00</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.612217</td>\n",
       "      <td>-0.790690</td>\n",
       "      <td>Day</td>\n",
       "      <td>A1</td>\n",
       "      <td>...</td>\n",
       "      <td>17.366667</td>\n",
       "      <td>20.616667</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>2025-07-18 09:29:56+00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47377</th>\n",
       "      <td>1752815340_A..S53R</td>\n",
       "      <td>2025-07-18T09:42:36+00:00</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.566406</td>\n",
       "      <td>-0.824126</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>...</td>\n",
       "      <td>17.366667</td>\n",
       "      <td>12.866667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.773551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>-0.416667</td>\n",
       "      <td>2025-07-18 09:42:36+00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47378</th>\n",
       "      <td>1752816300_A..S58R</td>\n",
       "      <td>2025-07-18T10:00:11+00:00</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>...</td>\n",
       "      <td>10.633333</td>\n",
       "      <td>17.583333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>3.773551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>2025-07-18 10:00:11+00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47379</th>\n",
       "      <td>1752817230_A..S57R</td>\n",
       "      <td>2025-07-18T10:12:37+00:00</td>\n",
       "      <td>A_South</td>\n",
       "      <td>A</td>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>0.453990</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>Day</td>\n",
       "      <td>A3</td>\n",
       "      <td>...</td>\n",
       "      <td>10.633333</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>-0.116667</td>\n",
       "      <td>3.773551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>-0.233333</td>\n",
       "      <td>2025-07-18 10:12:37+00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    trip_uid               arrival_time group_id route_id  \\\n",
       "47375  1752813480_A..S74X043  2025-07-18T09:09:03+00:00  A_South        A   \n",
       "47376  1752814680_A..S74X043  2025-07-18T09:29:56+00:00  A_South        A   \n",
       "47377     1752815340_A..S53R  2025-07-18T09:42:36+00:00  A_South        A   \n",
       "47378     1752816300_A..S58R  2025-07-18T10:00:11+00:00  A_South        A   \n",
       "47379     1752817230_A..S57R  2025-07-18T10:12:37+00:00  A_South        A   \n",
       "\n",
       "       time_idx  day_of_week  hour_sin  hour_cos regime_id track_id  ...  \\\n",
       "47375         0            4  0.678801 -0.734323       Day       A1  ...   \n",
       "47376        20            4  0.612217 -0.790690       Day       A1  ...   \n",
       "47377        33            4  0.566406 -0.824126       Day       A3  ...   \n",
       "47378        51            4  0.500000 -0.866025       Day       A3  ...   \n",
       "47379        63            4  0.453990 -0.891007       Day       A3  ...   \n",
       "\n",
       "       empirical_median  upstream_headway_14th travel_time_14th  \\\n",
       "47375         17.366667              15.083333         1.933333   \n",
       "47376         17.366667              20.616667         2.200000   \n",
       "47377         17.366667              12.866667         2.000000   \n",
       "47378         10.633333              17.583333         2.000000   \n",
       "47379         10.633333              12.600000         1.833333   \n",
       "\n",
       "       travel_time_14th_deviation  travel_time_23rd  \\\n",
       "47375                   -0.066667          3.200000   \n",
       "47376                    0.200000          3.650000   \n",
       "47377                    0.000000          3.773551   \n",
       "47378                    0.050000          3.773551   \n",
       "47379                   -0.116667          3.773551   \n",
       "\n",
       "       travel_time_23rd_deviation  travel_time_34th  \\\n",
       "47375                   -0.241667          4.766667   \n",
       "47376                    0.208333          4.916667   \n",
       "47377                    0.000000          4.416667   \n",
       "47378                    0.000000          4.100000   \n",
       "47379                    0.000000          3.933333   \n",
       "\n",
       "       travel_time_34th_deviation           arrival_time_dt  stops_at_23rd  \n",
       "47375                   -0.066667 2025-07-18 09:09:03+00:00            1.0  \n",
       "47376                    0.083333 2025-07-18 09:29:56+00:00            1.0  \n",
       "47377                   -0.416667 2025-07-18 09:42:36+00:00            0.0  \n",
       "47378                   -0.066667 2025-07-18 10:00:11+00:00            0.0  \n",
       "47379                   -0.233333 2025-07-18 10:12:37+00:00            0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. ensure categoricals are strings\n",
    "cat_cols = ['group_id','route_id','direction','regime_id','track_id', 'preceding_route_id']\n",
    "for col in cat_cols:\n",
    "    if col in data.columns:\n",
    "        # Handle nulls in categoricals (e.g. preceding_route_id might be null for first train)\n",
    "        data[col] = data[col].fillna(\"None\").astype(str)\n",
    "\n",
    "# 2. Parse dates (Required for splitting)\n",
    "if 'arrival_time' in data.columns:\n",
    "    data['arrival_time_dt'] = pd.to_datetime(data['arrival_time'])\n",
    "\n",
    "# --- NEW IMPUTATION LOGIC FOR PIPELINE OUTPUTS ---\n",
    "\n",
    "# 3a. Handle General Missing Reals (Filling with Neutrals)\n",
    "# Preceding Train Gap & Upstream Headway: Fill with Median\n",
    "for col in ['preceding_train_gap', 'upstream_headway_14th']:\n",
    "    if col in data.columns:\n",
    "         data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "# Travel Time Deviations: Fill with 0.0 (Assume on-time if unknown)\n",
    "dev_cols = [c for c in data.columns if 'deviation' in c]\n",
    "for col in dev_cols:\n",
    "    data[col] = data[col].fillna(0.0)\n",
    "\n",
    "# Other Travel Times (14th, 34th): Fill with Median\n",
    "tt_cols = ['travel_time_14th', 'travel_time_34th']\n",
    "for col in tt_cols:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "# 3b. Handle 23rd St Express/Local logic (Special Case)\n",
    "# Logic: Express trains don't stop at 23rd, so they have NULL travel times.\n",
    "# We create a binary flag 'stops_at_23rd' to tell the model this is intentional.\n",
    "# We then fill the actual travel_time value with the MEAN of valid stops to \"neutralize\" it input-wise.\n",
    "\n",
    "# Create flag: 1 if valid stop (not null and > 0), 0 otherwise\n",
    "data['stops_at_23rd'] = np.where((data['travel_time_23rd'].notna()) & (data['travel_time_23rd'] > 0), 1.0, 0.0)\n",
    "\n",
    "# Calculate mean of VALID stops only\n",
    "valid_mean_23rd = data.loc[data['stops_at_23rd'] == 1.0, 'travel_time_23rd'].mean()\n",
    "\n",
    "# Fill invalid/missing rows with that mean\n",
    "data.loc[data['stops_at_23rd'] == 0.0, 'travel_time_23rd'] = valid_mean_23rd\n",
    "# Just in case any NaNs remain (e.g. the column was purely NaN)\n",
    "data['travel_time_23rd'] = data['travel_time_23rd'].fillna(valid_mean_23rd if pd.notna(valid_mean_23rd) else 0.0)\n",
    "\n",
    "\n",
    "# 4. Correct Time Index (Physical Time)\n",
    "# REFACTOR: Decouple time_idx from row count. Use absolute minute-index.\n",
    "# This prevents warping of time during service gaps.\n",
    "if 'arrival_time_dt' not in data.columns:\n",
    "    data['arrival_time_dt'] = pd.to_datetime(data['arrival_time'])\n",
    "    \n",
    "# Find global min for anchor\n",
    "min_time = data['arrival_time_dt'].min()\n",
    "# Calculate minutes elapsed since start\n",
    "data['time_idx'] = ((data['arrival_time_dt'] - min_time).dt.total_seconds() / 60).astype(int)\n",
    "\n",
    "# Sort by group and new physical time index\n",
    "data = data.sort_values(['group_id', 'time_idx'])\n",
    "\n",
    "print(f\"Cleaned Shape:{data.shape}\")\n",
    "print(f\"Max time_idx (Sequence length): {data['time_idx'].max()}\")\n",
    "try:\n",
    "    print(f\"Valid Mean 23rd: {valid_mean_23rd:.2f}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Verify no nulls remain\n",
    "print(\"\\nRemaining Nulls:\")\n",
    "print(data.isnull().sum().sum())\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d97da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking time_idx continuity and uniqueness...\n",
      "Duplicate (group_id, time_idx) pairs: 25\n",
      "⚠️ WARNING: Found 74313 gaps in time_idx sequence!\n",
      "Top 5 gaps:\n",
      "      group_id               arrival_time  time_idx  time_idx_diff\n",
      "47376  A_South  2025-07-18T09:29:56+00:00        20           20.0\n",
      "47377  A_South  2025-07-18T09:42:36+00:00        33           13.0\n",
      "47378  A_South  2025-07-18T10:00:11+00:00        51           18.0\n",
      "47379  A_South  2025-07-18T10:12:37+00:00        63           12.0\n",
      "47380  A_South  2025-07-18T10:23:36+00:00        74           11.0\n",
      "Advice: If gaps exist, the model cannot learn temporal patterns effectively. Re-run the data pipeline with the fix for time_idx generation.\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC: Check time_idx continuity\n",
    "# This helps diagnose \"no meaningful learning\" issues caused by broken time scales\n",
    "\n",
    "print(\"Checking time_idx continuity and uniqueness...\")\n",
    "\n",
    "# 1. Uniqueness check\n",
    "duplicates = data.duplicated(['group_id', 'time_idx']).sum()\n",
    "print(f\"Duplicate (group_id, time_idx) pairs: {duplicates}\")\n",
    "\n",
    "# 2. Continuity check\n",
    "# We expect time_idx to strictly increment by 1 for each group\n",
    "data = data.sort_values(['group_id', 'time_idx'])\n",
    "data['time_idx_diff'] = data.groupby('group_id')['time_idx'].diff()\n",
    "\n",
    "# The first row of each group will have NaN diff, which is fine.\n",
    "# Subsequent rows MUST have diff == 1.0\n",
    "gaps = data[data['time_idx_diff'] > 1.0]\n",
    "\n",
    "if len(gaps) > 0:\n",
    "    print(f\"⚠️ WARNING: Found {len(gaps)} gaps in time_idx sequence!\")\n",
    "    print(\"Top 5 gaps:\")\n",
    "    print(gaps[['group_id', 'arrival_time', 'time_idx', 'time_idx_diff']].head())\n",
    "    print(\"Advice: If gaps exist, the model cannot learn temporal patterns effectively. Re-run the data pipeline with the fix for time_idx generation.\")\n",
    "else:\n",
    "    print(\"✅ time_idx is continuous (no gaps found).\")\n",
    "\n",
    "# Clean up temporary column\n",
    "if 'time_idx_diff' in data.columns:\n",
    "    data.drop(columns=['time_idx_diff'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6160e505",
   "metadata": {},
   "source": [
    "## Time Based Splits and ML Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f23f743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Timezone Aware: True\n",
      "Cutoff Dates (Train/Val/Test): 2025-11-18 00:00:00+00:00, 2025-12-18 00:00:00+00:00, 2026-01-18 00:00:00+00:00\n",
      "Train rows: 49743\n",
      "Val Rows (with context): 12336\n",
      "Test Rows (with context): 12620\n",
      "Train Batches: 430\n",
      "Val Batches: 11\n",
      "Test Batches 11\n",
      "Total Batches (Train/Val/Test): 430 / 11 / 11\n",
      "\n",
      "Feature names: ['route_id', 'regime_id', 'track_id', 'preceding_route_id']\n",
      "Encoder Shape (Batch, Time, Features): torch.Size([128, 20, 18])\n"
     ]
    }
   ],
   "source": [
    "# 1 define date splits\n",
    "# Ensure datetime column exists and handle timezone\n",
    "if 'arrival_time_dt' not in data.columns:\n",
    "    data['arrival_time_dt'] = pd.to_datetime(data['arrival_time'])\n",
    "\n",
    "# Detect if the dataset is Timezone Aware (The new parquet file likely is UTC)\n",
    "is_tz_aware = data['arrival_time_dt'].dt.tz is not None\n",
    "print(f\"Dataset Timezone Aware: {is_tz_aware}\")\n",
    "\n",
    "# CUTOFF DATES REVERTED TO ORIGINAL PER USER REQUEST\n",
    "# We match the timezone of the cutoffs to the data to avoid TypeError: \"Cannot compare tz-naive and tz-aware\"\n",
    "tz = \"UTC\" if is_tz_aware else None\n",
    "\n",
    "train_end_date = pd.Timestamp(\"2025-11-18\", tz=tz) # Adjusted approx\n",
    "val_end_date = pd.Timestamp(\"2025-12-18\", tz=tz)\n",
    "test_end_date = pd.Timestamp(\"2026-01-18\", tz=tz) \n",
    "\n",
    "print(f\"Cutoff Dates (Train/Val/Test): {train_end_date}, {val_end_date}, {test_end_date}\")\n",
    "\n",
    "# 2 helper function to create dataset inputs with correct lookback context\n",
    "def get_slice_with_lookback(full_df, start_date, end_date, lookback=20):\n",
    "    \"\"\"\n",
    "    Return rows between start_date and end_date,\n",
    "    Plus the last 'lookback' rows before start_date for EACH group\n",
    "    \"\"\"\n",
    "    # get the core data for the period\n",
    "    mask = (full_df['arrival_time_dt'] >= start_date) & (full_df['arrival_time_dt'] < end_date)\n",
    "    core_df = full_df[mask]\n",
    "\n",
    "    # we need to preprend the last 'lookback' rows from BEFORE start_date for EACH group\n",
    "    # to serve as the history for the first few predictions\n",
    "    prior_df = full_df[full_df['arrival_time_dt'] < start_date]\n",
    "    pre_data = []\n",
    "\n",
    "    for g_id, group in prior_df.groupby('group_id'):\n",
    "        pre_data.append(group.tail(lookback))\n",
    "    \n",
    "    if pre_data:\n",
    "        lookback_df = pd.concat(pre_data)\n",
    "        # concat and sort to ensure time continuity per group\n",
    "        return pd.concat([lookback_df, core_df]).sort_values(['group_id','time_idx'])\n",
    "    return core_df\n",
    "\n",
    "# 3 create physical dataframes\n",
    "train_df = data[data['arrival_time_dt']< train_end_date]\n",
    "val_df_input = get_slice_with_lookback(data, train_end_date, val_end_date, lookback=20)\n",
    "test_df_input = get_slice_with_lookback(data, val_end_date, test_end_date, lookback=20)\n",
    "\n",
    "print(f\"Train rows: {len(train_df)}\")\n",
    "print(f\"Val Rows (with context): {len(val_df_input)}\")\n",
    "print(f\"Test Rows (with context): {len(test_df_input)}\")\n",
    "\n",
    "# create datasets\n",
    "max_prediction_length = 1\n",
    "# improvement plan said \"Start with 20\". \n",
    "max_encoder_length = 20 \n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"service_headway\",\n",
    "    group_ids=[\"group_id\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    # REMOVED: direction, added route_id as static\n",
    "    static_categoricals=[\"route_id\"], \n",
    "    time_varying_known_categoricals=[\"regime_id\", \"track_id\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"hour_sin\", \"hour_cos\", \"empirical_median\"],\n",
    "    # ADDED: preceding_route_id (unknown because it varies per step)\n",
    "    time_varying_unknown_categoricals=[\"preceding_route_id\"],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"service_headway\",\n",
    "        \"preceding_train_gap\",\n",
    "        \"upstream_headway_14th\",\n",
    "        \"travel_time_14th\",\n",
    "        \"travel_time_14th_deviation\",\n",
    "        \"travel_time_23rd\",\n",
    "        \"travel_time_23rd_deviation\",\n",
    "        \"travel_time_34th\",\n",
    "        \"travel_time_34th_deviation\",\n",
    "        \"stops_at_23rd\" # ADDED: binary flag for express trains\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"group_id\"], transformation=\"softplus\"\n",
    "    ),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    # ADDED: Allow gaps in time_idx because we switched to physical minutes\n",
    "    allow_missing_timesteps=True \n",
    ")\n",
    "\n",
    "# use from_dataset with sliced dataframes\n",
    "validation = TimeSeriesDataSet.from_dataset(training, val_df_input, predict=False, stop_randomization=True)\n",
    "test = TimeSeriesDataSet.from_dataset(training, test_df_input, predict=False, stop_randomization=True)\n",
    "\n",
    "# OPTIMIZED: Per Infrastructure Plan\n",
    "batch_size = 128 # User requested 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=8)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=4)\n",
    "test_dataloader = test.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=4)\n",
    "\n",
    "print(f\"Train Batches: {len(train_dataloader)}\")\n",
    "print(f\"Val Batches: {len(val_dataloader)}\")\n",
    "print(f\"Test Batches {len(test_dataloader)}\")\n",
    "print(f\"Total Batches (Train/Val/Test): {len(train_dataloader)} / {len(val_dataloader)} / {len(test_dataloader)}\")\n",
    "\n",
    "# visualize what the model sees\n",
    "x, y = next(iter(train_dataloader))\n",
    "print(\"\\nFeature names:\", training.static_categoricals + training.time_varying_known_categoricals + training.time_varying_unknown_categoricals)\n",
    "print(\"Encoder Shape (Batch, Time, Features):\", x['encoder_cont'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae8d7a",
   "metadata": {},
   "source": [
    "## Model Architecture and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fa537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch TensorBoard pointing to the logs directory we configured above\n",
    "# This will open a panel below. It might be empty at first until training starts writing logs.\n",
    "%tensorboard --logdir tensorboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 configure the Temporal Fusion Transformer\n",
    "\n",
    "# FIX: Define a subclass to handle BFloat16 plotting issue with Matplotlib\n",
    "# PyTorch Forecasting's plot_prediction method crashes when passing BFloat16 tensors to Matplotlib.\n",
    "class TFTWithBF16Fix(TemporalFusionTransformer):\n",
    "    def plot_prediction(self, x, out, idx, **kwargs):\n",
    "        # Helper to cast BF16 tensors to Float32\n",
    "        def to_float32(val):\n",
    "            if isinstance(val, torch.Tensor) and val.dtype == torch.bfloat16:\n",
    "                return val.float()\n",
    "            return val\n",
    "            \n",
    "        # Recursive cast for nested dictionaries/tensors\n",
    "        # NOTE: 'x' is typically a dictionary of tensors, so preserving the dict structure is key.\n",
    "        # However, we must ensure we don't accidentally cast things that aren't tensors.\n",
    "        if isinstance(x, dict):\n",
    "             # Create a new dictionary with casted values\n",
    "             x = {k: to_float32(v) for k, v in x.items()}\n",
    "             \n",
    "    \n",
    "        \n",
    "        # PROPOSED FIX: Cast everything recursively but carefully reconstruction the object.\n",
    "        from pytorch_forecasting.utils import to_list\n",
    "        from collections import namedtuple\n",
    "        \n",
    "        # If 'out' is the special Output class or NamedTuple\n",
    "        if hasattr(out, \"_asdict\"):\n",
    "            out_dict = out._asdict()\n",
    "            out_dict = {k: to_float32(v) for k, v in out_dict.items()}\n",
    "            # Reconstruct\n",
    "            out = out.__class__(**out_dict)\n",
    "        elif isinstance(out, dict):\n",
    "            # If it's already a dict (some versions do this), just cast\n",
    "            # If it needs to be an object with .iget, we can't easily fake it if it wasn't one already.\n",
    "            # But usually it IS one.\n",
    "             out = {k: to_float32(v) for k, v in out.items()}\n",
    "             # If the parent expects an object with .iget, we are in trouble if we just pass a dict.\n",
    "             # However, usually 'out' coming from .step() is the object.\n",
    "        \n",
    "        # We also need to traverse lists/tuples if they exist\n",
    "        out = to_float32(out) # Fallback for single tensor\n",
    "        \n",
    "        return super().plot_prediction(x, out, idx, **kwargs)\n",
    "\n",
    "# SIMPLER FIX: Just disable plotting during training log\n",
    "# If the casting is too brittle due to internal object structures, \n",
    "# we can override log_prediction to do nothing or handle it manually.\n",
    "\n",
    "class TFTDisablePlotting(TemporalFusionTransformer):\n",
    "    def log_prediction(self, x, out, batch_idx, **kwargs):\n",
    "        # SKIP plotting during training to avoid BFloat16 Matplotlib crash\n",
    "        pass\n",
    "\n",
    "tft = TFTDisablePlotting.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.001,\n",
    "    hidden_size=128,            # INCREASED: Scaled to 128 per optimization report\n",
    "    attention_head_size=4,      # INCREASED: Multi-scale attention\n",
    "    dropout=0.1,                # REDUCED: 0.1 to facilitate initial learning\n",
    "    hidden_continuous_size=64,  # INCREASED: 64 to prevent bottleneck\n",
    "    output_size=3,              # 3 quantiles [0.1, 0.5, 0.9]\n",
    "    loss=QuantileLoss([0.1, 0.5, 0.9]),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# 2 configure training callbacks\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=15, \n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# configure logger\n",
    "logger = TensorBoardLogger(\"tensorboard_logs\", name=\"headway_tft\")\n",
    "\n",
    "# initialize trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,             # INCREASED: 100 epochs\n",
    "    accelerator=\"auto\", \n",
    "    devices=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,      # CHANGED: 0.1 for LSTM stability\n",
    "    callbacks=[lr_logger, early_stop_callback, TQDMProgressBar(refresh_rate=20)], \n",
    "    logger=logger,\n",
    "    limit_train_batches=1.0,    # CHANGED: Use 100% of data\n",
    "    precision=\"bf16-mixed\"      # CHANGED: Mixed precision for A100\n",
    ")\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42283488",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "Part A: Loss Visualization (The Developer's View)<br>\n",
    "Standard TensorBoard-style plot of Train vs Val Loss.<br>\n",
    "<br>\n",
    "Part B: Performance Metrics (The Data Scientist's View)<br>\n",
    "We will run the model on the Test Set (which it has never seen) and compute:<br>\n",
    "<br>\n",
    "MAE (in minutes)<br>\n",
    "sMAPE (Percentage error)<br>\n",
    "Part C: Representative Predictions (The Operator's View)<br>\n",
    "We will select a few specific examples from the test set to plot:<br>\n",
    "<br>\n",
    "Routine: A standard rush-hour sequence.<br>\n",
    "Disruption: A case where preceding_train_gap was high (interaction delay).<br>\n",
    "Overnight: A case from the \"Regime Shift\" (22:00-23:00) to see if it widens confidence intervals as requested.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06663005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load best model\n",
    "best_model_path = None\n",
    "if getattr(trainer, \"checkpoint_callbacks\", None):\n",
    "    for cb in trainer.checkpoint_callbacks:\n",
    "        path = getattr(cb, \"best_model_path\", None)\n",
    "        if path:\n",
    "            best_model_path = path\n",
    "            print(f\"Best model found at: {best_model_path}\")\n",
    "            break\n",
    "\n",
    "if best_model_path is None and getattr(trainer, \"checkpoint_callback\", None):\n",
    "    best_model_path = getattr(trainer.checkpoint_callback, \"best_model_path\", None)\n",
    "    print(f\"Best model found via fallback: {best_model_path}\")\n",
    "\n",
    "if best_model_path is None:\n",
    "    # Just a warning or fallback if training didn't complete / save\n",
    "    print(\"No best model found. Using last model state if available.\")\n",
    "    best_tft = tft\n",
    "else:\n",
    "    # Load model using the standard class\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# Performance: Move to GPU if available for inference\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Inference Device: {device}\")\n",
    "best_tft.to(device)\n",
    "\n",
    "# 2. Global Predictions\n",
    "print(\"Generating predictions on full test set...\")\n",
    "raw_prediction = best_tft.predict(test_dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "# Extract components\n",
    "predictions = raw_prediction.output[\"prediction\"] \n",
    "x = raw_prediction.x\n",
    "\n",
    "# 3. Metrics (Move to CPU for calculation)\n",
    "print(\"Calculating Metrics...\")\n",
    "predictions_cpu = predictions.cpu()\n",
    "actuals_cpu = x[\"decoder_target\"].cpu()\n",
    "\n",
    "# QuantileLoss expects target with shape (Batch, Time), so we strictly preserve dimensions here.\n",
    "# NOTE: removing previous squeeze() logic which caused IndexError\n",
    "mae_metric = MAE()\n",
    "smape_metric = SMAPE()\n",
    "quantile_loss_metric = QuantileLoss(quantiles=[0.1, 0.5, 0.9])\n",
    "\n",
    "loss_val = quantile_loss_metric(predictions_cpu, actuals_cpu)\n",
    "\n",
    "# For Point Metrics (MAE, SMAPE), we use the Median (P50)\n",
    "p50_forecast = predictions_cpu[:, :, 1] # shape (Batch, prediction_len)\n",
    "mae_val = mae_metric(p50_forecast, actuals_cpu)\n",
    "smape_val = smape_metric(p50_forecast, actuals_cpu)\n",
    "\n",
    "print(f\"\\n--- Global Test Metrics ---\")\n",
    "print(f\"Quantile Loss: {loss_val.mean().item():.4f}\")\n",
    "print(f\"MAE (P50):     {mae_val.mean().item():.4f} minutes\")\n",
    "print(f\"sMAPE (P50):   {smape_val.mean().item():.4f}\")\n",
    "\n",
    "# FLATTEN TENSORS for Analysis and Plotting (Batch*Time)\n",
    "# This prevents shape mismatch issues with Pandas/Matplotlib\n",
    "actuals_flat = actuals_cpu.view(-1)\n",
    "p50_flat = p50_forecast.view(-1)\n",
    "p10_flat = predictions_cpu[:, :, 0].view(-1)\n",
    "p90_flat = predictions_cpu[:, :, 2].view(-1)\n",
    "\n",
    "# Calibration\n",
    "p10_coverage = (actuals_flat <= p10_flat).float().mean()\n",
    "p90_coverage = (actuals_flat <= p90_flat).float().mean()\n",
    "print(f\"P10 Coverage:  {p10_coverage.item():.3f} (Target 0.10)\")\n",
    "print(f\"P90 Coverage:  {p90_coverage.item():.3f} (Target 0.90)\")\n",
    "\n",
    "# --- NEW SECTION: Group Breakdown ---\n",
    "print(\"\\n--- Metrics by Group ID ---\")\n",
    "if \"groups\" in x:\n",
    "    # Groups are repeated for prediction_length if > 1, so we must be careful.\n",
    "    # But usually 'groups' is (Batch,), so we repeat it to match flattened outputs if needed.\n",
    "    # However x['groups'] from raw_prediction.x usually maps 1-to-1 with Batch.\n",
    "    # Since we flattened prediction-wise (Batch*Time), we need to repeat group_ids if time > 1.\n",
    "    \n",
    "    group_ids_batch = x[\"groups\"].cpu().view(-1) # (Batch)\n",
    "    # If prediction_len > 1, we need to repeat group_ids\n",
    "    if predictions_cpu.shape[1] > 1:\n",
    "        group_ids = group_ids_batch.repeat_interleave(predictions_cpu.shape[1]).numpy()\n",
    "    else:\n",
    "        group_ids = group_ids_batch.numpy()\n",
    "    \n",
    "    # Get Decoder\n",
    "    if hasattr(training, \"categorical_encoders\") and training.categorical_encoders is not None:\n",
    "        group_encoder = training.categorical_encoders[\"group_id\"]\n",
    "    else:\n",
    "        group_encoder = best_tft.dataset_parameters[\"categorical_encoders\"][\"group_id\"]\n",
    "    \n",
    "    # Calculate errors\n",
    "    abs_errors = torch.abs(p50_flat - actuals_flat).numpy()\n",
    "    \n",
    "    res_df = pd.DataFrame({\n",
    "        \"group_idx\": group_ids,\n",
    "        \"mae\": abs_errors\n",
    "    })\n",
    "    \n",
    "    # Map index to name\n",
    "    unique_idxs = np.unique(group_ids)\n",
    "    decoded_names = group_encoder.inverse_transform(torch.tensor(unique_idxs, dtype=torch.long))\n",
    "    idx_map = dict(zip(unique_idxs, decoded_names))\n",
    "    res_df[\"group_name\"] = res_df[\"group_idx\"].map(idx_map)\n",
    "    \n",
    "    grouped_stats = res_df.groupby(\"group_name\")[\"mae\"].agg(['mean', 'count']).rename(columns={'mean': 'MAE', 'count': 'Samples'})\n",
    "    print(grouped_stats.sort_values(\"MAE\", ascending=False))\n",
    "else:\n",
    "    print(\"Could not find group information in input tensors.\")\n",
    "    res_df = pd.DataFrame() # Fallback\n",
    "\n",
    "# 4. EXECUTIVE SUITE VISUALIZATION\n",
    "print(\"\\n--- Generating Executive Visualization Suite ---\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.2)\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "\n",
    "# Prepare Data Arrays (use flattened)\n",
    "y_true = actuals_flat.numpy()\n",
    "y_pred = p50_flat.numpy()\n",
    "residuals = y_pred - y_true\n",
    "\n",
    "# PLOT 1: Truth Scatter (Top Left)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "# Downsample for scatter presentation if huge\n",
    "if len(y_true) > 5000:\n",
    "    idx_sample = np.random.choice(len(y_true), 5000, replace=False)\n",
    "    p_true, p_pred = y_true[idx_sample], y_pred[idx_sample]\n",
    "else:\n",
    "    p_true, p_pred = y_true, y_pred\n",
    "\n",
    "sns.scatterplot(x=p_true, y=p_pred, alpha=0.1, color=\"#2c3e50\", edgecolor=None, ax=ax1)\n",
    "# Identity line\n",
    "max_val = max(p_true.max(), p_pred.max())\n",
    "ax1.plot([0, max_val], [0, max_val], color=\"#e74c3c\", linestyle=\"--\", linewidth=2, label=\"Perfect Accuracy\")\n",
    "ax1.set_title(\"Observed vs. Predicted Headways\", fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Observed Headway (min)\")\n",
    "ax1.set_ylabel(\"Predicted Headway (min)\")\n",
    "ax1.legend()\n",
    "\n",
    "# PLOT 2: reliability Histogram (Top Right)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "sns.histplot(residuals, bins=50, kde=True, color=\"#3498db\", ax=ax2)\n",
    "ax2.axvline(x=0, color='black', linestyle='--')\n",
    "# Annotate 90% bounds\n",
    "r_p05 = np.percentile(residuals, 5)\n",
    "r_p95 = np.percentile(residuals, 95)\n",
    "ax2.axvline(x=r_p05, color=\"#e74c3c\", linestyle=\":\", alpha=0.5)\n",
    "ax2.axvline(x=r_p95, color=\"#e74c3c\", linestyle=\":\", alpha=0.5)\n",
    "ax2.set_title(f\"Error Distribution (90% within [{r_p05:.1f}, {r_p95:.1f}] min)\", fontweight=\"bold\")\n",
    "ax2.set_xlabel(\"Prediction Error (min)\")\n",
    "ax2.set_xlim(-5, 5) # Focus on the core\n",
    "\n",
    "# PLOT 3: Operator View / Timeline (Bottom Full Width)\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "\n",
    "if not res_df.empty:\n",
    "    # Find the largest group\n",
    "    biggest_group = grouped_stats[\"Samples\"].idxmax()\n",
    "    subset_mask = (res_df[\"group_name\"] == biggest_group).values\n",
    "    \n",
    "    # Extract sequence (assuming dataset is time-ordered per group)\n",
    "    y_true_sub = actuals_flat[subset_mask].numpy()\n",
    "    y_pred_sub = p50_flat[subset_mask].numpy()\n",
    "    y_p10_sub = p10_flat[subset_mask].numpy()\n",
    "    y_p90_sub = p90_flat[subset_mask].numpy()\n",
    "    \n",
    "    # Take a slice of busy time (e.g. 100 trains in the middle)\n",
    "    start_idx = max(0, len(y_true_sub) // 2 - 50)\n",
    "    end_idx = min(len(y_true_sub), start_idx + 100)\n",
    "    \n",
    "    if start_idx < end_idx:\n",
    "        x_seq = range(start_idx, end_idx)\n",
    "        \n",
    "        ax3.plot(x_seq, y_true_sub[start_idx:end_idx], label=\"Observed\", color=\"black\", linewidth=2, marker='o', markersize=4)\n",
    "        ax3.plot(x_seq, y_pred_sub[start_idx:end_idx], label=\"AI Forecast\", color=\"#2980b9\", linewidth=2, linestyle=\"--\")\n",
    "        ax3.fill_between(x_seq, y_p10_sub[start_idx:end_idx], y_p90_sub[start_idx:end_idx], color=\"#3498db\", alpha=0.2, label=\"Confidence Interval\")\n",
    "        \n",
    "        ax3.set_title(f\"Live Tracking Concept: '{biggest_group}' (Sequence of 100 Arrivals)\", fontweight=\"bold\")\n",
    "        ax3.set_xlabel(\"Sequential Train Arrivals\")\n",
    "        ax3.set_ylabel(\"Headway (min)\")\n",
    "        ax3.legend(loc=\"upper right\")\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, \"Insufficient data for timeline\", ha='center')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, \"No Group Data Available for Timeline\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
