{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c0425b",
   "metadata": {},
   "source": [
    "# Model Training & Experimentation Framework\n",
    "\n",
    "This notebook implements the \"Experiment Factory\" for the Headway Prediction model. \n",
    "It is designed to support the ablation analysis defined in the project abstract, allowing us to vary:\n",
    "1.  **Lookback Window ($L$):** 30, 45, 60 minutes.\n",
    "2.  **Input Features:** With or without Terminal Headways ($T$).\n",
    "3.  **Prediction Horizon:** Recursive prediction up to 60 minutes.\n",
    "\n",
    "We start by importing the necessary libraries, including TensorFlow/Keras for the Deep Learning components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d81a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set random seeds for reproduceability\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration class \n",
    "# We define an `ExperimentConfig` class to encapsulate all hyperparameters. This makes it easy to switch between different experimental setups (e.g., changing the lookback window or enabling/disabling terminal headways) without rewriting code. \n",
    "\n",
    "class ExperimentConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lookback_mins=60,\n",
    "        forecast_mins=30, \n",
    "        time_bin_size_min=5,\n",
    "        use_terminal_headway=True,\n",
    "        batch_size=32,\n",
    "        epochs=32, \n",
    "        learning_rate=0.001\n",
    "    ):\n",
    "        self.lookback_mins = lookback_mins\n",
    "        self.forecast_mins = forecast_mins\n",
    "        self.time_bin_size_min = time_bin_size_min\n",
    "        self.use_terminal_headway = use_terminal_headway\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # calculated properties\n",
    "        self.lookback_bins = lookback_mins // time_bin_size_min\n",
    "        self.forecast_bins = forecast_mins // time_bin_size_min\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"ExperimentalConfig(L={self.lookback_mins}m, \"\n",
    "                f\"F={self.forecast_mins}m, \"\n",
    "                f\"Use_T={self.use_terminal_headway}\")\n",
    "\n",
    "# create baseline configuration (exp-A1)\n",
    "config = ExperimentConfig(\n",
    "    lookback_mins=30, # Baseline from Abstract\n",
    "    forecast_mins=15, # single-step target for recursive prediction\n",
    "    use_terminal_headway=True\n",
    ")\n",
    "\n",
    "print(f\"Active Configuration {config}\")\n",
    "print(f\"Lookback Bins: {config.lookback_bins}\")\n",
    "print(f\"Forecast Bins: {config.forecast_bins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab463b",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation\n",
    "\n",
    "We load the preprocessed matrix and schedule data. We then use the `create_dataset` function (adapted from the EDA notebook) to generate the tensors based on the active `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f41e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "MATRIX_PATH = \"../data/headway_matrix_full.npy\"\n",
    "SCHEDULE_PATH = \"../data/target_terminal_headways.csv\"\n",
    "GLOBAL_START_TIME = \"2025-06-06 00:00:00\"\n",
    "\n",
    "def load_and_process_data(config):\n",
    "    \"\"\"\n",
    "    loads raw data and prepares the (X, T, Y) tensors based on the config\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # 1 load matrix\n",
    "    matrix = np.load(MATRIX_PATH)\n",
    "\n",
    "    # 2 normalize the data\n",
    "    SCALING_FACTOR = 20.0\n",
    "    matrix = matrix / SCALING_FACTOR\n",
    "\n",
    "    # 3 load and align schedule\n",
    "    schedule_df = pd.read_csv(SCHEDULE_PATH)\n",
    "    schedule_df['datetime'] = pd.to_datetime(schedule_df['service_date']) + \\\n",
    "                              pd.to_timedelta(schedule_df['departure_seconds'], unit='s')\n",
    "    schedule_df = schedule_df.set_index('datetime').sort_index()\n",
    "    schedule_df = schedule_df[~schedule_df.index.duplicated(keep='first')]\n",
    "    schedule_df = schedule_df[schedule_df.index >= GLOBAL_START_TIME]\n",
    "\n",
    "    # fill nans and resample\n",
    "    schedule_df['scheduled_headway_min'] = schedule_df['scheduled_headway_min'].bfill()\n",
    "    \n",
    "    # normalize schedule too\n",
    "    schedule_df['scheduled_headway_min'] = schedule_df['scheduled_headway_min'] / SCALING_FACTOR\n",
    "    \n",
    "    time_coords = pd.date_range(start=GLOBAL_START_TIME, periods=matrix.shape[0], freq=f\"{config.time_bin_size_min}min\")\n",
    "    schedule_resampled = schedule_df['scheduled_headway_min'].resample(f'{config.time_bin_size_min}min').ffill()\n",
    "    schedule_aligned = schedule_resampled.reindex(time_coords, method='ffill').bfill().values\n",
    "\n",
    "    # 4. create tensors\n",
    "    print(f\"Generating tensors with L={config.lookback_bins} bins, F={config.forecast_bins} bins...\")\n",
    "    X, T, Y = [], [], []\n",
    "\n",
    "    for i in range(config.lookback_bins, len(matrix) - config.forecast_bins):\n",
    "        # Input X: Past L steps\n",
    "        X.append(matrix[i-config.lookback_bins:i, :])\n",
    "        # Input T: Future F steps\n",
    "        T.append(schedule_aligned[i:i+config.forecast_bins])\n",
    "        # Target Y: Future F steps\n",
    "        Y.append(matrix[i:i+config.forecast_bins, :])\n",
    "\n",
    "    X = np.array(X)[..., np.newaxis] #(Batch, Time, Space, 1)\n",
    "    T = np.array(T)[..., np.newaxis] #(Batch, Time, 1)\n",
    "    Y = np.array(Y)[..., np.newaxis] #(Batch, Time, Space, 1)\n",
    "\n",
    "    return X, T, Y\n",
    "\n",
    "# execute data loading\n",
    "X, T, Y = load_and_process_data(config)\n",
    "\n",
    "print(f\"\\nData Shapes:\")\n",
    "print(f\"X (Context): {X.shape}\")\n",
    "print(f\"T (Intent): {T.shape}\")\n",
    "print(f\"Y (Target): {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9c6b7",
   "metadata": {},
   "source": [
    "## 3. Model Architecture (ConvLSTM)\n",
    "\n",
    "We define the `build_model` function. It constructs a Keras model using `ConvLSTM2D` layers to capture spatiotemporal dependencies.\n",
    "\n",
    "**Key Features:**\n",
    "*   **5D Input Handling:** `ConvLSTM2D` expects `(Batch, Time, Rows, Cols, Channels)`. Since our data is 1D space (Stations), we reshape it to `(Time, Stations, 1, 1)` inside the model.\n",
    "*   **Dual Input Support:** If `config.use_terminal_headway` is True, the model creates a secondary input branch for the schedule ($T$), processes it, and concatenates it with the main traffic flow features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606822fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, input_shape_x, input_shape_t=None):\n",
    "    \"\"\"\n",
    "    Constructs the ConvLSTM model based on configuration\n",
    "    \"\"\"\n",
    "    # branch 1 : Spatiotemporal context (X)\n",
    "    # input shape: (time, space, 1)\n",
    "    input_x = layers.Input(shape=input_shape_x, name=\"input_x\")\n",
    "\n",
    "    # reshape to 5D for ConvLSTM2D: (time, rows=space, cols=1, channels=1)\n",
    "    # we treat the line of stations as a \"height\" of space with \"width\" of 1.\n",
    "    x = layers.Reshape((config.lookback_bins, input_shape_x[1], 1, 1))(input_x)\n",
    "\n",
    "    # Encoder: ConvLSTM layer\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 1),\n",
    "        padding=\"same\",\n",
    "        return_sequences=True,\n",
    "        activation=\"relu\"\n",
    "    )(x)\n",
    "\n",
    "    # branch 2: Dispathcer Intent (T) - Optional\n",
    "    if config.use_terminal_headway and input_shape_t is not None:\n",
    "        # Input Shape: (Time, 1)\n",
    "        input_t = layers.Input(shape=input_shape_t, name=\"input_t\")\n",
    "\n",
    "        # process T to match X's dimensions for concatenation\n",
    "        # 1. expand features: (Time, 32)\n",
    "        t = layers.TimeDistributed(layers.Dense(32, activation=\"relu\"))((input_t))\n",
    "        # 2 add spatial Dims: (Time, 1, 1, 32)\n",
    "        t = layers.Reshape((config.forecast_bins, 1, 1, 32))(t)\n",
    "        # 3. tile across all stations: (time, space, 1, 32)\n",
    "        # we repeat the single terminal value for every station\n",
    "        t = tf.tile(t, [1, 1, input_shape_x[1], 1, 1])\n",
    "\n",
    "        # Note: In a real recursive loop, X (History) and T (Future) have different time lengths.\n",
    "        # For this specific training step (Single Step Horizon), we might need to align them.\n",
    "        # However, for the baseline experiment, let's assume we are mapping \n",
    "        # L steps of History -> F steps of Future directly (Seq2Seq) or \n",
    "        # L steps of History -> Next Step (Many-to-One).\n",
    "        \n",
    "        # SIMPLIFICATION FOR BASELINE: \n",
    "        # We will use a \"Many-to-Many\" approach where we map the LAST step of encoder \n",
    "        # to the target. But since X and T have different lengths (L vs F), \n",
    "        # standard concatenation is tricky without a full Seq2Seq decoder.\n",
    "        \n",
    "        # To keep it simple and robust for the first run:\n",
    "        # We will ignore T for the very first compilation to ensure the pipeline runs,\n",
    "        # then refine the fusion logic.\n",
    "        pass \n",
    "\n",
    "    # Decoder / output\n",
    "    # map back to 1 channel (headway)\n",
    "    outputs = layers.TimeDistributed(layers.Conv2D(filters=1, kernel_size=(1, 1)))(x)\n",
    "\n",
    "    # reshape back to 3d: (time, space, 1)\n",
    "    outputs = layers.Reshape((config.lookback_bins, input_shape_x[1], 1))(outputs)\n",
    "\n",
    "     # Since we want to predict F steps into the future, but ConvLSTM outputs L steps (same as input),\n",
    "    # we typically take the last step or use a proper decoder.\n",
    "    # For this specific \"Single Step\" training (predicting t+15min), we just need an output \n",
    "    # that matches the target Y shape.\n",
    "\n",
    "    # lets slice the output to match the forecast horizon if L > F\n",
    "    if config.lookback_bins > config.forecast_bins:\n",
    "        outputs = layers.Lambda(lambda z: z[:, -config.forecast_bins:, :, :])(outputs)\n",
    "    \n",
    "    model = keras.Model(inputs=[input_x], outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=config.learning_rate), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "# note: we drop the batch dimension (0) from shape\n",
    "model = build_model(config, input_shape_x=X.shape[1:])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ea4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(f\"Training model for {config.epochs} epochs...\")\n",
    "\n",
    "# callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    validation_split=0.2,\n",
    "    epochs=config.epochs,\n",
    "    batch_size=config.batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfdd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93160a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a prediction\n",
    "# select a random sample\n",
    "sample_idx = np.random.randint(0, len(X))\n",
    "x_sample = X[sample_idx:sample_idx+1]\n",
    "y_true = Y[sample_idx:sample_idx+1]\n",
    "\n",
    "y_pred = model.predict(x_sample)\n",
    "\n",
    "# shapes (1, Time, Space, 1)\n",
    "# lets plot the space-time heatmap for truth vs Pred\n",
    "# we transpose to ahve time on x-axis and space (stations) on y axis\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ground truth\n",
    "sns.heatmap(y_true[0, :, :, 0].T, ax=axes[0], cmap=\"viridis\")\n",
    "axes[0].set_title(f\"Ground Truth (Sample {sample_idx})\")\n",
    "axes[0].set_xlabel(\"Time Step (Future)\")\n",
    "axes[0].set_ylabel(\"Station ID\")\n",
    "\n",
    "# prediction\n",
    "sns.heatmap(y_pred[0, :, :, 0].T, ax=axes[1], cmap=\"viridis\")\n",
    "axes[1].set_title(f\"Prediction (Sample {sample_idx})\")\n",
    "axes[1].set_xlabel(\"Time Step (Future)\")\n",
    "axes[1].set_ylabel(\"Station ID\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855dc29e",
   "metadata": {},
   "source": [
    "## updated with T (target terminal headways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57874e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, input_shape_x, input_shape_t=None):\n",
    "    \"\"\"\n",
    "    Constructs the ConvLSTM model with Schedule Fusion\n",
    "    \"\"\"\n",
    "    # --- Branch 1: Spatiotemporal Context (X) ---\n",
    "    # Input: (Time, Space, 1)\n",
    "    input_x = layers.Input(shape=input_shape_x, name=\"input_x\")\n",
    "    \n",
    "    # Reshape for ConvLSTM: (Time, Space, 1, 1)\n",
    "    x_reshaped = layers.Reshape((config.lookback_bins, input_shape_x[1], 1, 1))(input_x)\n",
    "\n",
    "    # Encoder: ConvLSTM layer\n",
    "    # We keep it simple: Map L steps of history to features\n",
    "    lstm_out = layers.ConvLSTM2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 1),\n",
    "        padding=\"same\",\n",
    "        return_sequences=True,\n",
    "        activation=\"relu\"\n",
    "    )(x_reshaped)\n",
    "    # Output: (Batch, L, Space, 1, 32)\n",
    "\n",
    "    # Remove the extra width dimension: (Batch, L, Space, 32)\n",
    "    lstm_out = layers.Reshape((config.lookback_bins, input_shape_x[1], 32))(lstm_out)\n",
    "\n",
    "    # --- Branch 2: Dispatcher Intent (T) ---\n",
    "    if config.use_terminal_headway and input_shape_t is not None:\n",
    "        # Input: (Time_Future, 1) -> We need to align this with LSTM output\n",
    "        input_t = layers.Input(shape=input_shape_t, name=\"input_t\")\n",
    "        \n",
    "        # 1. Embed the scalar headway: (F, 1) -> (F, 8)\n",
    "        t_emb = layers.TimeDistributed(layers.Dense(8, activation=\"relu\"))(input_t)\n",
    "        \n",
    "        # 2. Tile across space: (F, 1, 8) -> (F, Space, 8)\n",
    "        t_tiled = layers.Lambda(lambda x: tf.tile(x[:, :, tf.newaxis, :], [1, 1, input_shape_x[1], 1]))(t_emb)\n",
    "        \n",
    "        # CRITICAL ALIGNMENT:\n",
    "        # LSTM outputs L steps (History). Schedule is F steps (Future).\n",
    "        # To fuse them, we need to decide on the architecture.\n",
    "        # SIMPLEST FIX: We only use the LAST step of the LSTM (the \"Summary\" of history)\n",
    "        # and repeat it F times to match the Schedule.\n",
    "        \n",
    "        # Take last step of LSTM: (Batch, Space, 32)\n",
    "        lstm_last = layers.Lambda(lambda x: x[:, -1, :, :])(lstm_out)\n",
    "        \n",
    "        # Repeat for F steps: (Batch, F, Space, 32)\n",
    "        lstm_repeated = layers.RepeatVector(config.forecast_bins)(lstm_last)\n",
    "        lstm_repeated = layers.Reshape((config.forecast_bins, input_shape_x[1], 32))(lstm_repeated)\n",
    "        \n",
    "        # Concatenate: (Batch, F, Space, 32+8)\n",
    "        merged = layers.Concatenate(axis=-1)([lstm_repeated, t_tiled])\n",
    "        \n",
    "        # Decoder: Process the merged future context\n",
    "        x = layers.TimeDistributed(layers.Dense(32, activation=\"relu\"))(merged)\n",
    "        \n",
    "        inputs = [input_x, input_t]\n",
    "    else:\n",
    "        # Fallback if no T (not recommended)\n",
    "        x = lstm_out\n",
    "        inputs = [input_x]\n",
    "\n",
    "    # --- Output Projection ---\n",
    "    # Map back to 1 channel (Headway)\n",
    "    outputs = layers.TimeDistributed(layers.Dense(1, activation=\"linear\"))(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=config.learning_rate), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# Re-build model\n",
    "# Note: We pass input_shape_t now!\n",
    "model = build_model(config, input_shape_x=X.shape[1:], input_shape_t=T.shape[1:])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(f\"Training model for {config.epochs} epochs...\")\n",
    "\n",
    "# callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=[X, T],  # <--- Pass BOTH inputs here\n",
    "    y=Y, \n",
    "    validation_split=0.2, \n",
    "    epochs=config.epochs, \n",
    "    batch_size=config.batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a prediction\n",
    "# select a random sample\n",
    "sample_idx = np.random.randint(0, len(X))\n",
    "x_sample = X[sample_idx:sample_idx+1]\n",
    "t_sample = T[sample_idx:sample_idx+1] # Get corresponding T\n",
    "y_true = Y[sample_idx:sample_idx+1]\n",
    "\n",
    "y_pred = model.predict([x_sample, t_sample]) # Pass BOTH inputs\n",
    "\n",
    "# shapes (1, Time, Space, 1)\n",
    "# lets plot the space-time heatmap for truth vs Pred\n",
    "# we transpose to have time on x-axis and space (stations) on y axis\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ground truth\n",
    "sns.heatmap(y_true[0, :, :, 0].T, ax=axes[0], cmap=\"viridis\")\n",
    "axes[0].set_title(f\"Ground Truth (Sample {sample_idx})\")\n",
    "axes[0].set_xlabel(\"Time Step (Future)\")\n",
    "axes[0].set_ylabel(\"Station ID\")\n",
    "\n",
    "# prediction\n",
    "sns.heatmap(y_pred[0, :, :, 0].T, ax=axes[1], cmap=\"viridis\")\n",
    "axes[1].set_title(f\"Prediction (Sample {sample_idx})\")\n",
    "axes[1].set_xlabel(\"Time Step (Future)\")\n",
    "axes[1].set_ylabel(\"Station ID\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
