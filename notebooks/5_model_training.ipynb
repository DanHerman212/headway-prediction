{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c0425b",
   "metadata": {},
   "source": [
    "# Model Training & Experimentation Framework\n",
    "\n",
    "This notebook implements the \"Experiment Factory\" for the Headway Prediction model. \n",
    "It is designed to support the ablation analysis defined in the project abstract, allowing us to vary:\n",
    "1.  **Lookback Window ($L$):** 30, 45, 60 minutes.\n",
    "2.  **Input Features:** With or without Terminal Headways ($T$).\n",
    "3.  **Prediction Horizon:** Recursive prediction up to 60 minutes.\n",
    "\n",
    "We start by importing the necessary libraries, including TensorFlow/Keras for the Deep Learning components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d81a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set random seeds for reproduceability\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration class \n",
    "# We define an `ExperimentConfig` class to encapsulate all hyperparameters. This makes it easy to switch between different experimental setups (e.g., changing the lookback window or enabling/disabling terminal headways) without rewriting code. \n",
    "\n",
    "class ExperimentConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lookback_mins=30, # Paper: 30 minutes\n",
    "        forecast_mins=15, # Paper: 15 minutes (Single Step)\n",
    "        time_bin_size_min=5,\n",
    "        use_terminal_headway=True,\n",
    "        batch_size=32,    # Paper: 32\n",
    "        epochs=32, \n",
    "        learning_rate=0.001 # Paper: 0.001\n",
    "    ):\n",
    "        self.lookback_mins = lookback_mins\n",
    "        self.forecast_mins = forecast_mins\n",
    "        self.time_bin_size_min = time_bin_size_min\n",
    "        self.use_terminal_headway = use_terminal_headway\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # calculated properties\n",
    "        self.lookback_bins = lookback_mins // time_bin_size_min\n",
    "        self.forecast_bins = forecast_mins // time_bin_size_min\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"ExperimentalConfig(L={self.lookback_mins}m, \"\n",
    "                f\"F={self.forecast_mins}m, \"\n",
    "                f\"Use_T={self.use_terminal_headway}\")\n",
    "\n",
    "# create baseline configuration (exp-A1)\n",
    "config = ExperimentConfig(\n",
    "    lookback_mins=30, # Baseline from Table 1\n",
    "    forecast_mins=15, # Baseline from Table 1\n",
    "    use_terminal_headway=True\n",
    ")\n",
    "\n",
    "print(f\"Active Configuration {config}\")\n",
    "print(f\"Lookback Bins: {config.lookback_bins}\")\n",
    "print(f\"Forecast Bins: {config.forecast_bins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab463b",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation\n",
    "\n",
    "We load the preprocessed matrix and schedule data. We then use the `create_dataset` function (adapted from the EDA notebook) to generate the tensors based on the active `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f41e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "MATRIX_PATH = \"../data/headway_matrix_bidirectional.npy\"\n",
    "SCHEDULE_PATH = \"../data/target_terminal_headways.csv\"\n",
    "GLOBAL_START_TIME = \"2025-06-06 00:00:00\"\n",
    "\n",
    "def load_and_process_data(config):\n",
    "    \"\"\"\n",
    "    loads the pre-processed matrix and prepares the (X, T, Y) tensors based on the config.\n",
    "    \n",
    "    NOTE: Now uses the Bidirectional Matrix (Time, Space, 2)\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # 1. Load Matrix (Bidirectional)\n",
    "    # Shape: (Time, Space, 2)\n",
    "    matrix = np.load(MATRIX_PATH)\n",
    "\n",
    "    # 2. Normalize to [0, 1] (MinMax Scaling)\n",
    "    # We assume max headway is 20 mins (1200s) based on our outlier removal\n",
    "    MAX_HEADWAY_MIN = 20.0\n",
    "    matrix = np.clip(matrix, 0, MAX_HEADWAY_MIN) / MAX_HEADWAY_MIN\n",
    "\n",
    "    # 3 load and align schedule\n",
    "    schedule_df = pd.read_csv(SCHEDULE_PATH)\n",
    "    schedule_df['datetime'] = pd.to_datetime(schedule_df['service_date']) + \\\n",
    "                              pd.to_timedelta(schedule_df['departure_seconds'], unit='s')\n",
    "    schedule_df = schedule_df.set_index('datetime').sort_index()\n",
    "    schedule_df = schedule_df[~schedule_df.index.duplicated(keep='first')]\n",
    "    schedule_df = schedule_df[schedule_df.index >= GLOBAL_START_TIME]\n",
    "\n",
    "    # fill nans and resample\n",
    "    schedule_df['scheduled_headway_min'] = schedule_df['scheduled_headway_min'].bfill()\n",
    "    \n",
    "    # Normalize Schedule too\n",
    "    schedule_df['scheduled_headway_min'] = np.clip(schedule_df['scheduled_headway_min'], 0, MAX_HEADWAY_MIN) / MAX_HEADWAY_MIN\n",
    "    \n",
    "    time_coords = pd.date_range(start=GLOBAL_START_TIME, periods=matrix.shape[0], freq=f\"{config.time_bin_size_min}min\")\n",
    "    schedule_resampled = schedule_df['scheduled_headway_min'].resample(f'{config.time_bin_size_min}min').ffill()\n",
    "    schedule_aligned = schedule_resampled.reindex(time_coords, method='ffill').bfill().values\n",
    "\n",
    "    # 4. create tensors\n",
    "    print(f\"Generating tensors with L={config.lookback_bins} bins, F={config.forecast_bins} bins...\")\n",
    "    X, T, Y = [], [], []\n",
    "\n",
    "    for i in range(config.lookback_bins, len(matrix) - config.forecast_bins):\n",
    "        # Input X: Past L steps\n",
    "        # Shape: (L, Space, 2) -> We add channel dim later\n",
    "        X.append(matrix[i-config.lookback_bins:i, :, :])\n",
    "        \n",
    "        # Input T: Future F steps\n",
    "        # We need T to have shape (F, 2, 1) to match the architecture\n",
    "        # Currently schedule is 1D (Time). We duplicate for both directions as a baseline approximation\n",
    "        # (Ideally we would have separate N/S schedules, but for now we assume symmetric dispatching intent)\n",
    "        t_window = schedule_aligned[i:i+config.forecast_bins]\n",
    "        t_window_2d = np.stack([t_window, t_window], axis=1) # (F, 2)\n",
    "        T.append(t_window_2d)\n",
    "        \n",
    "        # Target Y: Future F steps\n",
    "        Y.append(matrix[i:i+config.forecast_bins, :, :])\n",
    "\n",
    "    X = np.array(X)[..., np.newaxis] #(Batch, Time, Space, Dir, 1)\n",
    "    T = np.array(T)[..., np.newaxis] #(Batch, Time, Dir, 1)\n",
    "    Y = np.array(Y)[..., np.newaxis] #(Batch, Time, Space, Dir, 1)\n",
    "\n",
    "    return X, T, Y\n",
    "\n",
    "# execute data loading\n",
    "X, T, Y = load_and_process_data(config)\n",
    "\n",
    "print(f\"\\nData Shapes:\")\n",
    "print(f\"X (Context): {X.shape}\")\n",
    "print(f\"T (Intent): {T.shape}\")\n",
    "print(f\"Y (Target): {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9c6b7",
   "metadata": {},
   "source": [
    "## 3. Model Architecture (ConvLSTM)\n",
    "\n",
    "We define the `build_model` function. It constructs a Keras model using `ConvLSTM2D` layers to capture spatiotemporal dependencies.\n",
    "\n",
    "**Key Features:**\n",
    "*   **5D Input Handling:** `ConvLSTM2D` expects `(Batch, Time, Rows, Cols, Channels)`. Since our data is 1D space (Stations), we reshape it to `(Time, Stations, 1, 1)` inside the model.\n",
    "*   **Dual Input Support:** If `config.use_terminal_headway` is True, the model creates a secondary input branch for the schedule ($T$), processes it, and concatenates it with the main traffic flow features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57874e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metro_headway_net(\n",
    "    history_steps=30,   # 'L' in paper\n",
    "    future_steps=15,    # 'F' in paper\n",
    "    distance_bins=64,   # 'Nd' in paper\n",
    "    directions=2,       # 'Ndir' in paper\n",
    "    filters=32          # Table 1\n",
    "):\n",
    "    # --- Input 1: Historical Headways (The \"Video\" of the metro line) ---\n",
    "    # Shape: (Batch, 30, 64, 2, 1) -> (Time, Rows, Cols, Channels)\n",
    "    input_history = layers.Input(\n",
    "        shape=(history_steps, distance_bins, directions, 1), \n",
    "        name=\"history_input\"\n",
    "    )\n",
    "\n",
    "    # --- Input 2: Future Terminal Headways (The \"Schedule\") ---\n",
    "    # Shape: (Batch, 15, 2, 1) -> Future scheduled departures at terminals\n",
    "    input_terminal = layers.Input(\n",
    "        shape=(future_steps, directions, 1), \n",
    "        name=\"terminal_input\"\n",
    "    )\n",
    "\n",
    "    # --- Encoder Branch: ConvLSTM Layers ---\n",
    "    # Layer 1: Returns sequences to feed the next layer\n",
    "    # Kernel Size (3, 1) slides over distance but keeps directions separate\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=filters,\n",
    "        kernel_size=(3, 1),\n",
    "        padding='same',\n",
    "        return_sequences=True,\n",
    "        activation='relu', \n",
    "        name=\"convlstm_1\"\n",
    "    )(input_history)\n",
    "\n",
    "    # Layer 2: We only need the FINAL state (summary of the last 30 mins)\n",
    "    # So we set return_sequences=False\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=filters,\n",
    "        kernel_size=(3, 1),\n",
    "        padding='same',\n",
    "        return_sequences=False, # Compress time dimension to single vector\n",
    "        activation='relu',\n",
    "        name=\"convlstm_2\"\n",
    "    )(x)\n",
    "    \n",
    "    # Current shape of x: (Batch, 64, 2, 32) -> Spatial Map * Filters\n",
    "\n",
    "    # --- Flattening for Merge ---\n",
    "    # We must flatten the spatial features to concatenate with the schedule\n",
    "    x_flat = layers.Flatten(name=\"flatten_history\")(x)\n",
    "\n",
    "    # We also flatten the Terminal Schedule\n",
    "    t_flat = layers.Flatten(name=\"flatten_terminal\")(input_terminal)\n",
    "\n",
    "    # --- The \"Secret Sauce\": Fusion ---\n",
    "    # Concatenate the LSTM context with the Future Schedule\n",
    "    concat = layers.Concatenate(name=\"fusion_layer\")([x_flat, t_flat])\n",
    "\n",
    "    # --- Decoder: Dense Layer ---\n",
    "    # Project to total output dimensions: 15 * 64 * 2\n",
    "    # Paper explicitly calls for a \"Dense Layer\" here\n",
    "    output_dim = future_steps * distance_bins * directions\n",
    "    \n",
    "    dense_out = layers.Dense(output_dim, activation='linear', name=\"dense_projection\")(concat)\n",
    "\n",
    "    # --- Final Reshape ---\n",
    "    # Reshape back to the grid format: (Batch, 15, 64, 2, 1)\n",
    "    output = layers.Reshape(\n",
    "        (future_steps, distance_bins, directions, 1), \n",
    "        name=\"final_output\"\n",
    "    )(dense_out)\n",
    "\n",
    "    # --- Compile Model ---\n",
    "    model = keras.Model(inputs=[input_history, input_terminal], outputs=output)\n",
    "    \n",
    "    # Optimizer and Loss from Section 2.2 / Table 1\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001) \n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate\n",
    "model = build_metro_headway_net(\n",
    "    history_steps=config.lookback_bins,\n",
    "    future_steps=config.forecast_bins,\n",
    "    distance_bins=X.shape[2], # Should be 64\n",
    "    directions=X.shape[3],    # Should be 2\n",
    "    filters=32\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(f\"Training model for {config.epochs} epochs...\")\n",
    "\n",
    "# callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=[X, T],  # <--- Pass BOTH inputs here\n",
    "    y=Y, \n",
    "    validation_split=0.2, \n",
    "    epochs=config.epochs, \n",
    "    batch_size=config.batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a prediction\n",
    "# select a random sample\n",
    "sample_idx = np.random.randint(0, len(X))\n",
    "x_sample = X[sample_idx:sample_idx+1]\n",
    "t_sample = T[sample_idx:sample_idx+1] # Get corresponding T\n",
    "y_true = Y[sample_idx:sample_idx+1]\n",
    "\n",
    "y_pred = model.predict([x_sample, t_sample]) # Pass BOTH inputs\n",
    "\n",
    "# shapes (1, Time, Space, Dir, 1)\n",
    "# lets plot the space-time heatmap for truth vs Pred (Northbound: Dir=0)\n",
    "# we transpose to have time on x-axis and space (stations) on y axis\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ground truth (Northbound)\n",
    "sns.heatmap(y_true[0, :, :, 0, 0].T, ax=axes[0], cmap=\"viridis\", vmin=0, vmax=1)\n",
    "axes[0].set_title(\"Ground Truth (Northbound)\")\n",
    "axes[0].set_xlabel(\"Time (Future)\")\n",
    "axes[0].set_ylabel(\"Space (Stations)\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# prediction (Northbound)\n",
    "sns.heatmap(y_pred[0, :, :, 0, 0].T, ax=axes[1], cmap=\"viridis\", vmin=0, vmax=1)\n",
    "axes[1].set_title(\"Prediction (Northbound)\")\n",
    "axes[1].set_xlabel(\"Time (Future)\")\n",
    "axes[1].set_yticks([])\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1957223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Re-create Validation Split (Last 20%)\n",
    "val_split_idx = int(len(X) * 0.8)\n",
    "X_val = X[val_split_idx:]\n",
    "T_val = T[val_split_idx:]\n",
    "Y_val = Y[val_split_idx:]\n",
    "\n",
    "print(f\"Evaluating on {len(X_val)} validation samples...\")\n",
    "\n",
    "# 2. Predict\n",
    "# Note: We pass BOTH inputs [X_val, T_val]\n",
    "Y_pred_norm = model.predict([X_val, T_val], verbose=1)\n",
    "\n",
    "# 3. Inverse Transform (Normalized -> Minutes -> Seconds)\n",
    "# Recall: We divided by 20.0 to normalize\n",
    "SCALING_FACTOR = 20.0\n",
    "Y_val_sec = Y_val * SCALING_FACTOR * 60\n",
    "Y_pred_sec = Y_pred_norm * SCALING_FACTOR * 60\n",
    "\n",
    "# 4. Calculate Metrics\n",
    "# Flatten arrays because metrics expect 1D arrays\n",
    "rmse = np.sqrt(mean_squared_error(Y_val_sec.flatten(), Y_pred_sec.flatten()))\n",
    "r2 = r2_score(Y_val_sec.flatten(), Y_pred_sec.flatten())\n",
    "\n",
    "print(\"\\n--- Experiment Results ---\")\n",
    "print(f\"RMSE: {rmse:.2f} seconds\")\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
