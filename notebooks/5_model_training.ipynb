{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c0425b",
   "metadata": {},
   "source": [
    "# Model Training & Experimentation Framework\n",
    "\n",
    "This notebook implements the \"Experiment Factory\" for the Headway Prediction model. \n",
    "It is designed to support the ablation analysis defined in the project abstract, allowing us to vary:\n",
    "1.  **Lookback Window ($L$):** 30, 45, 60 minutes.\n",
    "2.  **Input Features:** With or without Terminal Headways ($T$).\n",
    "3.  **Prediction Horizon:** Recursive prediction up to 60 minutes.\n",
    "\n",
    "We start by importing the necessary libraries, including TensorFlow/Keras for the Deep Learning components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d81a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set random seeds for reproduceability\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration class \n",
    "# We define an `ExperimentConfig` class to encapsulate all hyperparameters. This makes it easy to switch between different experimental setups (e.g., changing the lookback window or enabling/disabling terminal headways) without rewriting code. \n",
    "\n",
    "class ExperimentConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lookback_mins=30, # Paper: 30 minutes\n",
    "        forecast_mins=15, # Paper: 15 minutes (Single Step)\n",
    "        time_bin_size_min=5, # Reverted to 5 minutes for stability\n",
    "        use_terminal_headway=True,\n",
    "        batch_size=32,    # Paper: 32\n",
    "        epochs=32, \n",
    "        learning_rate=0.001 # Paper: 0.001\n",
    "    ):\n",
    "        self.lookback_mins = lookback_mins\n",
    "        self.forecast_mins = forecast_mins\n",
    "        self.time_bin_size_min = time_bin_size_min\n",
    "        self.use_terminal_headway = use_terminal_headway\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # calculated properties\n",
    "        self.lookback_bins = lookback_mins // time_bin_size_min\n",
    "        self.forecast_bins = forecast_mins // time_bin_size_min\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"ExperimentalConfig(L={self.lookback_mins}m, \"\n",
    "                f\"F={self.forecast_mins}m, \"\n",
    "                f\"Use_T={self.use_terminal_headway}\")\n",
    "\n",
    "# create baseline configuration (exp-A1)\n",
    "config = ExperimentConfig(\n",
    "    lookback_mins=30, # Baseline from Table 1\n",
    "    forecast_mins=15, # Baseline from Table 1\n",
    "    use_terminal_headway=True\n",
    ")\n",
    "\n",
    "print(f\"Active Configuration {config}\")\n",
    "print(f\"Lookback Bins: {config.lookback_bins}\")\n",
    "print(f\"Forecast Bins: {config.forecast_bins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab463b",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation\n",
    "\n",
    "We load the preprocessed matrix and schedule data. We then use the `create_dataset` function (adapted from the EDA notebook) to generate the tensors based on the active `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f41e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "MATRIX_PATH = \"../data/headway_matrix_bidirectional.npy\"\n",
    "SCHEDULE_PATH = \"../data/target_terminal_headways.csv\"\n",
    "GLOBAL_START_TIME = \"2025-06-06 00:00:00\"\n",
    "\n",
    "def create_dataset(matrix, schedule_aligned, config):\n",
    "    \"\"\"\n",
    "    Creates (X, T, Y) tensors from the matrix and aligned schedule.\n",
    "    X: Historical Headways (Batch, Lookback, Space, Dir, 1)\n",
    "    T: Future Terminal Headways (Batch, Forecast, Dir, 1)\n",
    "    Y: Future Headways (Batch, Forecast, Space, Dir, 1)\n",
    "    \"\"\"\n",
    "    X, T, Y = [], [], []\n",
    "    \n",
    "    # We need to ensure we have enough data for lookback AND forecast\n",
    "    # Range: [Lookback, Total - Forecast]\n",
    "    total_bins = len(matrix)\n",
    "    start_idx = config.lookback_bins\n",
    "    end_idx = total_bins - config.forecast_bins\n",
    "    \n",
    "    for i in range(start_idx, end_idx):\n",
    "        # 1. History Window (i-L to i)\n",
    "        # Shape: (L, Space, Dir)\n",
    "        x_window = matrix[i-config.lookback_bins : i]\n",
    "        \n",
    "        # 2. Target Window (i to i+F)\n",
    "        # Shape: (F, Space, Dir)\n",
    "        y_window = matrix[i : i+config.forecast_bins]\n",
    "        \n",
    "        # 3. Schedule Window (i to i+F)\n",
    "        # Shape: (F, Dir) -> We need to slice the aligned schedule\n",
    "        # The schedule is already aligned to the matrix time index\n",
    "        t_window = schedule_aligned[i : i+config.forecast_bins]\n",
    "        \n",
    "        X.append(x_window)\n",
    "        Y.append(y_window)\n",
    "        T.append(t_window)\n",
    "        \n",
    "    # Convert to numpy arrays and add channel dimension\n",
    "    X = np.array(X)[..., np.newaxis] # (Batch, L, Space, Dir, 1)\n",
    "    Y = np.array(Y)[..., np.newaxis] # (Batch, F, Space, Dir, 1)\n",
    "    T = np.array(T)[..., np.newaxis] # (Batch, F, Dir, 1)\n",
    "    \n",
    "    return X, T, Y\n",
    "\n",
    "def load_and_process_data(config):\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # 1. Load Matrix (Bidirectional)\n",
    "    # Shape: (Time, Space, 2)\n",
    "    matrix = np.load(MATRIX_PATH)\n",
    "\n",
    "    # 2. Normalize to [0, 1] (MinMax Scaling)\n",
    "    # We assume max headway is around 20 mins for normalization stability\n",
    "    # Real values can be higher, but we clip to avoid outliers\n",
    "    MAX_HEADWAY_MIN = 20.0\n",
    "    matrix = np.clip(matrix, 0, MAX_HEADWAY_MIN) / MAX_HEADWAY_MIN\n",
    "\n",
    "    # 3 load and align schedule\n",
    "    schedule_df = pd.read_csv(SCHEDULE_PATH)\n",
    "    \n",
    "    # Convert departure seconds to absolute datetime\n",
    "    # We need to map the schedule to the matrix time bins\n",
    "    # The schedule has 'service_date' and 'departure_seconds'\n",
    "    schedule_df['datetime'] = pd.to_datetime(schedule_df['service_date']) + \\\n",
    "                              pd.to_timedelta(schedule_df['departure_seconds'], unit='s')\n",
    "    \n",
    "    # Set index and sort\n",
    "    schedule_df = schedule_df.set_index('datetime').sort_index()\n",
    "    \n",
    "    # Filter duplicates (if any)\n",
    "    schedule_df = schedule_df[~schedule_df.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Filter to start time\n",
    "    schedule_df = schedule_df[schedule_df.index >= GLOBAL_START_TIME]\n",
    "\n",
    "    # fill nans and resample\n",
    "    schedule_df['scheduled_headway_min'] = schedule_df['scheduled_headway_min'].bfill()\n",
    "    \n",
    "    # Normalize Schedule too\n",
    "    schedule_df['scheduled_headway_min'] = np.clip(schedule_df['scheduled_headway_min'], 0, MAX_HEADWAY_MIN) / MAX_HEADWAY_MIN\n",
    "    \n",
    "    # Resample to match matrix bins (5 min)\n",
    "    # We use forward fill because the schedule is sparse (terminal departures)\n",
    "    # But wait, 'terminal departures' is a single point in space.\n",
    "    # We need a continuous time series of \"what is the scheduled headway at the terminal right now?\"\n",
    "    # Actually, the paper implies T is \"Scheduled Headway\".\n",
    "    # If we resample to 5 mins, we just take the scheduled value active at that time.\n",
    "    \n",
    "    time_coords = pd.date_range(start=GLOBAL_START_TIME, periods=matrix.shape[0], freq=f\"{config.time_bin_size_min}min\")\n",
    "    \n",
    "    # Reindex schedule to match matrix time\n",
    "    # We use ffill to propagate the last known scheduled headway\n",
    "    schedule_resampled = schedule_df['scheduled_headway_min'].resample(f'{config.time_bin_size_min}min').ffill()\n",
    "    \n",
    "    # Align with Matrix Time Coords\n",
    "    schedule_aligned_1d = schedule_resampled.reindex(time_coords, method='ffill').bfill().values\n",
    "    \n",
    "    # Create 2D Schedule (Time, 2)\n",
    "    # Since we only have one schedule file (likely Northbound or combined), \n",
    "    # and the matrix has 2 directions, we need to be careful.\n",
    "    # The current CSV likely has mixed directions or just one.\n",
    "    # Let's assume for now the schedule applies to both or we duplicate it.\n",
    "    # TODO: Refine schedule processing to distinguish directions if CSV has 'direction_id'\n",
    "    # For now, we duplicate the 1D schedule for both directions to match shape (Time, 2)\n",
    "    schedule_aligned = np.stack([schedule_aligned_1d, schedule_aligned_1d], axis=1)\n",
    "\n",
    "    # 4. Create Tensors\n",
    "    print(f\"Creating tensors with L={config.lookback_bins}, F={config.forecast_bins}...\")\n",
    "    X, T, Y = create_dataset(matrix, schedule_aligned, config)\n",
    "    \n",
    "    return X, T, Y\n",
    "\n",
    "# execute data loading\n",
    "X, T, Y = load_and_process_data(config)\n",
    "\n",
    "print(f\"Data Shapes:\")\n",
    "print(f\"X (History): {X.shape}\")\n",
    "print(f\"T (Schedule): {T.shape}\")\n",
    "print(f\"Y (Target):  {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9c6b7",
   "metadata": {},
   "source": [
    "## 3. Model Architecture (ConvLSTM)\n",
    "\n",
    "We define the `build_model` function. It constructs a Keras model using `ConvLSTM2D` layers to capture spatiotemporal dependencies.\n",
    "\n",
    "**Key Features:**\n",
    "*   **5D Input Handling:** `ConvLSTM2D` expects `(Batch, Time, Rows, Cols, Channels)`. Since our data is 1D space (Stations), we reshape it to `(Time, Stations, 1, 1)` inside the model.\n",
    "*   **Dual Input Support:** If `config.use_terminal_headway` is True, the model creates a secondary input branch for the schedule ($T$), processes it, and concatenates it with the main traffic flow features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57874e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metro_headway_net(\n",
    "    history_steps=30,   # 'L' in paper\n",
    "    future_steps=15,    # 'F' in paper\n",
    "    distance_bins=64,   # 'Nd' in paper\n",
    "    directions=2,       # 'Ndir' in paper\n",
    "    filters=32          # Table 1\n",
    "):\n",
    "    # --- Input 1: Historical Headways (The \"Video\" of the metro line) ---\n",
    "    # Shape: (Batch, 30, 64, 2, 1) -> (Time, Rows, Cols, Channels)\n",
    "    input_history = layers.Input(\n",
    "        shape=(history_steps, distance_bins, directions, 1), \n",
    "        name=\"history_input\"\n",
    "    )\n",
    "\n",
    "    # --- Input 2: Future Terminal Headways (The \"Schedule\") ---\n",
    "    # Shape: (Batch, 15, 2, 1) -> Future scheduled departures at terminals\n",
    "    input_terminal = layers.Input(\n",
    "        shape=(future_steps, directions, 1), \n",
    "        name=\"terminal_input\"\n",
    "    )\n",
    "\n",
    "    # --- Encoder Branch: ConvLSTM Layers ---\n",
    "    # Layer 1: Returns sequences to feed the next layer\n",
    "    # Kernel Size (3, 1) slides over distance but keeps directions separate\n",
    "    # Reverted activation to 'tanh' (default) for stability\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=filters,\n",
    "        kernel_size=(3, 1),\n",
    "        padding='same',\n",
    "        return_sequences=True,\n",
    "        activation='tanh', \n",
    "        name=\"convlstm_1\"\n",
    "    )(input_history)\n",
    "\n",
    "    # Layer 2: We only need the FINAL state (summary of the last 30 mins)\n",
    "    # So we set return_sequences=False\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=filters,\n",
    "        kernel_size=(3, 1),\n",
    "        padding='same',\n",
    "        return_sequences=False, # Compress time dimension to single vector\n",
    "        activation='tanh',\n",
    "        name=\"convlstm_2\"\n",
    "    )(x)\n",
    "    \n",
    "    # Current shape of x: (Batch, 64, 2, 32) -> Spatial Map * Filters\n",
    "\n",
    "    # --- Flattening for Merge ---\n",
    "    # We must flatten the spatial features to concatenate with the schedule\n",
    "    x_flat = layers.Flatten(name=\"flatten_history\")(x)\n",
    "\n",
    "    # We also flatten the Terminal Schedule\n",
    "    t_flat = layers.Flatten(name=\"flatten_terminal\")(input_terminal)\n",
    "\n",
    "    # --- The \"Secret Sauce\": Fusion ---\n",
    "    # Concatenate the LSTM context with the Future Schedule\n",
    "    concat = layers.Concatenate(name=\"fusion_layer\")([x_flat, t_flat])\n",
    "\n",
    "    # --- Decoder: Dense Layer ---\n",
    "    # Project to total output dimensions: 15 * 64 * 2\n",
    "    # Paper explicitly calls for a \"Dense Layer\" here\n",
    "    output_dim = future_steps * distance_bins * directions\n",
    "    \n",
    "    dense_out = layers.Dense(output_dim, activation='linear', name=\"dense_projection\")(concat)\n",
    "\n",
    "    # --- Final Reshape ---\n",
    "    # Reshape back to the grid format: (Batch, 15, 64, 2, 1)\n",
    "    output = layers.Reshape(\n",
    "        (future_steps, distance_bins, directions, 1), \n",
    "        name=\"final_output\"\n",
    "    )(dense_out)\n",
    "\n",
    "    # --- Compile Model ---\n",
    "    model = keras.Model(inputs=[input_history, input_terminal], outputs=output)\n",
    "    \n",
    "    # Optimizer and Loss from Section 2.2 / Table 1\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001) \n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate\n",
    "model = build_metro_headway_net(\n",
    "    history_steps=config.lookback_bins,\n",
    "    future_steps=config.forecast_bins,\n",
    "    distance_bins=X.shape[2], # Should be 64\n",
    "    directions=X.shape[3],    # Should be 2\n",
    "    filters=32\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(f\"Training model for {config.epochs} epochs...\")\n",
    "\n",
    "# Mixed Precision Removed for Stability\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Re-compile model to ensure clean state\n",
    "model = build_metro_headway_net(\n",
    "    history_steps=config.lookback_bins,\n",
    "    future_steps=config.forecast_bins,\n",
    "    distance_bins=64,\n",
    "    directions=2,\n",
    "    filters=32\n",
    ")\n",
    "\n",
    "# Split Data (80/20)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "T_train, T_val = T[:train_size], T[train_size:]\n",
    "Y_train, Y_val = Y[:train_size], Y[train_size:]\n",
    "\n",
    "# callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=[X_train, T_train], \n",
    "    y=Y_train,\n",
    "    validation_data=([X_val, T_val], Y_val),\n",
    "    batch_size=config.batch_size,\n",
    "    epochs=config.epochs, \n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a prediction\n",
    "# select a random sample\n",
    "sample_idx = np.random.randint(0, len(X))\n",
    "x_sample = X[sample_idx:sample_idx+1]\n",
    "t_sample = T[sample_idx:sample_idx+1] # Get corresponding T\n",
    "y_true = Y[sample_idx:sample_idx+1]\n",
    "\n",
    "y_pred = model.predict([x_sample, t_sample]) # Pass BOTH inputs\n",
    "\n",
    "# shapes (1, Time, Space, Dir, 1)\n",
    "# lets plot the space-time heatmap for truth vs Pred (Northbound: Dir=0)\n",
    "# we transpose to have time on x-axis and space (stations) on y axis\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ground truth (Northbound)\n",
    "sns.heatmap(y_true[0, :, :, 0, 0].T, ax=axes[0], cmap=\"viridis\", vmin=0, vmax=1)\n",
    "axes[0].set_title(\"Ground Truth (Northbound)\")\n",
    "axes[0].set_xlabel(\"Time (Future)\")\n",
    "axes[0].set_ylabel(\"Space (Stations)\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# prediction (Northbound)\n",
    "sns.heatmap(y_pred[0, :, :, 0, 0].T, ax=axes[1], cmap=\"viridis\", vmin=0, vmax=1)\n",
    "axes[1].set_title(\"Prediction (Northbound)\")\n",
    "axes[1].set_xlabel(\"Time (Future)\")\n",
    "axes[1].set_yticks([])\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1957223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Re-create Validation Split (Last 20%)\n",
    "val_split_idx = int(len(X) * 0.8)\n",
    "X_val = X[val_split_idx:]\n",
    "T_val = T[val_split_idx:]\n",
    "Y_val = Y[val_split_idx:]\n",
    "\n",
    "print(f\"Evaluating on {len(X_val)} validation samples...\")\n",
    "\n",
    "# 2. Predict\n",
    "# Note: We pass BOTH inputs [X_val, T_val]\n",
    "Y_pred_norm = model.predict([X_val, T_val], verbose=1)\n",
    "\n",
    "# 3. Inverse Transform (Normalized -> Minutes -> Seconds)\n",
    "# Recall: We divided by 20.0 to normalize\n",
    "SCALING_FACTOR = 20.0\n",
    "Y_val_sec = Y_val * SCALING_FACTOR * 60\n",
    "Y_pred_sec = Y_pred_norm * SCALING_FACTOR * 60\n",
    "\n",
    "# 4. Calculate Metrics\n",
    "# Flatten arrays because metrics expect 1D arrays\n",
    "rmse = np.sqrt(mean_squared_error(Y_val_sec.flatten(), Y_pred_sec.flatten()))\n",
    "r2 = r2_score(Y_val_sec.flatten(), Y_pred_sec.flatten())\n",
    "\n",
    "print(\"\\n--- Experiment Results ---\")\n",
    "print(f\"RMSE: {rmse:.2f} seconds\")\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
