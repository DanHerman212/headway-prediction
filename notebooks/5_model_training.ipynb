{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "LOOKBACK_MINS = 30\n",
    "FORECAST_MINS = 15\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# File Paths\n",
    "DATA_DIR = \"../data\"\n",
    "HEADWAY_MATRIX_FILE = os.path.join(DATA_DIR, \"headway_matrix_full.npy\")\n",
    "SCHEDULE_MATRIX_FILE = os.path.join(DATA_DIR, \"schedule_matrix_full.npy\")\n",
    "\n",
    "print(\"Imports complete.\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "print(f\"Loading Headway Matrix from {HEADWAY_MATRIX_FILE}...\")\n",
    "headway_matrix = np.load(HEADWAY_MATRIX_FILE)\n",
    "print(f\"Headway Matrix Shape: {headway_matrix.shape}\") # (Time, Stations, Directions, Channels)\n",
    "\n",
    "print(f\"Loading Schedule Matrix from {SCHEDULE_MATRIX_FILE}...\")\n",
    "schedule_matrix = np.load(SCHEDULE_MATRIX_FILE)\n",
    "print(f\"Schedule Matrix Shape: {schedule_matrix.shape}\") # (Time, Directions, Channels)\n",
    "\n",
    "# Verify shapes match in time dimension\n",
    "assert headway_matrix.shape[0] == schedule_matrix.shape[0], \"Time dimensions do not match!\"\n",
    "\n",
    "# Normalize Data (if not already normalized)\n",
    "# Data from Notebook 3 is normalized to [0, 1] using MAX_HEADWAY = 30 minutes.\n",
    "SCALER = 30.0 # Minutes\n",
    "\n",
    "max_headway = np.max(headway_matrix)\n",
    "print(f\"Max Headway in Data: {max_headway}\")\n",
    "\n",
    "# If max > 1, we need to normalize.\n",
    "if max_headway > 1.0:\n",
    "    print(f\"Normalizing data to [0, 1] range using SCALER={SCALER}...\")\n",
    "    headway_matrix = headway_matrix / SCALER\n",
    "    \n",
    "    max_schedule = np.max(schedule_matrix)\n",
    "    print(f\"Max Schedule Value: {max_schedule}\")\n",
    "    if max_schedule > 1.0:\n",
    "         schedule_matrix = schedule_matrix / SCALER\n",
    "else:\n",
    "    print(\"Data appears to be already normalized.\")\n",
    "\n",
    "print(\"Data Loading & Normalization Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 3. Create Datasets\n",
    "    def create_dataset(headway_data, schedule_data, start_index, end_index, batch_size):\n",
    "        # Alignment Logic:\n",
    "        # Target Y[t]: Future Headways from t to t+15.\n",
    "        # Input X[t]: Past Headways from t-30 to t.\n",
    "        # Input T[t]: Future Schedule from t to t+15.\n",
    "        \n",
    "        # We align the datasets so that for a given index `i`:\n",
    "        # Y starts at `i`\n",
    "        # T starts at `i`\n",
    "        # X starts at `i - 30` (so it ends at `i`)\n",
    "        \n",
    "        ds_x = keras.utils.timeseries_dataset_from_array(\n",
    "            data=headway_data,\n",
    "            targets=None,\n",
    "            sequence_length=LOOKBACK_MINS,\n",
    "            sequence_stride=1,\n",
    "            sampling_rate=1,\n",
    "            batch_size=batch_size,\n",
    "            start_index=start_index - LOOKBACK_MINS,\n",
    "            end_index=end_index - FORECAST_MINS\n",
    "        )\n",
    "        \n",
    "        ds_t = keras.utils.timeseries_dataset_from_array(\n",
    "            data=schedule_data,\n",
    "            targets=None,\n",
    "            sequence_length=FORECAST_MINS,\n",
    "            sequence_stride=1,\n",
    "            sampling_rate=1,\n",
    "            batch_size=batch_size,\n",
    "            start_index=start_index,\n",
    "            end_index=end_index\n",
    "        )\n",
    "        \n",
    "        ds_y = keras.utils.timeseries_dataset_from_array(\n",
    "            data=headway_data,\n",
    "            targets=None,\n",
    "            sequence_length=FORECAST_MINS,\n",
    "            sequence_stride=1,\n",
    "            sampling_rate=1,\n",
    "            batch_size=batch_size,\n",
    "            start_index=start_index,\n",
    "            end_index=end_index\n",
    "        )\n",
    "        \n",
    "        # Zip inputs and targets\n",
    "        # Inputs: (X, T)\n",
    "        # Target: Y\n",
    "        dataset = tf.data.Dataset.zip(((ds_x, ds_t), ds_y))\n",
    "        return dataset\n",
    "\n",
    "    # Split Data\n",
    "    total_samples = len(headway_matrix)\n",
    "    train_split_idx = int(total_samples * 0.8)\n",
    "\n",
    "    # Ensure we have enough history for the first sample\n",
    "    start_idx = LOOKBACK_MINS \n",
    "\n",
    "    print(f\"Creating Training Dataset (0 to {train_split_idx})...\")\n",
    "    train_ds = create_dataset(\n",
    "        headway_matrix, \n",
    "        schedule_matrix, \n",
    "        start_index=start_idx, \n",
    "        end_index=train_split_idx, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    print(f\"Creating Validation Dataset ({train_split_idx} to {total_samples})...\")\n",
    "    val_ds = create_dataset(\n",
    "        headway_matrix, \n",
    "        schedule_matrix, \n",
    "        start_index=train_split_idx, \n",
    "        end_index=total_samples - 1, # Fix: end_index must be < len(data)\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Prefetch for performance\n",
    "    train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(\"Datasets created.\")\n",
    "    for (x, t), y in train_ds.take(1):\n",
    "        print(f\"Input X shape: {x.shape}\")\n",
    "        print(f\"Input T shape: {t.shape}\")\n",
    "        print(f\"Target Y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ece0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build Model\n",
    "def build_model(input_shape_x, input_shape_t, output_shape):\n",
    "    # Input 1: History (Batch, 30, 156, 2, 1)\n",
    "    input_x = layers.Input(shape=input_shape_x, name='history_input')\n",
    "    \n",
    "    # Input 2: Terminal Schedule (Batch, 15, 2, 1)\n",
    "    input_t = layers.Input(shape=input_shape_t, name='terminal_input')\n",
    "    \n",
    "    # --- Encoder (ConvLSTM Branch) ---\n",
    "    # Layer 1\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=16,\n",
    "        kernel_size=(3, 1),\n",
    "        padding='same',\n",
    "        return_sequences=True,\n",
    "        activation='relu'\n",
    "    )(input_x)\n",
    "    \n",
    "    # Layer 2\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 1),\n",
    "        padding='same',\n",
    "        return_sequences=False, # Compress time dimension\n",
    "        activation='relu'\n",
    "    )(x)\n",
    "    # x shape: (Batch, 156, 2, 32)\n",
    "    \n",
    "    # --- Fusion ---\n",
    "    # We want to combine the global schedule info with local spatial features.\n",
    "    # Instead of flattening everything (which causes the 46M params),\n",
    "    # we broadcast the schedule features to every station.\n",
    "    \n",
    "    # 1. Process Schedule\n",
    "    # input_t: (Batch, 15, 2, 1) -> Flatten to (Batch, 30)\n",
    "    # Explicitly calculate size to avoid Flatten() shape inference issues\n",
    "    flat_dim_t = input_shape_t[0] * input_shape_t[1] * input_shape_t[2]\n",
    "    t_flat = layers.Reshape((flat_dim_t,))(input_t)\n",
    "    \n",
    "    # 2. Repeat for each station/direction\n",
    "    # Target spatial grid: 156 * 2 = 312 locations\n",
    "    num_locations = output_shape[1] * output_shape[2]\n",
    "    t_rep = layers.RepeatVector(num_locations)(t_flat) # (Batch, 312, 30)\n",
    "    \n",
    "    # 3. Reshape to match spatial grid\n",
    "    t_grid = layers.Reshape((output_shape[1], output_shape[2], -1))(t_rep) # (Batch, 156, 2, 30)\n",
    "    \n",
    "    # 4. Concatenate with ConvLSTM output\n",
    "    combined = layers.Concatenate(axis=-1)([x, t_grid]) # (Batch, 156, 2, 62)\n",
    "    \n",
    "    # --- Decoder (Convolutional Projection) ---\n",
    "    # We use a 1x1 Conv to project features to the output time steps (15)\n",
    "    # This acts as a \"Per-Pixel Dense Layer\" sharing weights across stations\n",
    "    out_steps = output_shape[0] # 15\n",
    "    \n",
    "    z = layers.Conv2D(\n",
    "        filters=out_steps, \n",
    "        kernel_size=(1, 1), \n",
    "        activation='sigmoid'\n",
    "    )(combined)\n",
    "    # z shape: (Batch, 156, 2, 15)\n",
    "    \n",
    "    # Reshape to (Batch, 15, 156, 2, 1)\n",
    "    # Permute to put Time dim first: (Batch, 15, 156, 2)\n",
    "    z = layers.Permute((3, 1, 2))(z)\n",
    "    \n",
    "    # Add channel dim\n",
    "    output = layers.Reshape(output_shape)(z)\n",
    "    \n",
    "    model = keras.Model(inputs=[input_x, input_t], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Define shapes\n",
    "# X: (30, 156, 2, 1)\n",
    "input_shape_x = (LOOKBACK_MINS, headway_matrix.shape[1], headway_matrix.shape[2], headway_matrix.shape[3])\n",
    "# T: (15, 2, 1)\n",
    "input_shape_t = (FORECAST_MINS, schedule_matrix.shape[1], schedule_matrix.shape[2])\n",
    "# Y: (15, 156, 2, 1)\n",
    "output_shape = (FORECAST_MINS, headway_matrix.shape[1], headway_matrix.shape[2], headway_matrix.shape[3])\n",
    "\n",
    "print(f\"Input X Shape: {input_shape_x}\")\n",
    "print(f\"Input T Shape: {input_shape_t}\")\n",
    "print(f\"Output Y Shape: {output_shape}\")\n",
    "\n",
    "model = build_model(input_shape_x, input_shape_t, output_shape)\n",
    "model.summary()\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='mse',\n",
    "    metrics=[keras.metrics.RootMeanSquaredError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005895b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),\n",
    "    keras.callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Plot History\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss (MSE)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')\n",
    "plt.plot(history.history['val_root_mean_squared_error'], label='Val RMSE')\n",
    "plt.title('RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Evaluate Predictions\n",
    "# Take a batch from validation set\n",
    "for (x_val, t_val), y_val in val_ds.take(1):\n",
    "    predictions = model.predict([x_val, t_val])\n",
    "    break\n",
    "\n",
    "# Select a sample index\n",
    "sample_idx = 0\n",
    "\n",
    "# Extract Ground Truth and Prediction for this sample\n",
    "# Shape: (15, Stations, 2, 1)\n",
    "y_true_sample = y_val[sample_idx]\n",
    "y_pred_sample = predictions[sample_idx]\n",
    "\n",
    "# Select Direction 0 (Northbound) and remove channel dim\n",
    "# Shape: (15, Stations)\n",
    "y_true_d0 = y_true_sample[:, :, 0, 0]\n",
    "y_pred_d0 = y_pred_sample[:, :, 0, 0]\n",
    "\n",
    "# Denormalize if we scaled earlier\n",
    "# SCALER is defined in Cell 2 as 30.0 (minutes)\n",
    "try:\n",
    "    scale_factor = SCALER\n",
    "except NameError:\n",
    "    scale_factor = 30.0 # Default to 30 mins if not defined\n",
    "\n",
    "y_true_d0_min = y_true_d0 * scale_factor \n",
    "y_pred_d0_min = y_pred_d0 * scale_factor\n",
    "\n",
    "# Plot Heatmaps\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Ground Truth\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(y_true_d0_min.T, cmap='viridis', vmin=0, vmax=30)\n",
    "plt.title(f\"Ground Truth (Next 15 Mins)\\nSample {sample_idx}, Dir 0\")\n",
    "plt.xlabel(\"Time Step (Future)\")\n",
    "plt.ylabel(\"Station Index\")\n",
    "\n",
    "# Prediction\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(y_pred_d0_min.T, cmap='viridis', vmin=0, vmax=30)\n",
    "plt.title(f\"Prediction (Next 15 Mins)\\nSample {sample_idx}, Dir 0\")\n",
    "plt.xlabel(\"Time Step (Future)\")\n",
    "plt.ylabel(\"Station Index\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Time Series for a specific station (e.g., middle of the line)\n",
    "station_idx = y_true_d0.shape[1] // 2\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(y_true_d0_min[:, station_idx], label='Ground Truth', marker='o')\n",
    "plt.plot(y_pred_d0_min[:, station_idx], label='Prediction', marker='x')\n",
    "plt.title(f\"Headway Forecast at Station {station_idx} (Dir 0)\")\n",
    "plt.ylabel(\"Headway (Minutes)\")\n",
    "plt.xlabel(\"Minutes into Future\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
