{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c0425b",
   "metadata": {},
   "source": [
    "# Model Training & Experimentation Framework\n",
    "\n",
    "This notebook implements the \"Experiment Factory\" for the Headway Prediction model. \n",
    "It is designed to support the ablation analysis defined in the project abstract, allowing us to vary:\n",
    "1.  **Lookback Window ($L$):** 30, 45, 60 minutes.\n",
    "2.  **Input Features:** With or without Terminal Headways ($T$).\n",
    "3.  **Prediction Horizon:** Recursive prediction up to 60 minutes.\n",
    "\n",
    "We start by importing the necessary libraries, including TensorFlow/Keras for the Deep Learning components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d81a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set random seeds for reproduceability\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration class \n",
    "# We define an `ExperimentConfig` class to encapsulate all hyperparameters. This makes it easy to switch between different experimental setups (e.g., changing the lookback window or enabling/disabling terminal headways) without rewriting code. \n",
    "\n",
    "class ExperimentConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lookback_mins=30, # Paper: 30 minutes\n",
    "        forecast_mins=15, # Paper: 15 minutes (Single Step)\n",
    "        time_bin_size_min=5,\n",
    "        use_terminal_headway=True,\n",
    "        batch_size=32,    # Paper: 32\n",
    "        epochs=32, \n",
    "        learning_rate=0.001 # Paper: 0.001\n",
    "    ):\n",
    "        self.lookback_mins = lookback_mins\n",
    "        self.forecast_mins = forecast_mins\n",
    "        self.time_bin_size_min = time_bin_size_min\n",
    "        self.use_terminal_headway = use_terminal_headway\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # calculated properties\n",
    "        self.lookback_bins = lookback_mins // time_bin_size_min\n",
    "        self.forecast_bins = forecast_mins // time_bin_size_min\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"ExperimentalConfig(L={self.lookback_mins}m, \"\n",
    "                f\"F={self.forecast_mins}m, \"\n",
    "                f\"Use_T={self.use_terminal_headway}\")\n",
    "\n",
    "# create baseline configuration (exp-A1)\n",
    "config = ExperimentConfig(\n",
    "    lookback_mins=30, # Baseline from Table 1\n",
    "    forecast_mins=15, # Baseline from Table 1\n",
    "    use_terminal_headway=True\n",
    ")\n",
    "\n",
    "print(f\"Active Configuration {config}\")\n",
    "print(f\"Lookback Bins: {config.lookback_bins}\")\n",
    "print(f\"Forecast Bins: {config.forecast_bins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab463b",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation\n",
    "\n",
    "We load the preprocessed matrix and schedule data. We then use the `create_dataset` function (adapted from the EDA notebook) to generate the tensors based on the active `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f41e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "MATRIX_PATH = \"../data/headway_matrix_full.npy\"\n",
    "SCHEDULE_PATH = \"../data/target_terminal_headways.csv\"\n",
    "GLOBAL_START_TIME = \"2025-06-06 00:00:00\"\n",
    "\n",
    "def load_and_process_data(config):\n",
    "    \"\"\"\n",
    "    loads the pre-processed matrix and prepares the (X, T, Y) tensors based on the config.\n",
    "    \n",
    "    NOTE: This function expects 'headway_matrix_full.npy' to contain ONLY Northbound traffic\n",
    "    (filtered upstream in 3_data_merging.ipynb) to avoid directional collision.\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # 1. Load Matrix (Northbound Only)\n",
    "    matrix = np.load(MATRIX_PATH)\n",
    "\n",
    "    # 2 normalize the data\n",
    "    SCALING_FACTOR = 20.0\n",
    "    matrix = matrix / SCALING_FACTOR\n",
    "\n",
    "    # 3 load and align schedule\n",
    "    # Note: target_terminal_headways.csv is verified to contain only Northbound trips (N..R)\n",
    "    schedule_df = pd.read_csv(SCHEDULE_PATH)\n",
    "    schedule_df['datetime'] = pd.to_datetime(schedule_df['service_date']) + \\\n",
    "                              pd.to_timedelta(schedule_df['departure_seconds'], unit='s')\n",
    "    schedule_df = schedule_df.set_index('datetime').sort_index()\n",
    "    schedule_df = schedule_df[~schedule_df.index.duplicated(keep='first')]\n",
    "    schedule_df = schedule_df[schedule_df.index >= GLOBAL_START_TIME]\n",
    "\n",
    "    # fill nans and resample\n",
    "    schedule_df['scheduled_headway_min'] = schedule_df['scheduled_headway_min'].bfill()\n",
    "    \n",
    "    # normalize schedule too\n",
    "    schedule_df['scheduled_headway_min'] = schedule_df['scheduled_headway_min'] / SCALING_FACTOR\n",
    "    \n",
    "    time_coords = pd.date_range(start=GLOBAL_START_TIME, periods=matrix.shape[0], freq=f\"{config.time_bin_size_min}min\")\n",
    "    schedule_resampled = schedule_df['scheduled_headway_min'].resample(f'{config.time_bin_size_min}min').ffill()\n",
    "    schedule_aligned = schedule_resampled.reindex(time_coords, method='ffill').bfill().values\n",
    "\n",
    "    # 4. create tensors\n",
    "    print(f\"Generating tensors with L={config.lookback_bins} bins, F={config.forecast_bins} bins...\")\n",
    "    X, T, Y = [], [], []\n",
    "\n",
    "    for i in range(config.lookback_bins, len(matrix) - config.forecast_bins):\n",
    "        # Input X: Past L steps\n",
    "        X.append(matrix[i-config.lookback_bins:i, :])\n",
    "        # Input T: Future F steps\n",
    "        T.append(schedule_aligned[i:i+config.forecast_bins])\n",
    "        # Target Y: Future F steps\n",
    "        Y.append(matrix[i:i+config.forecast_bins, :])\n",
    "\n",
    "    X = np.array(X)[..., np.newaxis] #(Batch, Time, Space, 1)\n",
    "    T = np.array(T)[..., np.newaxis] #(Batch, Time, 1)\n",
    "    Y = np.array(Y)[..., np.newaxis] #(Batch, Time, Space, 1)\n",
    "\n",
    "    return X, T, Y\n",
    "\n",
    "# execute data loading\n",
    "X, T, Y = load_and_process_data(config)\n",
    "\n",
    "print(f\"\\nData Shapes:\")\n",
    "print(f\"X (Context): {X.shape}\")\n",
    "print(f\"T (Intent): {T.shape}\")\n",
    "print(f\"Y (Target): {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9c6b7",
   "metadata": {},
   "source": [
    "## 3. Model Architecture (ConvLSTM)\n",
    "\n",
    "We define the `build_model` function. It constructs a Keras model using `ConvLSTM2D` layers to capture spatiotemporal dependencies.\n",
    "\n",
    "**Key Features:**\n",
    "*   **5D Input Handling:** `ConvLSTM2D` expects `(Batch, Time, Rows, Cols, Channels)`. Since our data is 1D space (Stations), we reshape it to `(Time, Stations, 1, 1)` inside the model.\n",
    "*   **Dual Input Support:** If `config.use_terminal_headway` is True, the model creates a secondary input branch for the schedule ($T$), processes it, and concatenates it with the main traffic flow features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57874e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, input_shape_x, input_shape_t=None):\n",
    "    \"\"\"\n",
    "    Constructs a 'Stripped Down' ConvLSTM model.\n",
    "    Removes the Recurrent Decoder to improve speed and convergence.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Encoder: ConvLSTM (Reads History) -> Summary State\n",
    "    2. Projection: Conv2D (Projects State to Future Horizon)\n",
    "    3. Merger: Combines Projected State with Schedule (T)\n",
    "    \"\"\"\n",
    "    # --- Encoder (Reads History X) ---\n",
    "    # Input: (L, Space, 1)\n",
    "    input_x = layers.Input(shape=input_shape_x, name=\"input_x\")\n",
    "    \n",
    "    # Reshape for ConvLSTM: (L, Space, 1, 1)\n",
    "    x_reshaped = layers.Reshape((config.lookback_bins, input_shape_x[1], 1, 1))(input_x)\n",
    "\n",
    "    # Encoder: Single ConvLSTM layer\n",
    "    # Collapses time dimension (return_sequences=False)\n",
    "    # Output Shape: (Batch, Space, 1, 32)\n",
    "    encoder_out = layers.ConvLSTM2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 1),\n",
    "        padding=\"same\",\n",
    "        return_state=False,\n",
    "        return_sequences=False, \n",
    "        activation=\"relu\"\n",
    "    )(x_reshaped)\n",
    "    \n",
    "    # --- Projection (CNN Decoder) ---\n",
    "    # Instead of unrolling a loop, we project the features directly to the forecast horizon.\n",
    "    # We use filters=ForecastBins to generate all time steps at once.\n",
    "    # Output Shape: (Batch, Space, 1, F)\n",
    "    projection = layers.Conv2D(\n",
    "        filters=config.forecast_bins, \n",
    "        kernel_size=(1, 1), \n",
    "        activation=\"relu\"\n",
    "    )(encoder_out)\n",
    "    \n",
    "    # Reshape to (Batch, F, Space, 1)\n",
    "    # 1. Permute dims: (Batch, Space, 1, F) -> (Batch, F, Space, 1)\n",
    "    # Keras Permute expects 1-based index excluding batch. \n",
    "    # Current dims (excluding batch): (Space=1, 1=2, F=3) -> Want (F=3, Space=1, 1=2)\n",
    "    forecast_base = layers.Permute((3, 1, 2))(projection)\n",
    "    \n",
    "    # --- Merge with Schedule (T) ---\n",
    "    if config.use_terminal_headway and input_shape_t is not None:\n",
    "        input_t = layers.Input(shape=input_shape_t, name=\"input_t\") # (F, 1)\n",
    "        \n",
    "        # Reshape T to (Batch, F, 1, 1)\n",
    "        t_reshaped = layers.Reshape((config.forecast_bins, 1, 1))(input_t)\n",
    "        \n",
    "        # Tile T across space: (Batch, F, Space, 1)\n",
    "        t_tiled = layers.Lambda(lambda x: tf.tile(x, [1, 1, input_shape_x[1], 1]))(t_reshaped)\n",
    "        \n",
    "        # Concatenate: (Batch, F, Space, 2)\n",
    "        merged = layers.Concatenate(axis=-1)([forecast_base, t_tiled])\n",
    "        \n",
    "        # Refine with 1x1 Conv\n",
    "        outputs = layers.TimeDistributed(layers.Conv2D(16, (1, 1), activation=\"relu\"))(merged)\n",
    "        outputs = layers.TimeDistributed(layers.Conv2D(1, (1, 1), activation=\"linear\"))(outputs)\n",
    "        \n",
    "        inputs = [input_x, input_t]\n",
    "    else:\n",
    "        outputs = forecast_base\n",
    "        inputs = [input_x]\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=config.learning_rate), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# Re-build model\n",
    "model = build_model(config, input_shape_x=X.shape[1:], input_shape_t=T.shape[1:])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(f\"Training model for {config.epochs} epochs...\")\n",
    "\n",
    "# callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=[X, T],  # <--- Pass BOTH inputs here\n",
    "    y=Y, \n",
    "    validation_split=0.2, \n",
    "    epochs=config.epochs, \n",
    "    batch_size=config.batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a prediction\n",
    "# select a random sample\n",
    "sample_idx = np.random.randint(0, len(X))\n",
    "x_sample = X[sample_idx:sample_idx+1]\n",
    "t_sample = T[sample_idx:sample_idx+1] # Get corresponding T\n",
    "y_true = Y[sample_idx:sample_idx+1]\n",
    "\n",
    "y_pred = model.predict([x_sample, t_sample]) # Pass BOTH inputs\n",
    "\n",
    "# shapes (1, Time, Space, 1)\n",
    "# lets plot the space-time heatmap for truth vs Pred\n",
    "# we transpose to have time on x-axis and space (stations) on y axis\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ground truth\n",
    "sns.heatmap(y_true[0, :, :, 0].T, ax=axes[0], cmap=\"viridis\")\n",
    "axes[0].set_title(f\"Ground Truth (Sample {sample_idx})\")\n",
    "axes[0].set_xlabel(\"Time Step (Future)\")\n",
    "axes[0].set_ylabel(\"Station ID\")\n",
    "\n",
    "# prediction\n",
    "sns.heatmap(y_pred[0, :, :, 0].T, ax=axes[1], cmap=\"viridis\")\n",
    "axes[1].set_title(f\"Prediction (Sample {sample_idx})\")\n",
    "axes[1].set_xlabel(\"Time Step (Future)\")\n",
    "axes[1].set_ylabel(\"Station ID\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1957223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Re-create Validation Split (Last 20%)\n",
    "val_split_idx = int(len(X) * 0.8)\n",
    "X_val = X[val_split_idx:]\n",
    "T_val = T[val_split_idx:]\n",
    "Y_val = Y[val_split_idx:]\n",
    "\n",
    "print(f\"Evaluating on {len(X_val)} validation samples...\")\n",
    "\n",
    "# 2. Predict\n",
    "# Note: We pass BOTH inputs [X_val, T_val]\n",
    "Y_pred_norm = model.predict([X_val, T_val], verbose=1)\n",
    "\n",
    "# 3. Inverse Transform (Normalized -> Minutes -> Seconds)\n",
    "# Recall: We divided by 20.0 to normalize\n",
    "SCALING_FACTOR = 20.0\n",
    "Y_val_sec = Y_val * SCALING_FACTOR * 60\n",
    "Y_pred_sec = Y_pred_norm * SCALING_FACTOR * 60\n",
    "\n",
    "# 4. Calculate Metrics\n",
    "# Flatten arrays because metrics expect 1D arrays\n",
    "rmse = np.sqrt(mean_squared_error(Y_val_sec.flatten(), Y_pred_sec.flatten()))\n",
    "r2 = r2_score(Y_val_sec.flatten(), Y_pred_sec.flatten())\n",
    "\n",
    "print(\"\\n--- Experiment Results ---\")\n",
    "print(f\"RMSE: {rmse:.2f} seconds\")\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
