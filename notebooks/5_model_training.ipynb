{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "LOOKBACK_MINS = 30\n",
    "FORECAST_MINS = 15\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# File Paths\n",
    "DATA_DIR = \"../data\"\n",
    "HEADWAY_MATRIX_FILE = os.path.join(DATA_DIR, \"headway_matrix_full.npy\")\n",
    "SCHEDULE_MATRIX_FILE = os.path.join(DATA_DIR, \"schedule_matrix_full.npy\")\n",
    "\n",
    "print(\"Imports complete.\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "print(f\"Loading Headway Matrix from {HEADWAY_MATRIX_FILE}...\")\n",
    "headway_matrix = np.load(HEADWAY_MATRIX_FILE)\n",
    "print(f\"Headway Matrix Shape: {headway_matrix.shape}\") # (Time, Stations, Directions, Channels)\n",
    "\n",
    "print(f\"Loading Schedule Matrix from {SCHEDULE_MATRIX_FILE}...\")\n",
    "schedule_matrix = np.load(SCHEDULE_MATRIX_FILE)\n",
    "print(f\"Schedule Matrix Shape: {schedule_matrix.shape}\") # (Time, Directions, Channels)\n",
    "\n",
    "# Verify shapes match in time dimension\n",
    "assert headway_matrix.shape[0] == schedule_matrix.shape[0], \"Time dimensions do not match!\"\n",
    "\n",
    "# Normalize Data (if not already normalized)\n",
    "# Master plan says: \"Scale all values to range [0, 1]\"\n",
    "# Assuming max headway is around 30-60 mins (1800-3600 seconds).\n",
    "# Let's check the max value in the data.\n",
    "max_headway = np.max(headway_matrix)\n",
    "print(f\"Max Headway in Data: {max_headway}\")\n",
    "\n",
    "# If max > 1, we need to normalize.\n",
    "if max_headway > 1.0:\n",
    "    print(\"Normalizing data to [0, 1] range...\")\n",
    "    # Use a fixed scaler to allow inverse transform later. \n",
    "    # 3600 seconds (60 mins) is a safe upper bound for subway headways.\n",
    "    SCALER = 3600.0 \n",
    "    headway_matrix = headway_matrix / SCALER\n",
    "    # Schedule might also be in seconds?\n",
    "    max_schedule = np.max(schedule_matrix)\n",
    "    print(f\"Max Schedule Value: {max_schedule}\")\n",
    "    if max_schedule > 1.0:\n",
    "         schedule_matrix = schedule_matrix / SCALER\n",
    "else:\n",
    "    print(\"Data appears to be already normalized.\")\n",
    "\n",
    "print(\"Data Loading & Normalization Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create Datasets\n",
    "def create_dataset(headway_data, schedule_data, start_index, end_index, batch_size):\n",
    "    # Alignment Logic:\n",
    "    # Target Y[t]: Future Headways from t to t+15.\n",
    "    # Input X[t]: Past Headways from t-30 to t.\n",
    "    # Input T[t]: Future Schedule from t to t+15.\n",
    "    \n",
    "    # We align the datasets so that for a given index `i`:\n",
    "    # Y starts at `i`\n",
    "    # T starts at `i`\n",
    "    # X starts at `i - 30` (so it ends at `i`)\n",
    "    \n",
    "    ds_x = keras.utils.timeseries_dataset_from_array(\n",
    "        data=headway_data,\n",
    "        targets=None,\n",
    "        sequence_length=LOOKBACK_MINS,\n",
    "        sequence_stride=1,\n",
    "        sampling_rate=1,\n",
    "        batch_size=batch_size,\n",
    "        start_index=start_index - LOOKBACK_MINS,\n",
    "        end_index=end_index - FORECAST_MINS\n",
    "    )\n",
    "    \n",
    "    ds_t = keras.utils.timeseries_dataset_from_array(\n",
    "        data=schedule_data,\n",
    "        targets=None,\n",
    "        sequence_length=FORECAST_MINS,\n",
    "        sequence_stride=1,\n",
    "        sampling_rate=1,\n",
    "        batch_size=batch_size,\n",
    "        start_index=start_index,\n",
    "        end_index=end_index\n",
    "    )\n",
    "    \n",
    "    ds_y = keras.utils.timeseries_dataset_from_array(\n",
    "        data=headway_data,\n",
    "        targets=None,\n",
    "        sequence_length=FORECAST_MINS,\n",
    "        sequence_stride=1,\n",
    "        sampling_rate=1,\n",
    "        batch_size=batch_size,\n",
    "        start_index=start_index,\n",
    "        end_index=end_index\n",
    "    )\n",
    "    \n",
    "    # Zip inputs and targets\n",
    "    # Inputs: (X, T)\n",
    "    # Target: Y\n",
    "    dataset = tf.data.Dataset.zip(((ds_x, ds_t), ds_y))\n",
    "    return dataset\n",
    "\n",
    "# Split Data\n",
    "total_samples = len(headway_matrix)\n",
    "train_split_idx = int(total_samples * 0.8)\n",
    "\n",
    "# Ensure we have enough history for the first sample\n",
    "start_idx = LOOKBACK_MINS \n",
    "\n",
    "print(f\"Creating Training Dataset (0 to {train_split_idx})...\")\n",
    "train_ds = create_dataset(\n",
    "    headway_matrix, \n",
    "    schedule_matrix, \n",
    "    start_index=start_idx, \n",
    "    end_index=train_split_idx, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Creating Validation Dataset ({train_split_idx} to {total_samples})...\")\n",
    "val_ds = create_dataset(\n",
    "    headway_matrix, \n",
    "    schedule_matrix, \n",
    "    start_index=train_split_idx, \n",
    "    end_index=total_samples, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Prefetch for performance\n",
    "train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Datasets created.\")\n",
    "for (x, t), y in train_ds.take(1):\n",
    "    print(f\"Input X shape: {x.shape}\")\n",
    "    print(f\"Input T shape: {t.shape}\")\n",
    "    print(f\"Target Y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ece0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build Model\n",
    "def build_model(input_shape_x, input_shape_t, output_shape):\n",
    "    # Input 1: History (Batch, 30, 64, 2, 1)\n",
    "    input_x = layers.Input(shape=input_shape_x, name='history_input')\n",
    "    \n",
    "    # Input 2: Terminal Schedule (Batch, 15, 2, 1)\n",
    "    input_t = layers.Input(shape=input_shape_t, name='terminal_input')\n",
    "    \n",
    "    # --- Encoder (ConvLSTM Branch) ---\n",
    "    # Layer 1\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 1),\n",
    "        padding='same',\n",
    "        return_sequences=True,\n",
    "        activation='relu'\n",
    "    )(input_x)\n",
    "    \n",
    "    # Layer 2\n",
    "    x = layers.ConvLSTM2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 1),\n",
    "        padding='same',\n",
    "        return_sequences=False, # Compress time dimension\n",
    "        activation='relu'\n",
    "    )(x)\n",
    "    \n",
    "    # Flatten Spatial Features\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # --- Fusion ---\n",
    "    # Flatten Schedule\n",
    "    t = layers.Flatten()(input_t)\n",
    "    \n",
    "    # Concatenate\n",
    "    combined = layers.Concatenate()([x, t])\n",
    "    \n",
    "    # --- Decoder (Projector) ---\n",
    "    # Calculate output dimensions\n",
    "    # Output shape: (15, 64, 2, 1)\n",
    "    out_steps = output_shape[0]\n",
    "    out_stations = output_shape[1]\n",
    "    out_dirs = output_shape[2]\n",
    "    out_channels = output_shape[3]\n",
    "    \n",
    "    flat_output_size = out_steps * out_stations * out_dirs * out_channels\n",
    "    \n",
    "    z = layers.Dense(flat_output_size, activation='sigmoid')(combined) # Sigmoid for [0, 1] output\n",
    "    \n",
    "    # Reshape to target shape\n",
    "    output = layers.Reshape(output_shape)(z)\n",
    "    \n",
    "    model = keras.Model(inputs=[input_x, input_t], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Define shapes\n",
    "# X: (30, 156, 2, 1)\n",
    "input_shape_x = (LOOKBACK_MINS, headway_matrix.shape[1], headway_matrix.shape[2], headway_matrix.shape[3])\n",
    "# T: (15, 2, 1)\n",
    "input_shape_t = (FORECAST_MINS, schedule_matrix.shape[1], schedule_matrix.shape[2])\n",
    "# Y: (15, 156, 2, 1)\n",
    "output_shape = (FORECAST_MINS, headway_matrix.shape[1], headway_matrix.shape[2], headway_matrix.shape[3])\n",
    "\n",
    "print(f\"Input X Shape: {input_shape_x}\")\n",
    "print(f\"Input T Shape: {input_shape_t}\")\n",
    "print(f\"Output Y Shape: {output_shape}\")\n",
    "\n",
    "model = build_model(input_shape_x, input_shape_t, output_shape)\n",
    "model.summary()\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='mse',\n",
    "    metrics=[keras.metrics.RootMeanSquaredError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005895b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),\n",
    "    keras.callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Plot History\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss (MSE)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')\n",
    "plt.plot(history.history['val_root_mean_squared_error'], label='Val RMSE')\n",
    "plt.title('RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
