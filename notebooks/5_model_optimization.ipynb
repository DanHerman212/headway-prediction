{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4iFL_Y7jG3qA",
   "metadata": {
    "id": "4iFL_Y7jG3qA"
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# === CRITICAL: Enable Mixed Precision for A100/V100/T4 GPUs ===\n",
    "# This provides 2-3x speedup by using float16 for compute\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print(f\"‚úÖ Mixed Precision enabled: {policy.name}\")\n",
    "print(f\"   Compute dtype: {policy.compute_dtype}\")\n",
    "print(f\"   Variable dtype: {policy.variable_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LCOcxhtZG6MN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1767729407264,
     "user": {
      "displayName": "Dan Herman",
      "userId": "12632515228691427055"
     },
     "user_tz": 300
    },
    "id": "LCOcxhtZG6MN",
    "outputId": "22683568-289c-4737-92de-98f1422e3653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Colab\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "added to sys.path: /content/drive/MyDrive/Colab Notebooks/headway-prediction\n",
      "Project Root: /content/drive/MyDrive/Colab Notebooks/headway-prediction\n"
     ]
    }
   ],
   "source": [
    "# environment configuration\n",
    "# check if running in colab\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "  from google.colab import drive\n",
    "  print(\"Running in Colab\")\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  # EDIT THIS: Your exact folder path in Drive\n",
    "  PROJECT_ROOT = \"/content/drive/MyDrive/Colab Notebooks/headway-prediction\"\n",
    "else:\n",
    "    print(\"üíª Running in Local Environment\")\n",
    "    # Assuming notebook is in /notebooks, root is one level up\n",
    "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# system setup path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "  sys.path.append(PROJECT_ROOT)\n",
    "  print(f\"added to sys.path: {PROJECT_ROOT}\")\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pN-RNMkSHDiZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1767729410267,
     "user": {
      "displayName": "Dan Herman",
      "userId": "12632515228691427055"
     },
     "user_tz": 300
    },
    "id": "pN-RNMkSHDiZ",
    "outputId": "7e5f066e-3373-4fe9-b6f8-01b1e303088c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: All custom 'src' modules imported.\n"
     ]
    }
   ],
   "source": [
    "# Validate Imports\n",
    "try:\n",
    "    from src.config import Config\n",
    "    from src.data.dataset import SubwayDataGenerator\n",
    "    from src.models.st_convnet import HeadwayConvLSTM  # V1 architecture\n",
    "    from src.training.trainer import Trainer  # ‚Üê Use trainer module\n",
    "    from src.evaluator import Evaluator\n",
    "    print(\"‚úÖ Success: All custom 'src' modules imported.\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå IMPORT ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l6G9YsokLQkC",
   "metadata": {
    "id": "l6G9YsokLQkC"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882CebtOLP64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1062,
     "status": "ok",
     "timestamp": 1767729413901,
     "user": {
      "displayName": "Dan Herman",
      "userId": "12632515228691427055"
     },
     "user_tz": 300
    },
    "id": "882CebtOLP64",
    "outputId": "42b1d181-39f2-4b83-ebb0-c59acc0f6d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /content/drive/MyDrive/Colab Notebooks/headway-prediction/data\n",
      "Loading data from /content/drive/MyDrive/Colab Notebooks/headway-prediction/data...\n",
      "Headway Shape: (264222, 66, 2, 1)\n",
      "Schedule Shape: (264222, 2, 1)\n",
      "Raw max headway values: 30.0 min (should be ~30.0)\n",
      "\n",
      "Fitting RobustScaler on first 158533 steps\n",
      "Transforming Headway and Schedule Data\n",
      "Scaler saved to /content/drive/MyDrive/Colab Notebooks/headway-prediction/models/robust_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Data loading and scaling\n",
    "# Paper uses MinMax normalization to [0,1] (Section 3.1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "config = Config()\n",
    "config.DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "print(f\"Loading data from {config.DATA_DIR}\")\n",
    "\n",
    "# 1. Instantiate data generator\n",
    "data_gen = SubwayDataGenerator(config)\n",
    "\n",
    "# 2. Load raw .npy files (no normalization - we'll use MinMaxScaler per paper)\n",
    "data_gen.load_data(normalize=False)\n",
    "print(f\"Raw max headway values: {data_gen.headway_data.max():.2f} min\")\n",
    "\n",
    "# 3. Fit MinMaxScaler (Paper Section 3.1: \"normalized to [0,1] using min-max scaling\")\n",
    "total_timesteps = len(data_gen.headway_data)\n",
    "train_limit = int(total_timesteps * 0.6)\n",
    "\n",
    "print(f\"\\nüìÑ Paper: 'headway values normalized to the interval [0, 1] using min-max scaling'\")\n",
    "print(f\"Fitting MinMaxScaler on first {train_limit} steps\")\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "flat_train = data_gen.headway_data[:train_limit].reshape(-1, 1)\n",
    "scaler.fit(flat_train)\n",
    "\n",
    "# 4. Transform All Data\n",
    "print(\"Transforming Headway and Schedule Data\")\n",
    "data_gen.headway_data = scaler.transform(data_gen.headway_data.reshape(-1, 1)).reshape(data_gen.headway_data.shape)\n",
    "data_gen.schedule_data = scaler.transform(data_gen.schedule_data.reshape(-1, 1)).reshape(data_gen.schedule_data.shape)\n",
    "\n",
    "# 5. Save scaler for inference\n",
    "scaler_path = os.path.join(PROJECT_ROOT, \"models\", \"minmax_scaler.pkl\")\n",
    "os.makedirs(os.path.dirname(scaler_path), exist_ok=True)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"‚úÖ Scaler saved to {scaler_path}\")\n",
    "\n",
    "print(f\"\\nScaled data range: [{data_gen.headway_data.min():.4f}, {data_gen.headway_data.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T4QZ-e6KTszh",
   "metadata": {
    "id": "T4QZ-e6KTszh"
   },
   "source": [
    "# Baseline Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I3A-oEGpTsFb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1767729417897,
     "user": {
      "displayName": "Dan Herman",
      "userId": "12632515228691427055"
     },
     "user_tz": 300
    },
    "id": "I3A-oEGpTsFb",
    "outputId": "5733fad4-02ca-46c5-9e29-218b181d8e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Baseline Run Config --- \n",
      "lookback: 30\n",
      "batch: 64\n",
      "epochs: 20\n",
      "filters: 64\n",
      "\n",
      "creating datasets...\n",
      "Creating dataset from index 0 to 158533\n",
      "Creating dataset from index 158533 to 211377\n",
      "Creating dataset from index 211377 to 264177\n",
      "Input headway shape: (64, 30, 66, 2, 1)\n",
      "Target shape: (64, 15, 66, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# Using batch size 128 (same as notebook 7 which runs at ~1 min/epoch)\n",
    "# Paper used batch 32, but that's 4x slower due to 4x more batches\n",
    "\n",
    "config.LOOKBACK_MINS = 30\n",
    "config.FORECAST_MINS = 15\n",
    "config.BATCH_SIZE = 128  # ‚Üê Match notebook 7 (paper used 32, but 4x slower)\n",
    "config.EPOCHS = 100\n",
    "config.LEARNING_RATE = 0.001\n",
    "\n",
    "print(f'--- Configuration ---')\n",
    "print(f'Lookback: {config.LOOKBACK_MINS} minutes')\n",
    "print(f'Forecast: {config.FORECAST_MINS} minutes')\n",
    "print(f'Batch Size: {config.BATCH_SIZE}')\n",
    "print(f'Epochs: {config.EPOCHS}')\n",
    "print(f'Learning Rate: {config.LEARNING_RATE}')\n",
    "\n",
    "# Create tf datasets (60% train, 20% val, 20% test)\n",
    "train_end = int(0.6 * total_timesteps)\n",
    "val_end = int(0.8 * total_timesteps)\n",
    "\n",
    "print(f\"\\nCreating datasets...\")\n",
    "train_ds = data_gen.make_dataset(start_index=0, end_index=train_end, shuffle=True)\n",
    "val_ds = data_gen.make_dataset(start_index=train_end, end_index=val_end, shuffle=False)\n",
    "test_ds = data_gen.make_dataset(start_index=val_end, end_index=None, shuffle=False)\n",
    "\n",
    "# Quick shape check\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"Input headway shape: {inputs['headway_input'].shape}\")\n",
    "    print(f\"Input schedule shape: {inputs['schedule_input'].shape}\")\n",
    "    print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xDCAccOXVW53",
   "metadata": {
    "id": "xDCAccOXVW53"
   },
   "source": [
    "# Model Build and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-jvmAwj2VaPd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jvmAwj2VaPd",
    "outputId": "5678fd20-5714-4249-b4bb-d8bdc51723bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building V2 Architecture\n",
      "Starting Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m2477/2477\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 197ms/step - loss: 0.1695 - mae: 0.4089 - mse: 0.3959 - val_loss: 0.1649 - val_mae: 0.3905 - val_mse: 0.3955 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m 145/2477\u001b[0m \u001b[32m‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m6:46\u001b[0m 175ms/step - loss: 0.1515 - mae: 0.3731 - mse: 0.3572"
     ]
    }
   ],
   "source": [
    "# Build and train using the Trainer module\n",
    "print(f\"\\nüèóÔ∏è Building Model...\")\n",
    "\n",
    "builder = HeadwayConvLSTM(config)\n",
    "model = builder.build_model()\n",
    "model.summary()\n",
    "\n",
    "# Use Trainer class for clean compilation and training\n",
    "checkpoint_dir = os.path.join(PROJECT_ROOT, \"models\")\n",
    "trainer = Trainer(model, config, checkpoint_dir=checkpoint_dir)\n",
    "trainer.compile_model()\n",
    "\n",
    "print(\"\\nüöÄ Starting Training...\")\n",
    "history = trainer.fit(\n",
    "    train_ds, \n",
    "    val_ds,\n",
    "    patience=10,  # Early stopping patience\n",
    "    reduce_lr_patience=5  # Reduce LR if no improvement for 5 epochs\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rpu1TCpAXc6b",
   "metadata": {
    "id": "rpu1TCpAXc6b"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T3UlzE0HXMPd",
   "metadata": {
    "id": "T3UlzE0HXMPd"
   },
   "outputs": [],
   "source": [
    "# Full Model Evaluation using the Evaluator module\n",
    "# Evaluator now handles unit conversion (normalized ‚Üí seconds) and paper-style visualizations\n",
    "\n",
    "# Initialize evaluator with scaler for proper unit conversion\n",
    "evaluator = Evaluator(config, scaler=scaler)\n",
    "\n",
    "# Run full evaluation pipeline:\n",
    "# 1. Metrics summary (MAE/RMSE in seconds, production readiness)\n",
    "# 2. Training curves with dual y-axis (normalized + seconds)\n",
    "# 3. Paper-style heatmap visualizations (Figure 7 style)\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"images\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "metrics = evaluator.full_evaluation(\n",
    "    model=trainer.model,\n",
    "    history=history,\n",
    "    test_dataset=test_ds,\n",
    "    save_dir=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n67pUBVJrRaZ",
   "metadata": {
    "id": "n67pUBVJrRaZ"
   },
   "outputs": [],
   "source": [
    "# Smoke Test: Verify paper architecture trains quickly\n",
    "import time\n",
    "from src.models.st_convnet_paper import HeadwayConvLSTM\n",
    "\n",
    "# Paper parameters (Table 1)\n",
    "BATCH_SIZE = 32\n",
    "N_STATIONS = 64  # Paper uses 64 distance bins\n",
    "LOOKBACK = 30\n",
    "FORECAST = 15\n",
    "\n",
    "# Generate Random Data in RAM\n",
    "print(\"Generating synthetic data (matching paper dimensions)...\")\n",
    "X_headway = np.random.rand(BATCH_SIZE * 4, LOOKBACK, N_STATIONS, 2, 1).astype('float32')  # [0,1] range\n",
    "X_schedule = np.random.rand(BATCH_SIZE * 4, FORECAST, 2, 1).astype('float32')\n",
    "Y_target = np.random.rand(BATCH_SIZE * 4, FORECAST, N_STATIONS, 2, 1).astype('float32')\n",
    "\n",
    "# Build & Compile\n",
    "print(\"Building paper-faithful model...\")\n",
    "builder = HeadwayConvLSTM(n_stations=N_STATIONS, lookback=LOOKBACK, forecast=FORECAST)\n",
    "test_model = builder.build_model()\n",
    "test_model.compile(optimizer='adam', loss='mse')\n",
    "print(f\"Model parameters: {test_model.count_params():,}\")\n",
    "\n",
    "# Time the Training Loop\n",
    "print(\"\\n‚è±Ô∏è Starting Smoke Test (4 batches, 1 epoch)...\")\n",
    "start = time.time()\n",
    "test_model.fit([X_headway, X_schedule], Y_target, epochs=1, batch_size=BATCH_SIZE, verbose=1)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\n‚úÖ Total Time: {end - start:.2f} seconds\")\n",
    "print(f\"   Paper model has ~187K params (vs V1's 371K, V2's 389K)\")\n",
    "print(f\"   Expected: ~5-15s on GPU with mixed precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BI8lms3N3vKn",
   "metadata": {
    "id": "BI8lms3N3vKn"
   },
   "outputs": [],
   "source": [
    "# Quick training speed test with real data\n",
    "print(\"Creating MINI dataset for speed test...\")\n",
    "train_ds_mini = data_gen.make_dataset(start_index=0, end_index=2000, shuffle=True)\n",
    "val_ds_mini = data_gen.make_dataset(start_index=2000, end_index=2500, shuffle=False)\n",
    "\n",
    "print(\"Running 5 epochs to measure per-epoch time...\")\n",
    "import time\n",
    "start = time.time()\n",
    "model.fit(train_ds_mini, validation_data=val_ds_mini, epochs=5, verbose=1)\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nüìä Average time per epoch: {elapsed/5:.1f}s\")\n",
    "print(f\"   Paper target: ~45-60s/epoch on A100 with full dataset\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
