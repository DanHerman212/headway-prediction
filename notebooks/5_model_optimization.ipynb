{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4iFL_Y7jG3qA",
   "metadata": {
    "id": "4iFL_Y7jG3qA"
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.preprocessing import RobustScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LCOcxhtZG6MN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1767729407264,
     "user": {
      "displayName": "Dan Herman",
      "userId": "12632515228691427055"
     },
     "user_tz": 300
    },
    "id": "LCOcxhtZG6MN",
    "outputId": "22683568-289c-4737-92de-98f1422e3653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Colab\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "added to sys.path: /content/drive/MyDrive/Colab Notebooks/headway-prediction\n",
      "Project Root: /content/drive/MyDrive/Colab Notebooks/headway-prediction\n"
     ]
    }
   ],
   "source": [
    "# environment configuration\n",
    "# check if running in colab\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "  from google.colab import drive\n",
    "  print(\"Running in Colab\")\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  # EDIT THIS: Your exact folder path in Drive\n",
    "  PROJECT_ROOT = \"/content/drive/MyDrive/Colab Notebooks/headway-prediction\"\n",
    "else:\n",
    "    print(\"ðŸ’» Running in Local Environment\")\n",
    "    # Assuming notebook is in /notebooks, root is one level up\n",
    "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# system setup path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "  sys.path.append(PROJECT_ROOT)\n",
    "  print(f\"added to sys.path: {PROJECT_ROOT}\")\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pN-RNMkSHDiZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1767729410267,
     "user": {
      "displayName": "Dan Herman",
      "userId": "12632515228691427055"
     },
     "user_tz": 300
    },
    "id": "pN-RNMkSHDiZ",
    "outputId": "7e5f066e-3373-4fe9-b6f8-01b1e303088c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success: All custom 'src' modules imported.\n"
     ]
    }
   ],
   "source": [
    "# Validate Imports\n",
    "try:\n",
    "    from src.config import Config\n",
    "    from src.data.dataset import SubwayDataGenerator\n",
    "    from src.models.st_convnet_v2 import HeadwayConvLSTM\n",
    "    from src.evaluator import Evaluator\n",
    "    print(\"âœ… Success: All custom 'src' modules imported.\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ IMPORT ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l6G9YsokLQkC",
   "metadata": {
    "id": "l6G9YsokLQkC"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882CebtOLP64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1062,
     "status": "ok",
     "timestamp": 1767729413901,
     "user": {
      "displayName": "Dan Herman",
      "userId": "12632515228691427055"
     },
     "user_tz": 300
    },
    "id": "882CebtOLP64",
    "outputId": "42b1d181-39f2-4b83-ebb0-c59acc0f6d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /content/drive/MyDrive/Colab Notebooks/headway-prediction/data\n",
      "Loading data from /content/drive/MyDrive/Colab Notebooks/headway-prediction/data...\n",
      "Headway Shape: (264222, 66, 2, 1)\n",
      "Schedule Shape: (264222, 2, 1)\n",
      "Raw max headway values: 30.0 min (should be ~30.0)\n",
      "\n",
      "Fitting RobustScaler on first 158533 steps\n",
      "Transforming Headway and Schedule Data\n",
      "Scaler saved to /content/drive/MyDrive/Colab Notebooks/headway-prediction/models/robust_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# data loading and scaling\n",
    "config = Config()\n",
    "config.DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "print(f\"Loading data from {config.DATA_DIR}\")\n",
    "\n",
    "# 1 instantiate data generator\n",
    "data_gen = SubwayDataGenerator(config)\n",
    "\n",
    "# 2 load raw .npy files (no normalization - we'll use RobustScaler)\n",
    "data_gen.load_data(normalize=False)\n",
    "print(f\"Raw max headway values: {data_gen.headway_data.max():.2f} min\")\n",
    "\n",
    "# 3 fit robust scaler (training split only)\n",
    "# we fit on the first 60% to avoid leakage\n",
    "total_timesteps = len(data_gen.headway_data)\n",
    "train_limit = int(total_timesteps * 0.6)\n",
    "\n",
    "print(f\"\\nFitting RobustScaler on first {train_limit} steps\")\n",
    "scaler = RobustScaler()\n",
    "flat_train = data_gen.headway_data[:train_limit].reshape(-1, 1)\n",
    "scaler.fit(flat_train)\n",
    "\n",
    "# 4. Transform All Data\n",
    "print(\"Transforming Headway and Schedule Data\")\n",
    "data_gen.headway_data = scaler.transform(data_gen.headway_data.reshape(-1, 1)).reshape(data_gen.headway_data.shape)\n",
    "# use same scaler for schedule to match units\n",
    "data_gen.schedule_data = scaler.transform(data_gen.schedule_data.reshape(-1, 1)).reshape(data_gen.schedule_data.shape)\n",
    "\n",
    "# 5 save scaler for inference\n",
    "scaler_path = os.path.join(PROJECT_ROOT, \"models\", \"robust_scaler.pkl\")\n",
    "os.makedirs(os.path.dirname(scaler_path), exist_ok=True)\n",
    "joblib.dump(scaler, scaler_path)  # â† FIXED: Actually save the scaler\n",
    "print(f\"âœ… Scaler saved to {scaler_path}\")\n",
    "\n",
    "print(f\"\\nScaled data range: [{data_gen.headway_data.min():.2f}, {data_gen.headway_data.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T4QZ-e6KTszh",
   "metadata": {
    "id": "T4QZ-e6KTszh"
   },
   "source": [
    "# Baseline Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I3A-oEGpTsFb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1767729417897,
     "user": {
      "displayName": "Dan Herman",
      "userId": "12632515228691427055"
     },
     "user_tz": 300
    },
    "id": "I3A-oEGpTsFb",
    "outputId": "5733fad4-02ca-46c5-9e29-218b181d8e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Baseline Run Config --- \n",
      "lookback: 30\n",
      "batch: 64\n",
      "epochs: 20\n",
      "filters: 64\n",
      "\n",
      "creating datasets...\n",
      "Creating dataset from index 0 to 158533\n",
      "Creating dataset from index 158533 to 211377\n",
      "Creating dataset from index 211377 to 264177\n",
      "Input headway shape: (64, 30, 66, 2, 1)\n",
      "Target shape: (64, 15, 66, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "# 3 baseline config\n",
    "# override config for the baseline experiment\n",
    "config.LOOKBACK_MINS = 30\n",
    "config.FORECAST_MINS = 15\n",
    "config.BATCH_SIZE = 64\n",
    "config.EPOCHS = 20\n",
    "config.FILTERS = 64\n",
    "\n",
    "print(f'--- Baseline Run Config --- ')\n",
    "print(f'lookback: {config.LOOKBACK_MINS}')\n",
    "print(f'batch: {config.BATCH_SIZE}')\n",
    "print(f'epochs: {config.EPOCHS}')\n",
    "print(f'filters: {config.FILTERS}')\n",
    "\n",
    "# create tf datasets\n",
    "# temporal split 60^ train, 20% val and test\n",
    "train_end = int(0.6 * total_timesteps)\n",
    "val_end = int(0.8 * total_timesteps)\n",
    "\n",
    "print(f\"\\ncreating datasets...\")\n",
    "train_ds = data_gen.make_dataset(start_index=0, end_index=train_end, shuffle=True)\n",
    "val_ds = data_gen.make_dataset(start_index=train_end, end_index=val_end, shuffle=False)\n",
    "test_ds = data_gen.make_dataset(start_index=val_end, end_index=None, shuffle=False)\n",
    "\n",
    "# quick shape check\n",
    "for inputs, targets in train_ds.take(1):\n",
    "  print(f\"Input headway shape: {inputs['headway_input'].shape}\")\n",
    "  print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xDCAccOXVW53",
   "metadata": {
    "id": "xDCAccOXVW53"
   },
   "source": [
    "# Model Build and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-jvmAwj2VaPd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jvmAwj2VaPd",
    "outputId": "5678fd20-5714-4249-b4bb-d8bdc51723bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building V2 Architecture\n",
      "Starting Training...\n",
      "Epoch 1/20\n",
      "\u001b[1m2477/2477\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 197ms/step - loss: 0.1695 - mae: 0.4089 - mse: 0.3959 - val_loss: 0.1649 - val_mae: 0.3905 - val_mse: 0.3955 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m 145/2477\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m6:46\u001b[0m 175ms/step - loss: 0.1515 - mae: 0.3731 - mse: 0.3572"
     ]
    }
   ],
   "source": [
    "# train baseline\n",
    "print(f\"\\nBuilding V2 Architecture\")\n",
    "builder = HeadwayConvLSTM(\n",
    "    n_stations=config.NUM_STATIONS,\n",
    "    lookback=config.LOOKBACK_MINS,\n",
    "    forecast=config.FORECAST_MINS,\n",
    "    output_activation='linear'  # â† Use linear for RobustScaler (unbounded range)\n",
    ")\n",
    "\n",
    "model = builder.build_model()\n",
    "model.summary()\n",
    "\n",
    "# compile (huber loss - robust to outliers)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0)\n",
    "loss_fn = tf.keras.losses.Huber(delta=1.0)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=['mae', 'mse'],\n",
    "    jit_compile=False  # Disable XLA for compatibility\n",
    ")\n",
    "\n",
    "# callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"\\nðŸš€ Starting Training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=config.EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# save model (FIXED: added missing comma in path join)\n",
    "model_path = os.path.join(PROJECT_ROOT, \"models\", \"v2_baseline_30min.keras\")\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "model.save(model_path)\n",
    "print(f\"âœ… Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rpu1TCpAXc6b",
   "metadata": {
    "id": "rpu1TCpAXc6b"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T3UlzE0HXMPd",
   "metadata": {
    "id": "T3UlzE0HXMPd"
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "print(f\"Evaluating Baseline\")\n",
    "evaluator = Evaluator(config)\n",
    "\n",
    "# 1. plot loss curves (scaled units)\n",
    "evaluator.plot_loss(history)\n",
    "\n",
    "# 2. plot predcition (real minutes)\n",
    "print(f\"Visualizing Sample Prediction\")\n",
    "evaluator.plot_spatiotemporal_prediction(\n",
    "    model,\n",
    "    test_ds,\n",
    "    scaler=scaler,\n",
    "    sample_idx=0,\n",
    "    direction=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n67pUBVJrRaZ",
   "metadata": {
    "id": "n67pUBVJrRaZ"
   },
   "outputs": [],
   "source": [
    "# Smoke Test: Verify model trains quickly with synthetic data\n",
    "import time\n",
    "from src.models.st_convnet_v2 import HeadwayConvLSTM\n",
    "\n",
    "# 1. Setup parameters matching your data\n",
    "BATCH_SIZE = 32\n",
    "N_STATIONS = config.NUM_STATIONS  # Use actual station count\n",
    "LOOKBACK = 30\n",
    "FORECAST = 15\n",
    "\n",
    "# 2. Generate Random Data in RAM (No Disk I/O)\n",
    "print(\"Generating synthetic data in RAM...\")\n",
    "X_headway = np.random.randn(BATCH_SIZE * 4, LOOKBACK, N_STATIONS, 2, 1).astype('float32')\n",
    "X_schedule = np.random.randn(BATCH_SIZE * 4, FORECAST, 2, 1).astype('float32')\n",
    "Y_target = np.random.randn(BATCH_SIZE * 4, FORECAST, N_STATIONS, 2, 1).astype('float32')\n",
    "\n",
    "# 3. Build & Compile\n",
    "print(\"Building model...\")\n",
    "builder = HeadwayConvLSTM(\n",
    "    n_stations=N_STATIONS, \n",
    "    lookback=LOOKBACK, \n",
    "    forecast=FORECAST,\n",
    "    output_activation='linear'  # Match training setup\n",
    ")\n",
    "test_model = builder.build_model()\n",
    "test_model.compile(optimizer='adam', loss='mse', jit_compile=False)\n",
    "\n",
    "# 4. Time the Training Loop\n",
    "print(\"â±ï¸ Starting Smoke Test (4 batches, 1 epoch)...\")\n",
    "start = time.time()\n",
    "test_model.fit([X_headway, X_schedule], Y_target, epochs=1, batch_size=BATCH_SIZE, verbose=1)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\nâœ… Total Time: {end - start:.2f} seconds\")\n",
    "print(f\"   Expected: ~10-30s on GPU, ~60-120s on CPU\")\n",
    "print(f\"   If > 5 min: CuDNN is NOT being used (check activations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BI8lms3N3vKn",
   "metadata": {
    "id": "BI8lms3N3vKn"
   },
   "outputs": [],
   "source": [
    "# Create a tiny dataset (just for a speed test)\n",
    "print(\"Creating MINI dataset for speed test...\")\n",
    "train_ds = data_gen.make_dataset(start_index=0, end_index=2000, shuffle=True)\n",
    "val_ds   = data_gen.make_dataset(start_index=2000, end_index=2500, shuffle=False)\n",
    "\n",
    "# Run training\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
